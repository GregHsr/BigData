{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad1ce57-67fb-4b22-bc02-8bd8b7aad9eb",
   "metadata": {
    "tags": []
   },
   "source": [
    " # <strong> Introduction to artificial neural networks </strong>\n",
    " Spring 2023 - Toulouse INP/ENSEEIHT<br /> \n",
    " by Ruming PAN & Mohamed SAADI<br /> \n",
    " Last update: 20-01-2023<br /> \n",
    " \n",
    " This notebook contains three parts:<br />\n",
    " <ol>\n",
    "  <li><strong>Part 1: Building a neural network</strong>, in which a function ```ANN``` builds a DNN (dense neural network) knowing the number of layers (hidden + output), the number of neurons per layer, and given the parameters (weights and biases) for each layer. You are asked to implement an additional activation function (e.g., linear, ReLU) following the example given for the logistic function and test them in new ANN.</li>\n",
    "  <li><strong>Part 2: Training a neural network</strong>, where a set of data \"abalone_data.xlsx\" is given to train a DNN by back-propagating the gradient of a loss function with respect to the network parameters. You are required to do three things: (1) define the training and test datasets, (2) scale the features, and most importantly (3) complete the function ```ANN_backpro``` to successfully run the algorithm. </li>\n",
    "  <li><strong>Part 3: Make your life easier with Keras</strong>, where you are asked to implement a similar DNN using ```keras```, which is a high-level interface for ```TensorFlow```, a famous framework for deep machine learning. The objective here is to present one of the most used libraries in python-based machine learning eco-system. As you will notice, the implementation is very friendly and easy.</li>\n",
    "</ol> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d764ef-555f-4788-a195-a33fdd3767bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <strong>Part 1: Building a neural network</strong>\n",
    " In this part, you are required to understand the structure of a neural network and build one given a number of layer ```NL``` and a list of number of neurons per layer ```NpL```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aae1b-d6d9-4cb8-97a9-c9fae30f3fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Import libraries, define global functions and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14d628-1419-4e93-9930-a70e5f6f87e3",
   "metadata": {},
   "source": [
    "<strong>Task 1:</strong> Based on the example provided for the logistic function, build new activation functions (tanh, ReLU, and linear) in order to use them later for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3523df9-46de-4f00-95be-12eb6bf168d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "## a function that takes a dataframe and returns some basic stastics of its columns\n",
    "def datasum(df):\n",
    "    dfmin = df.apply(np.min, 0)\n",
    "    dfmedian = df.apply(np.median, 0)\n",
    "    dfmean = df.apply(np.mean,0)\n",
    "    dfmax = df.apply(np.max, 0)\n",
    "    dfstd = df.apply(np.std, 0)\n",
    "    dfsummary = pd.DataFrame(np.array([dfmean, dfstd, dfmin, dfmedian, dfmax]), columns=list(df.columns[:]))\n",
    "    dfsummary[\"Statistic\"] = [\"mean\", \"std\", \"min\", \"median\", \"max\"]\n",
    "    xcol = dfsummary.shape[1]\n",
    "    colarr = [dfsummary.columns[xcol-1]]\n",
    "    for item in dfsummary.columns[0:(xcol-1)]:\n",
    "        colarr.append(item)\n",
    "    dfto = dfsummary[colarr]\n",
    "    return dfto\n",
    "\n",
    "## activation functions and their derivatives\n",
    "def logistic(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    t = np.exp(-x)\n",
    "    if mode == \"n\":\n",
    "        fx = 1/(1+t)\n",
    "    if mode == \"d\":\n",
    "        fx = t/(1+t)**2\n",
    "    return fx\n",
    "\n",
    "## define new functions here\n",
    "\n",
    "def tanh(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    p = np.exp(x)\n",
    "    n = np.exp(-x)\n",
    "    if mode == \"n\":\n",
    "        fx = p-n/(p+n)\n",
    "    if mode == \"d\":\n",
    "        fx = 1- (p-n/(p+n))**2\n",
    "    return fx\n",
    "\n",
    "def relu(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    if mode == \"n\":\n",
    "        fx = np.where(x < 0, 0, x)\n",
    "    if mode == \"d\":\n",
    "        fx = np.where(x < 0, 0, x)\n",
    "        fx = np.where(x >= 0, 1, x)\n",
    "    return fx\n",
    "\n",
    "def Linear(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    if mode == \"n\":\n",
    "        fx = x\n",
    "    if mode == \"d\":\n",
    "        fx = 1\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756a2c4-d0aa-406c-83e5-7cb7612118f8",
   "metadata": {},
   "source": [
    "#### Hyperparameters: number of layers, size of each layer, and activation function per layer\n",
    "<strong>Indication:</strong> The hyperparameters ```NL```, ```NpL``` and ```Nfx``` are very important throughout the code. They define the number of layers, the number of neurons per each layer, and the number of input features. Always make sure that the size of ```NpL``` and ```ActivFun``` is equal to ```NL```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ad18c0-1f28-4b53-a7ed-d7afc91d3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons per layer:  [3, 1]\n",
      "Number of neurons from the previous layer:  [2, 3]\n"
     ]
    }
   ],
   "source": [
    "## NL is the number of layers including the hidden layers AND the output layer\n",
    "NL = 2\n",
    "## NpL defines the number of neurones per each layer. The length of NpL should be exactly equal to NL\n",
    "NpL = [3, 1]\n",
    "## ActivFun defines the activation functions per each layer\n",
    "ActivFun = ['logistic', 'logistic']\n",
    "\n",
    "## Add the number of features x\n",
    "Nfx = 2\n",
    "## List of size of layers -1: important for automatic definition of parameters\n",
    "NpLm1 = [Nfx]\n",
    "for iL in np.arange(len(NpL)-1):\n",
    "    NpLm1.append(NpL[iL])\n",
    "print(\"Number of neurons per layer: \", NpL)\n",
    "print(\"Number of neurons from the previous layer: \", NpLm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534c71d-16a5-4983-91ca-a17eaf93d704",
   "metadata": {},
   "source": [
    "#### Initialization of parameters\n",
    "<strong>Task 2:</strong> Initialize the parameters (weights and biases) using the function ```np.random.rand``` (https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html). Convenient initial parameter values should be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87b7284-6901-4ebb-ad03-7a4f084b38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "## List of weights and biases\n",
    "Wts = []\n",
    "bias = []\n",
    "for iL in np.arange(len(NpL)):\n",
    "    ## random initialization\n",
    "    ## the initial parameters should be between -1 and 1\n",
    "    ## use the function np.random.rand to make random initializations (but these will be between 0 and 1)\n",
    "    WL = (np.random.rand(NpL[iL], NpLm1[iL])-0.5)*2\n",
    "    bL = (np.random.rand(NpL[iL])-0.5)*2\n",
    "    ## appending\n",
    "    Wts.append(WL)\n",
    "    bias.append(bL)\n",
    "print(Wts[0].shape)\n",
    "print(bias[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1489f56-3100-4d8d-b896-ec61e8fefd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Construct the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8ca49-503a-42b0-aff5-8ac97870d7f3",
   "metadata": {},
   "source": [
    "<strong>Task 3:</strong> Complete this function with the equations for each layer to compute the vectors $z_{L}$ and their activation $y_{L} = \\sigma(z_{L})$. The function should return a list ```z``` and a list ```y``` that have ```NpL``` arrays each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394fa3c3-b925-4139-94bf-d8926b19a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(x, NpL, Nfx, Wts, bias, ActivFun):\n",
    "    '''\n",
    "    This function computes the output of a neural network containing NL layers\n",
    "    where NL is the length of NpL. \n",
    "    o NpL contains the number of neurons per each hidden layer + the output layer. \n",
    "    o Nfx is the number of input features.\n",
    "    o Wts is a list that contains the 2D arrays of weights for each layer.\n",
    "    Each 2D array of weights has dimensions nL x nL-1, nL being the number of neurons\n",
    "    of current layer L, and nL-1 the number of neurons (or features) of layer L-1.\n",
    "    o bias is a list of 1D arrays of weights for each layer.\n",
    "    Each 1D array has dimensions nL x 1. When there are many data points (or members), say\n",
    "    n data points, the bias should be repeated using the function np.tile.\n",
    "    o ActivFun contains the name of the activation function for each layer.\n",
    "    '''\n",
    "    print('x shape:', x.shape)\n",
    "    print('NpL:', NpL)\n",
    "    print('Nfx:', Nfx)\n",
    "    print('Wts:', len(Wts))\n",
    "\n",
    "    n = x.shape[1]\n",
    "    yLm1 = x\n",
    "\n",
    "    print('n vaut:', n)\n",
    "    print('NL vaut:', len(NpL))\n",
    "    \n",
    "    ## z is a list that saves the zL arrays for each hidden and output layer\n",
    "    z = []\n",
    "    ## similarly, y is a list that saves the yL arrays. Specifically, yL[NL-1] contains the output.\n",
    "    y = []\n",
    "    for iL in np.arange(len(NpL)):\n",
    "        ## parameters\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "        ## multiplication\n",
    "        ## make sure that the operation is correct for n individual points.\n",
    "        ## the dimension of zL should be nL x n. Since bL is nL x 1, use np.tile to overcome this issue.\n",
    "        print('WL shape:', WL.shape)\n",
    "        print('yLm1 shape:', yLm1.shape)\n",
    "        print('bL shape:', bL.shape)\n",
    "        \n",
    "        zL = np.dot(WL, yLm1) + np.tile(bL,(n,1)).T\n",
    "        z.append(zL)\n",
    "\n",
    "        print('zL shape:', zL.shape)\n",
    "        print('bL shape:', bL.shape)\n",
    "        print(\"n\",n)\n",
    "\n",
    "        ## activation\n",
    "        ## to call a function given its name, use the function fx = globals()[\"fun_name\"]\n",
    "        sigma = globals()[ActivFun[iL]]\n",
    "        yL = sigma(zL)\n",
    "        y.append(yL)\n",
    "        ## move to next layer\n",
    "        yLm1 = yL\n",
    "    return y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5a04e-934c-4581-959a-80a248f88955",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Verify that the ANN works for a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974c4cfb-ec16-4965-bb56-4c58f0ed574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "## Input: 4 data points x 2 features (Nfx = 2)\n",
    "input_features = np.array([[0,0], \n",
    "                           [0,1], \n",
    "                           [1,0], \n",
    "                           [1,1]])\n",
    "print(input_features.shape)\n",
    "print(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114e5650-c1ab-45b5-964c-ae5fa86d1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Output: 4 data points x 1 feature\n",
    "target_output = np.array([[0], [1], [1], [1]])\n",
    "print(target_output.shape)\n",
    "print(target_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3007438-db04-41c0-b8ff-f8979d623d60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate outputs for the 4 individuals\n",
    "Run this cell to test your neural network. It should deliver an output ysim of shape (n,1), where n is the number of data points (NOT features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd5b6d4-42df-4369-b1cb-7c810591382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (2, 4)\n",
      "NpL: [3, 1]\n",
      "Nfx: 2\n",
      "Wts: 2\n",
      "n vaut: 4\n",
      "NL vaut: 2\n",
      "WL shape: (3, 2)\n",
      "yLm1 shape: (2, 4)\n",
      "bL shape: (3,)\n",
      "zL shape: (3, 4)\n",
      "bL shape: (3,)\n",
      "n 4\n",
      "WL shape: (1, 3)\n",
      "yLm1 shape: (3, 4)\n",
      "bL shape: (1,)\n",
      "zL shape: (1, 4)\n",
      "bL shape: (1,)\n",
      "n 4\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "## This where you can test your neural network\n",
    "x_in = input_features.T\n",
    "y,z = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "ysim = y[NL-1].T\n",
    "print(ysim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e5665-4ef6-4d0f-ac01-b433335d741b",
   "metadata": {},
   "source": [
    "#### Goodness of fit: MSE and RMSE\n",
    "<strong>Task 4:</strong> Complete the equation of MSE and RMSE to estimate the errors of the simulation output of the neural network. Use the matrix product to compute the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566beb62-ba89-4d96-9296-0cdb112551d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  [[0.34564041]]\n",
      "RMSE =  [[0.5879119]]\n"
     ]
    }
   ],
   "source": [
    "n = target_output.shape[0]\n",
    "MSE = np.dot((target_output - ysim).T,(target_output - ysim))/n\n",
    "print(\"MSE = \", MSE)\n",
    "\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(\"RMSE = \", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342e97d-0aa0-425d-a088-5d56c0e91205",
   "metadata": {},
   "source": [
    "## <strong>Part 2: Training a neural network</strong>\n",
    " In this part, a backpropagation algorithm is implemented to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60de4bf-e0f8-43e4-b46e-c19178980f89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Read the data, define the training and the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81668774-7c27-4b55-bb2d-56c847cb8dec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading the dataset\n",
    "Any machine learning task involves preparing/preprocessing the data to make it ready for digestion by the machine learning algorithm.\n",
    "\n",
    "At this stage, the data has a 2D shape (lines = individuals or data points, columns = features). The dimensions of the shape of the data help you define the structure of your neural network.\n",
    "\n",
    "The \"abalone_data.xlsx\" dataset that are used for this exercise can be downloaded from https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/. They contain measurements of the following properties of 4177 members of marine snails (<em>haliotis</em>):\n",
    "<ol>\n",
    "  <li><em>Type</em>: male (1), immature (0), female (-1); </li>\n",
    "  <li><em>LongestShell_mm</em>: longest shell measurement in mm; </li>\n",
    "  <li><em>Diameter_mm</em>: length perpendicular to LongestShell in mm;</li>\n",
    "  <li><em>Height_mm</em>: height of the member with meat in shell in mm;</li>\n",
    "  <li><em>WholeWeight_g</em>: mass of the whole abalone in g;</li>\n",
    "  <li><em>ShuckedWeight_g</em>: weight of meat in the abalone in g;</li>  \n",
    "  <li><em>VisceraWeight_g</em>: gut weight of the abalone (after bleeding) in g;</li>\n",
    "  <li><em>ShellWeight_g</em>: weight of the abalone after being dried in g;</li>\n",
    "  <li><em>Age_yr</em>: age of the abalone in years.</li>\n",
    "</ol> \n",
    " \n",
    "The objective of the exercise is to succeed at predicting the age of the abalone knowing its gender, form, and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042e30d-1c19-4ec6-9a06-cf99eb3998b8",
   "metadata": {},
   "source": [
    "<strong>Task 5:</strong> Complete the first instruction ```pd.read_excel``` to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa2512e-0bad-4597-99f8-f38ec666b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ddb6d\">\n",
       "  <caption>Few lines of the dataset :</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ddb6d_level0_col0\" class=\"col_heading level0 col0\" >Type</th>\n",
       "      <th id=\"T_ddb6d_level0_col1\" class=\"col_heading level0 col1\" >LongestShell_mm</th>\n",
       "      <th id=\"T_ddb6d_level0_col2\" class=\"col_heading level0 col2\" >Diameter_mm</th>\n",
       "      <th id=\"T_ddb6d_level0_col3\" class=\"col_heading level0 col3\" >Height_mm</th>\n",
       "      <th id=\"T_ddb6d_level0_col4\" class=\"col_heading level0 col4\" >WholeWeight_g</th>\n",
       "      <th id=\"T_ddb6d_level0_col5\" class=\"col_heading level0 col5\" >ShuckedWeight_g</th>\n",
       "      <th id=\"T_ddb6d_level0_col6\" class=\"col_heading level0 col6\" >VisceraWeight_g</th>\n",
       "      <th id=\"T_ddb6d_level0_col7\" class=\"col_heading level0 col7\" >ShellWeight_g</th>\n",
       "      <th id=\"T_ddb6d_level0_col8\" class=\"col_heading level0 col8\" >Age_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb6d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ddb6d_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
       "      <td id=\"T_ddb6d_row0_col1\" class=\"data row0 col1\" >0.46</td>\n",
       "      <td id=\"T_ddb6d_row0_col2\" class=\"data row0 col2\" >0.36</td>\n",
       "      <td id=\"T_ddb6d_row0_col3\" class=\"data row0 col3\" >0.10</td>\n",
       "      <td id=\"T_ddb6d_row0_col4\" class=\"data row0 col4\" >0.51</td>\n",
       "      <td id=\"T_ddb6d_row0_col5\" class=\"data row0 col5\" >0.22</td>\n",
       "      <td id=\"T_ddb6d_row0_col6\" class=\"data row0 col6\" >0.10</td>\n",
       "      <td id=\"T_ddb6d_row0_col7\" class=\"data row0 col7\" >0.15</td>\n",
       "      <td id=\"T_ddb6d_row0_col8\" class=\"data row0 col8\" >15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb6d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ddb6d_row1_col0\" class=\"data row1 col0\" >1.00</td>\n",
       "      <td id=\"T_ddb6d_row1_col1\" class=\"data row1 col1\" >0.35</td>\n",
       "      <td id=\"T_ddb6d_row1_col2\" class=\"data row1 col2\" >0.27</td>\n",
       "      <td id=\"T_ddb6d_row1_col3\" class=\"data row1 col3\" >0.09</td>\n",
       "      <td id=\"T_ddb6d_row1_col4\" class=\"data row1 col4\" >0.23</td>\n",
       "      <td id=\"T_ddb6d_row1_col5\" class=\"data row1 col5\" >0.10</td>\n",
       "      <td id=\"T_ddb6d_row1_col6\" class=\"data row1 col6\" >0.05</td>\n",
       "      <td id=\"T_ddb6d_row1_col7\" class=\"data row1 col7\" >0.07</td>\n",
       "      <td id=\"T_ddb6d_row1_col8\" class=\"data row1 col8\" >7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb6d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ddb6d_row2_col0\" class=\"data row2 col0\" >-1.00</td>\n",
       "      <td id=\"T_ddb6d_row2_col1\" class=\"data row2 col1\" >0.53</td>\n",
       "      <td id=\"T_ddb6d_row2_col2\" class=\"data row2 col2\" >0.42</td>\n",
       "      <td id=\"T_ddb6d_row2_col3\" class=\"data row2 col3\" >0.14</td>\n",
       "      <td id=\"T_ddb6d_row2_col4\" class=\"data row2 col4\" >0.68</td>\n",
       "      <td id=\"T_ddb6d_row2_col5\" class=\"data row2 col5\" >0.26</td>\n",
       "      <td id=\"T_ddb6d_row2_col6\" class=\"data row2 col6\" >0.14</td>\n",
       "      <td id=\"T_ddb6d_row2_col7\" class=\"data row2 col7\" >0.21</td>\n",
       "      <td id=\"T_ddb6d_row2_col8\" class=\"data row2 col8\" >9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb6d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ddb6d_row3_col0\" class=\"data row3 col0\" >1.00</td>\n",
       "      <td id=\"T_ddb6d_row3_col1\" class=\"data row3 col1\" >0.44</td>\n",
       "      <td id=\"T_ddb6d_row3_col2\" class=\"data row3 col2\" >0.36</td>\n",
       "      <td id=\"T_ddb6d_row3_col3\" class=\"data row3 col3\" >0.12</td>\n",
       "      <td id=\"T_ddb6d_row3_col4\" class=\"data row3 col4\" >0.52</td>\n",
       "      <td id=\"T_ddb6d_row3_col5\" class=\"data row3 col5\" >0.22</td>\n",
       "      <td id=\"T_ddb6d_row3_col6\" class=\"data row3 col6\" >0.11</td>\n",
       "      <td id=\"T_ddb6d_row3_col7\" class=\"data row3 col7\" >0.15</td>\n",
       "      <td id=\"T_ddb6d_row3_col8\" class=\"data row3 col8\" >10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddb6d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ddb6d_row4_col0\" class=\"data row4 col0\" >0.00</td>\n",
       "      <td id=\"T_ddb6d_row4_col1\" class=\"data row4 col1\" >0.33</td>\n",
       "      <td id=\"T_ddb6d_row4_col2\" class=\"data row4 col2\" >0.26</td>\n",
       "      <td id=\"T_ddb6d_row4_col3\" class=\"data row4 col3\" >0.08</td>\n",
       "      <td id=\"T_ddb6d_row4_col4\" class=\"data row4 col4\" >0.20</td>\n",
       "      <td id=\"T_ddb6d_row4_col5\" class=\"data row4 col5\" >0.09</td>\n",
       "      <td id=\"T_ddb6d_row4_col6\" class=\"data row4 col6\" >0.04</td>\n",
       "      <td id=\"T_ddb6d_row4_col7\" class=\"data row4 col7\" >0.06</td>\n",
       "      <td id=\"T_ddb6d_row4_col8\" class=\"data row4 col8\" >7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x127484ef430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6cbd8\">\n",
       "  <caption>Statistics of the dataset</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6cbd8_level0_col0\" class=\"col_heading level0 col0\" >Statistic</th>\n",
       "      <th id=\"T_6cbd8_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_6cbd8_level0_col2\" class=\"col_heading level0 col2\" >LongestShell_mm</th>\n",
       "      <th id=\"T_6cbd8_level0_col3\" class=\"col_heading level0 col3\" >Diameter_mm</th>\n",
       "      <th id=\"T_6cbd8_level0_col4\" class=\"col_heading level0 col4\" >Height_mm</th>\n",
       "      <th id=\"T_6cbd8_level0_col5\" class=\"col_heading level0 col5\" >WholeWeight_g</th>\n",
       "      <th id=\"T_6cbd8_level0_col6\" class=\"col_heading level0 col6\" >ShuckedWeight_g</th>\n",
       "      <th id=\"T_6cbd8_level0_col7\" class=\"col_heading level0 col7\" >VisceraWeight_g</th>\n",
       "      <th id=\"T_6cbd8_level0_col8\" class=\"col_heading level0 col8\" >ShellWeight_g</th>\n",
       "      <th id=\"T_6cbd8_level0_col9\" class=\"col_heading level0 col9\" >Age_yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6cbd8_row0_col0\" class=\"data row0 col0\" >mean</td>\n",
       "      <td id=\"T_6cbd8_row0_col1\" class=\"data row0 col1\" >0.052909</td>\n",
       "      <td id=\"T_6cbd8_row0_col2\" class=\"data row0 col2\" >0.523992</td>\n",
       "      <td id=\"T_6cbd8_row0_col3\" class=\"data row0 col3\" >0.407881</td>\n",
       "      <td id=\"T_6cbd8_row0_col4\" class=\"data row0 col4\" >0.139516</td>\n",
       "      <td id=\"T_6cbd8_row0_col5\" class=\"data row0 col5\" >0.828742</td>\n",
       "      <td id=\"T_6cbd8_row0_col6\" class=\"data row0 col6\" >0.359367</td>\n",
       "      <td id=\"T_6cbd8_row0_col7\" class=\"data row0 col7\" >0.180594</td>\n",
       "      <td id=\"T_6cbd8_row0_col8\" class=\"data row0 col8\" >0.238831</td>\n",
       "      <td id=\"T_6cbd8_row0_col9\" class=\"data row0 col9\" >9.933684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6cbd8_row1_col0\" class=\"data row1 col0\" >std</td>\n",
       "      <td id=\"T_6cbd8_row1_col1\" class=\"data row1 col1\" >0.822142</td>\n",
       "      <td id=\"T_6cbd8_row1_col2\" class=\"data row1 col2\" >0.120079</td>\n",
       "      <td id=\"T_6cbd8_row1_col3\" class=\"data row1 col3\" >0.099228</td>\n",
       "      <td id=\"T_6cbd8_row1_col4\" class=\"data row1 col4\" >0.041822</td>\n",
       "      <td id=\"T_6cbd8_row1_col5\" class=\"data row1 col5\" >0.490330</td>\n",
       "      <td id=\"T_6cbd8_row1_col6\" class=\"data row1 col6\" >0.221936</td>\n",
       "      <td id=\"T_6cbd8_row1_col7\" class=\"data row1 col7\" >0.109601</td>\n",
       "      <td id=\"T_6cbd8_row1_col8\" class=\"data row1 col8\" >0.139186</td>\n",
       "      <td id=\"T_6cbd8_row1_col9\" class=\"data row1 col9\" >3.223783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6cbd8_row2_col0\" class=\"data row2 col0\" >min</td>\n",
       "      <td id=\"T_6cbd8_row2_col1\" class=\"data row2 col1\" >-1.000000</td>\n",
       "      <td id=\"T_6cbd8_row2_col2\" class=\"data row2 col2\" >0.075000</td>\n",
       "      <td id=\"T_6cbd8_row2_col3\" class=\"data row2 col3\" >0.055000</td>\n",
       "      <td id=\"T_6cbd8_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "      <td id=\"T_6cbd8_row2_col5\" class=\"data row2 col5\" >0.002000</td>\n",
       "      <td id=\"T_6cbd8_row2_col6\" class=\"data row2 col6\" >0.001000</td>\n",
       "      <td id=\"T_6cbd8_row2_col7\" class=\"data row2 col7\" >0.000500</td>\n",
       "      <td id=\"T_6cbd8_row2_col8\" class=\"data row2 col8\" >0.001500</td>\n",
       "      <td id=\"T_6cbd8_row2_col9\" class=\"data row2 col9\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6cbd8_row3_col0\" class=\"data row3 col0\" >median</td>\n",
       "      <td id=\"T_6cbd8_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "      <td id=\"T_6cbd8_row3_col2\" class=\"data row3 col2\" >0.545000</td>\n",
       "      <td id=\"T_6cbd8_row3_col3\" class=\"data row3 col3\" >0.425000</td>\n",
       "      <td id=\"T_6cbd8_row3_col4\" class=\"data row3 col4\" >0.140000</td>\n",
       "      <td id=\"T_6cbd8_row3_col5\" class=\"data row3 col5\" >0.799500</td>\n",
       "      <td id=\"T_6cbd8_row3_col6\" class=\"data row3 col6\" >0.336000</td>\n",
       "      <td id=\"T_6cbd8_row3_col7\" class=\"data row3 col7\" >0.171000</td>\n",
       "      <td id=\"T_6cbd8_row3_col8\" class=\"data row3 col8\" >0.234000</td>\n",
       "      <td id=\"T_6cbd8_row3_col9\" class=\"data row3 col9\" >9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6cbd8_row4_col0\" class=\"data row4 col0\" >max</td>\n",
       "      <td id=\"T_6cbd8_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_6cbd8_row4_col2\" class=\"data row4 col2\" >0.815000</td>\n",
       "      <td id=\"T_6cbd8_row4_col3\" class=\"data row4 col3\" >0.650000</td>\n",
       "      <td id=\"T_6cbd8_row4_col4\" class=\"data row4 col4\" >1.130000</td>\n",
       "      <td id=\"T_6cbd8_row4_col5\" class=\"data row4 col5\" >2.825500</td>\n",
       "      <td id=\"T_6cbd8_row4_col6\" class=\"data row4 col6\" >1.488000</td>\n",
       "      <td id=\"T_6cbd8_row4_col7\" class=\"data row4 col7\" >0.760000</td>\n",
       "      <td id=\"T_6cbd8_row4_col8\" class=\"data row4 col8\" >1.005000</td>\n",
       "      <td id=\"T_6cbd8_row4_col9\" class=\"data row4 col9\" >29.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12748540a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_excel(\"abalone_data.xlsx\", sheet_name=\"Sheet1\")\n",
    "display(df.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "dfsum = datasum(df)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "display(dfsum.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa151f77-adf9-44d7-abcf-dd0138ac06d9",
   "metadata": {},
   "source": [
    "#### Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b7b25-91fa-42b1-8199-668c2197ca6f",
   "metadata": {},
   "source": [
    "<strong>Task 6:</strong> After reading the dataset, complete the instructions using ```df.sample``` and ```df.drop``` to split the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870a38bc-be46-4b48-8b6d-99e0793f23e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percTrain 0.7000239406272444\n",
      "percTest 0.29997605937275557\n"
     ]
    }
   ],
   "source": [
    "## percentage of data to be used for training\n",
    "percTrain = 0.7\n",
    "\n",
    "## index of training and test\n",
    "#trainindex = np.random.rand(len(df)) < percTrain\n",
    "dftrain = df.sample(frac = percTrain, axis = 0)\n",
    "dftest = df.drop(dftrain.index)\n",
    "\n",
    "print(\"percTrain\",len(dftrain)/len(df))\n",
    "print(\"percTest\",len(dftest)/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79be5dc-1211-4f43-a0d5-4c003f50cf03",
   "metadata": {},
   "source": [
    "#### Defining features and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a21213-5d5a-48eb-9ccf-0c0b4702365f",
   "metadata": {},
   "source": [
    "<strong>Task 7:</strong> Complete the instructions to define the target/output/dependent variable (age of the abalone) and the attributes/input/independent variables (remaining variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2686eaf-5431-45fe-9c35-a4423890b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain.head       Type  LongestShell_mm  Diameter_mm  Height_mm  WholeWeight_g  \\\n",
      "1208     1             0.76         0.58       0.20           2.01   \n",
      "3059    -1             0.62         0.52       0.15           1.16   \n",
      "3018     0             0.44         0.32       0.10           0.39   \n",
      "1104     0             0.51         0.41       0.12           0.68   \n",
      "2894     0             0.54         0.42       0.13           0.82   \n",
      "\n",
      "      ShuckedWeight_g  VisceraWeight_g  ShellWeight_g  \n",
      "1208             0.83             0.40           0.59  \n",
      "3059             0.49             0.26           0.35  \n",
      "3018             0.18             0.07           0.12  \n",
      "1104             0.35             0.14           0.18  \n",
      "2894             0.37             0.14           0.25  \n",
      "Shape of original data :  (4177, 9)\n",
      "xtrain :  (2924, 8) ytrain :  (2924,)\n",
      "xtest  :  (1253, 8) ytest  :  (1253,)\n"
     ]
    }
   ],
   "source": [
    "ytrain = dftrain.Age_yr\n",
    "xtrain = dftrain.drop(\"Age_yr\", axis = 1)\n",
    "\n",
    "print(\"xtrain.head\" ,xtrain.head())\n",
    "\n",
    "ytest = dftest.Age_yr\n",
    "xtest = dftest.drop(\"Age_yr\", axis = 1)\n",
    " \n",
    "## printing some information\n",
    "print('Shape of original data : ', df.shape)\n",
    "print('xtrain : ',xtrain.shape, 'ytrain : ',ytrain.shape)\n",
    "print('xtest  : ',xtest.shape,  'ytest  : ',ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe48c69-9b0f-425f-9bde-f197b6985645",
   "metadata": {},
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11566c-b3bd-402f-a72e-9edac0bcb137",
   "metadata": {},
   "source": [
    "<strong>Task 8:</strong> Estimate the mean and standard deviation for each column/feature from the <ins>train</ins> dataset and use them to scale both the train and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da817fc5-9145-4ce1-8e01-e681db1ee91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimate the mean and the standard deviation from the train dataset\n",
    "xmean = xtrain.mean()\n",
    "xstd = xtrain.std()\n",
    "## scaling\n",
    "xtrain_scl = (xtrain - xmean)/xstd\n",
    "xtest_scl = (xtest - xmean)/xstd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20de3846-0fc6-4bed-bc0b-e3f24db3398a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e3f9b\">\n",
       "  <caption>Statistics of the dataset - before scaling</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e3f9b_level0_col0\" class=\"col_heading level0 col0\" >Statistic</th>\n",
       "      <th id=\"T_e3f9b_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_e3f9b_level0_col2\" class=\"col_heading level0 col2\" >LongestShell_mm</th>\n",
       "      <th id=\"T_e3f9b_level0_col3\" class=\"col_heading level0 col3\" >Diameter_mm</th>\n",
       "      <th id=\"T_e3f9b_level0_col4\" class=\"col_heading level0 col4\" >Height_mm</th>\n",
       "      <th id=\"T_e3f9b_level0_col5\" class=\"col_heading level0 col5\" >WholeWeight_g</th>\n",
       "      <th id=\"T_e3f9b_level0_col6\" class=\"col_heading level0 col6\" >ShuckedWeight_g</th>\n",
       "      <th id=\"T_e3f9b_level0_col7\" class=\"col_heading level0 col7\" >VisceraWeight_g</th>\n",
       "      <th id=\"T_e3f9b_level0_col8\" class=\"col_heading level0 col8\" >ShellWeight_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e3f9b_row0_col0\" class=\"data row0 col0\" >mean</td>\n",
       "      <td id=\"T_e3f9b_row0_col1\" class=\"data row0 col1\" >0.054036</td>\n",
       "      <td id=\"T_e3f9b_row0_col2\" class=\"data row0 col2\" >0.524449</td>\n",
       "      <td id=\"T_e3f9b_row0_col3\" class=\"data row0 col3\" >0.408386</td>\n",
       "      <td id=\"T_e3f9b_row0_col4\" class=\"data row0 col4\" >0.139342</td>\n",
       "      <td id=\"T_e3f9b_row0_col5\" class=\"data row0 col5\" >0.830281</td>\n",
       "      <td id=\"T_e3f9b_row0_col6\" class=\"data row0 col6\" >0.358978</td>\n",
       "      <td id=\"T_e3f9b_row0_col7\" class=\"data row0 col7\" >0.181127</td>\n",
       "      <td id=\"T_e3f9b_row0_col8\" class=\"data row0 col8\" >0.239426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e3f9b_row1_col0\" class=\"data row1 col0\" >std</td>\n",
       "      <td id=\"T_e3f9b_row1_col1\" class=\"data row1 col1\" >0.827755</td>\n",
       "      <td id=\"T_e3f9b_row1_col2\" class=\"data row1 col2\" >0.119447</td>\n",
       "      <td id=\"T_e3f9b_row1_col3\" class=\"data row1 col3\" >0.098758</td>\n",
       "      <td id=\"T_e3f9b_row1_col4\" class=\"data row1 col4\" >0.038408</td>\n",
       "      <td id=\"T_e3f9b_row1_col5\" class=\"data row1 col5\" >0.489381</td>\n",
       "      <td id=\"T_e3f9b_row1_col6\" class=\"data row1 col6\" >0.220078</td>\n",
       "      <td id=\"T_e3f9b_row1_col7\" class=\"data row1 col7\" >0.109551</td>\n",
       "      <td id=\"T_e3f9b_row1_col8\" class=\"data row1 col8\" >0.139964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e3f9b_row2_col0\" class=\"data row2 col0\" >min</td>\n",
       "      <td id=\"T_e3f9b_row2_col1\" class=\"data row2 col1\" >-1.000000</td>\n",
       "      <td id=\"T_e3f9b_row2_col2\" class=\"data row2 col2\" >0.075000</td>\n",
       "      <td id=\"T_e3f9b_row2_col3\" class=\"data row2 col3\" >0.055000</td>\n",
       "      <td id=\"T_e3f9b_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "      <td id=\"T_e3f9b_row2_col5\" class=\"data row2 col5\" >0.002000</td>\n",
       "      <td id=\"T_e3f9b_row2_col6\" class=\"data row2 col6\" >0.001000</td>\n",
       "      <td id=\"T_e3f9b_row2_col7\" class=\"data row2 col7\" >0.000500</td>\n",
       "      <td id=\"T_e3f9b_row2_col8\" class=\"data row2 col8\" >0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e3f9b_row3_col0\" class=\"data row3 col0\" >median</td>\n",
       "      <td id=\"T_e3f9b_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "      <td id=\"T_e3f9b_row3_col2\" class=\"data row3 col2\" >0.545000</td>\n",
       "      <td id=\"T_e3f9b_row3_col3\" class=\"data row3 col3\" >0.425000</td>\n",
       "      <td id=\"T_e3f9b_row3_col4\" class=\"data row3 col4\" >0.140000</td>\n",
       "      <td id=\"T_e3f9b_row3_col5\" class=\"data row3 col5\" >0.805000</td>\n",
       "      <td id=\"T_e3f9b_row3_col6\" class=\"data row3 col6\" >0.339250</td>\n",
       "      <td id=\"T_e3f9b_row3_col7\" class=\"data row3 col7\" >0.170750</td>\n",
       "      <td id=\"T_e3f9b_row3_col8\" class=\"data row3 col8\" >0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e3f9b_row4_col0\" class=\"data row4 col0\" >max</td>\n",
       "      <td id=\"T_e3f9b_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_e3f9b_row4_col2\" class=\"data row4 col2\" >0.815000</td>\n",
       "      <td id=\"T_e3f9b_row4_col3\" class=\"data row4 col3\" >0.650000</td>\n",
       "      <td id=\"T_e3f9b_row4_col4\" class=\"data row4 col4\" >0.250000</td>\n",
       "      <td id=\"T_e3f9b_row4_col5\" class=\"data row4 col5\" >2.825500</td>\n",
       "      <td id=\"T_e3f9b_row4_col6\" class=\"data row4 col6\" >1.348500</td>\n",
       "      <td id=\"T_e3f9b_row4_col7\" class=\"data row4 col7\" >0.760000</td>\n",
       "      <td id=\"T_e3f9b_row4_col8\" class=\"data row4 col8\" >1.005000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1274484b1c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_044a9\">\n",
       "  <caption>Statistics of the dataset - after scaling</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_044a9_level0_col0\" class=\"col_heading level0 col0\" >Statistic</th>\n",
       "      <th id=\"T_044a9_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_044a9_level0_col2\" class=\"col_heading level0 col2\" >LongestShell_mm</th>\n",
       "      <th id=\"T_044a9_level0_col3\" class=\"col_heading level0 col3\" >Diameter_mm</th>\n",
       "      <th id=\"T_044a9_level0_col4\" class=\"col_heading level0 col4\" >Height_mm</th>\n",
       "      <th id=\"T_044a9_level0_col5\" class=\"col_heading level0 col5\" >WholeWeight_g</th>\n",
       "      <th id=\"T_044a9_level0_col6\" class=\"col_heading level0 col6\" >ShuckedWeight_g</th>\n",
       "      <th id=\"T_044a9_level0_col7\" class=\"col_heading level0 col7\" >VisceraWeight_g</th>\n",
       "      <th id=\"T_044a9_level0_col8\" class=\"col_heading level0 col8\" >ShellWeight_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_044a9_row0_col0\" class=\"data row0 col0\" >mean</td>\n",
       "      <td id=\"T_044a9_row0_col1\" class=\"data row0 col1\" >-0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col3\" class=\"data row0 col3\" >-0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col4\" class=\"data row0 col4\" >-0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col6\" class=\"data row0 col6\" >-0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col7\" class=\"data row0 col7\" >-0.000000</td>\n",
       "      <td id=\"T_044a9_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_044a9_row1_col0\" class=\"data row1 col0\" >std</td>\n",
       "      <td id=\"T_044a9_row1_col1\" class=\"data row1 col1\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col2\" class=\"data row1 col2\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col3\" class=\"data row1 col3\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col4\" class=\"data row1 col4\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col5\" class=\"data row1 col5\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col6\" class=\"data row1 col6\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col7\" class=\"data row1 col7\" >0.999829</td>\n",
       "      <td id=\"T_044a9_row1_col8\" class=\"data row1 col8\" >0.999829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_044a9_row2_col0\" class=\"data row2 col0\" >min</td>\n",
       "      <td id=\"T_044a9_row2_col1\" class=\"data row2 col1\" >-1.273149</td>\n",
       "      <td id=\"T_044a9_row2_col2\" class=\"data row2 col2\" >-3.762104</td>\n",
       "      <td id=\"T_044a9_row2_col3\" class=\"data row2 col3\" >-3.577704</td>\n",
       "      <td id=\"T_044a9_row2_col4\" class=\"data row2 col4\" >-3.627314</td>\n",
       "      <td id=\"T_044a9_row2_col5\" class=\"data row2 col5\" >-1.692218</td>\n",
       "      <td id=\"T_044a9_row2_col6\" class=\"data row2 col6\" >-1.626321</td>\n",
       "      <td id=\"T_044a9_row2_col7\" class=\"data row2 col7\" >-1.648508</td>\n",
       "      <td id=\"T_044a9_row2_col8\" class=\"data row2 col8\" >-1.699621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_044a9_row3_col0\" class=\"data row3 col0\" >median</td>\n",
       "      <td id=\"T_044a9_row3_col1\" class=\"data row3 col1\" >-0.065268</td>\n",
       "      <td id=\"T_044a9_row3_col2\" class=\"data row3 col2\" >0.172018</td>\n",
       "      <td id=\"T_044a9_row3_col3\" class=\"data row3 col3\" >0.168204</td>\n",
       "      <td id=\"T_044a9_row3_col4\" class=\"data row3 col4\" >0.017138</td>\n",
       "      <td id=\"T_044a9_row3_col5\" class=\"data row3 col5\" >-0.051651</td>\n",
       "      <td id=\"T_044a9_row3_col6\" class=\"data row3 col6\" >-0.089628</td>\n",
       "      <td id=\"T_044a9_row3_col7\" class=\"data row3 col7\" >-0.094709</td>\n",
       "      <td id=\"T_044a9_row3_col8\" class=\"data row3 col8\" >-0.031619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_044a9_row4_col0\" class=\"data row4 col0\" >max</td>\n",
       "      <td id=\"T_044a9_row4_col1\" class=\"data row4 col1\" >1.142612</td>\n",
       "      <td id=\"T_044a9_row4_col2\" class=\"data row4 col2\" >2.432046</td>\n",
       "      <td id=\"T_044a9_row4_col3\" class=\"data row4 col3\" >2.446121</td>\n",
       "      <td id=\"T_044a9_row4_col4\" class=\"data row4 col4\" >2.880636</td>\n",
       "      <td id=\"T_044a9_row4_col5\" class=\"data row4 col5\" >4.076326</td>\n",
       "      <td id=\"T_044a9_row4_col6\" class=\"data row4 col6\" >4.495464</td>\n",
       "      <td id=\"T_044a9_row4_col7\" class=\"data row4 col7\" >5.283126</td>\n",
       "      <td id=\"T_044a9_row4_col8\" class=\"data row4 col8\" >5.468859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1272d8a69b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_summary = datasum(xtrain)\n",
    "x_test_summary = datasum(xtest)\n",
    "x_train_scl_summary = datasum(xtrain_scl)\n",
    "x_test_scl_summary = datasum(xtest_scl)\n",
    "## convert to arrays\n",
    "xtrain_scl, ytrain = np.array(xtrain_scl), np.array(ytrain)\n",
    "xtest_scl, ytest = np.array(xtest_scl), np.array(ytest)\n",
    "## print the dataset before and after scaling\n",
    "display(x_train_summary.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset - before scaling\"))\n",
    "display(x_train_scl_summary.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset - after scaling\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa1da9-2f7c-42d9-9545-a82495751e05",
   "metadata": {},
   "source": [
    "### Step 2: Build the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a2149-4fb3-4ddb-adb7-2078324fecbe",
   "metadata": {},
   "source": [
    "<strong>Task 9:</strong> Complete the following function to train the neural network using the gradient descent method based on backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe91295f-036a-4c83-9075-2498dfaa53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_backpro(x, ytrue, NpL, Nfx, Wts, bias, ActivFun, lr):\n",
    "    '''\n",
    "    o Shape of x: Nfx * n, where n is the number of data points, and Nfx the number of features\n",
    "    o Shape of ytrue: n * 1, where n is the number of data points.\n",
    "    \n",
    "    '''\n",
    "    print(\"x\",x.shape)\n",
    "    print(\"ytrue\",ytrue.shape)\n",
    "    \n",
    "    ## step 1: feed forward\n",
    "    n = x.shape[1]\n",
    "    yLm1 = x\n",
    "    z = []\n",
    "    y = []\n",
    "    for iL in np.arange(len(NpL)):\n",
    "        ## get the parameters for the current layer\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "        ## estimate zL from yLm1\n",
    "        zL = np.dot(WL, yLm1) + np.tile(bL,(n,1)).T\n",
    "        z.append(zL)\n",
    "        print(\"zL\",zL.shape)\n",
    "\n",
    "        ## estimate yL from zL\n",
    "        sigma = globals()[ActivFun[iL]]\n",
    "        print(\"sigma\",sigma)\n",
    "        yL = sigma(zL, mode = \"n\")\n",
    "        y.append(yL)\n",
    "\n",
    "        ## update yLm1\n",
    "        yLm1 = yL\n",
    "\n",
    "    ## step 2: backpropagation\n",
    "    ytrue = ytrue.T\n",
    "    dJ_dy = 2*(yL - ytrue)                                          ####### crochets en trop??\n",
    "\n",
    "    for iL in reversed(np.arange(len(NpL))):\n",
    "        print(\"iL\",iL)\n",
    "        ## getting zL of the current layer\n",
    "        zL = z[iL]\n",
    "\n",
    "        ## estimating dJ_dz from dJ_dy\n",
    "        sigma = globals()[ActivFun[iL]]\n",
    "        dJ_dz = dJ_dy * sigma(zL, mode = \"d\")\n",
    "\n",
    "        ## getting the parameters of current layer\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "       \n",
    "        ## estimating dJ_dW from dJ_dz\n",
    "        ## dJ_dz : (nL x n)\n",
    "        ## yLm1 : (nL-1 x n)\n",
    "        ## getting yL-1\n",
    "        if iL == 0:\n",
    "            yLm1 = x\n",
    "        else:\n",
    "            yLm1 = y[iL-1]\n",
    "\n",
    "        dJ_dW = np.dot(dJ_dz, yLm1.T)\n",
    "        \n",
    "        print(\"dJ_dz\",dJ_dz.shape, \"nL x n\")\n",
    "        print(\"yLm1\",yLm1.shape, \"nL-1 x n\")\n",
    "\n",
    "        # estimating dJ_db from dJ_dz\n",
    "        # J_db : nL x 1\n",
    "        # dJ_dz : nL x n\n",
    "        dJ_db = np.dot(dJ_dz, np.ones((n,1)))\n",
    "\n",
    "        print(\"dJ_db\",dJ_db.shape, \"nL x 1\")\n",
    "        print(\"dJ_dz\",dJ_dz.shape, \"nL x n\")\n",
    "\n",
    "        ## backpropagating the gradient from layer L to layer L-1\n",
    "        ## WL : nL x nL-1\n",
    "        ## dJ_dz : nL x n\n",
    "        ## dJ_dy (L-1) : nL-1 x n\n",
    "        dJ_dy = np.dot(WL.T, dJ_dz)\n",
    "\n",
    "        print(\"WL\",WL.shape, \"nL x nL-1\")\n",
    "        print(\"dJ_dz\",dJ_dz.shape, \"nL x n\")\n",
    "        print(\"dJ_dy\",dJ_dy.shape, \"nL-1 x n\")\n",
    "        print(\"dJ_dW\",dJ_dW.shape, \"nL x nL-1\")\n",
    "        print(\"bL\",bL.shape, \"nL x 1\",\"avant\")\n",
    "        ## Updating the parameters\n",
    "        print(type(np.multiply(dJ_db,lr)))\n",
    "\n",
    "        WL = WL - np.multiply(dJ_dW,lr)\n",
    "        bL = bL - np.multiply(dJ_db,lr).T\n",
    "        print(\"WL\",WL.shape, \"nL x nL-1\")\n",
    "        print(\"bL\",bL.shape, \"nL x 1\",\"apres\")\n",
    "        Wts[iL] = WL\n",
    "        bias[iL] = bL\n",
    "    return Wts, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3927e-e855-4dda-9272-c4453f22c6a9",
   "metadata": {},
   "source": [
    "<strong>Task 10:</strong> Complete the initialization of parameters (exactly the same as in part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d082f3a-ec71-4502-95af-6e52d8c4412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons per layer:  [6, 1]\n",
      "Number of neurons from the previous layer:  [8, 6]\n"
     ]
    }
   ],
   "source": [
    "## NL is the number of layers including the hidden layers AND the output layer\n",
    "NL = 2\n",
    "## NpL defines the number of neurones per each layer. The length of NpL should be exactly equal to NL\n",
    "NpL = [6,1]\n",
    "## Add the number of features x\n",
    "Nfx = 8\n",
    "## ActivFun defines the activation functions per each layer\n",
    "ActivFun = ['logistic', 'relu']\n",
    "\n",
    "## List of size of layers -1: important for automatic definition of parameters\n",
    "NpLm1 = [Nfx]\n",
    "for iL in np.arange(len(NpL)-1):\n",
    "    NpLm1.append(NpL[iL])\n",
    "print(\"Number of neurons per layer: \", NpL)\n",
    "print(\"Number of neurons from the previous layer: \", NpLm1)\n",
    "\n",
    "## Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "## Number of epochs\n",
    "epochs = 50\n",
    "\n",
    "## List of weights and biases\n",
    "Wts = []\n",
    "bias = []\n",
    "for iL in np.arange(len(NpL)):\n",
    "    ## random initialization\n",
    "    WL = (np.random.rand(NpL[iL], NpLm1[iL])-0.5)*2\n",
    "    bL = (np.random.rand(NpL[iL])-0.5)*2\n",
    "    ## appending\n",
    "    Wts.append(WL)\n",
    "    bias.append(bL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75e783-8bbc-4fdd-85ca-45edd9a9e8e0",
   "metadata": {},
   "source": [
    "<strong>Task 11:</strong> Complete the following lines in order to keep track of (1) epochs, (2) training error (MSE), and (3) test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a36fc6-aeec-4a24-8318-48283781d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001   Number of epochs:  50\n",
      "x (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1,) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (6,) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[0.99995624, 0.94476587, 0.99500418, ..., 0.99652445, 0.99996144,\n",
      "        0.99981677],\n",
      "       [0.56208173, 0.43923781, 0.76311525, ..., 0.65822527, 0.65939771,\n",
      "        0.70330876],\n",
      "       [1.        , 0.99999881, 0.11787903, ..., 0.99852402, 0.99999994,\n",
      "        1.        ],\n",
      "       [0.99999928, 0.99987757, 0.96460037, ..., 0.99917682, 0.99996577,\n",
      "        0.99999974],\n",
      "       [0.55350795, 0.57020569, 0.18478925, ..., 0.29138708, 0.27505522,\n",
      "        0.78448198],\n",
      "       [0.91301599, 0.99504373, 0.99999401, ..., 0.99995738, 0.99778109,\n",
      "        0.97310517]]), array([[165.72956119, 163.87741292, 135.53006679, ..., 162.19587579,\n",
      "        161.8571645 , 176.48789953]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00001/00050 Training error: 0.17  Test error: 0.41x (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[1.05633750e-246, 0.00000000e+000, 1.53172985e-022, ...,\n",
      "        5.55429487e-150, 3.67935545e-073, 8.87174723e-302],\n",
      "       [0.00000000e+000, 0.00000000e+000, 1.00000000e+000, ...,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "       [1.00000000e+000, 1.00000000e+000, 0.00000000e+000, ...,\n",
      "        0.00000000e+000, 1.00000000e+000, 1.00000000e+000],\n",
      "       [1.00000000e+000, 1.00000000e+000, 0.00000000e+000, ...,\n",
      "        0.00000000e+000, 1.00000000e+000, 1.00000000e+000],\n",
      "       [0.00000000e+000, 0.00000000e+000, 1.00000000e+000, ...,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "       [0.00000000e+000, 0.00000000e+000, 1.00000000e+000, ...,\n",
      "        4.48359009e-022, 0.00000000e+000, 0.00000000e+000]]), array([[0., 0., 0., ..., 0., 0., 0.]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00002/00050 Training error: 0.19  Test error: 0.45x (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_13112\\3287838991.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  t = np.exp(-x)\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_13112\\3287838991.py:34: RuntimeWarning: overflow encountered in square\n",
      "  fx = t/(1+t)**2\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_13112\\3287838991.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  fx = t/(1+t)**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00003/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00004/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00005/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00006/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00007/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00008/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00009/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00010/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00011/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00012/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00013/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00014/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00015/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00016/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00017/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00018/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00019/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00020/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00021/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00022/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00023/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00024/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00025/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00026/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00027/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00028/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00029/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00030/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00031/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00032/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00033/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00034/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00035/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00036/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00037/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00038/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00039/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00040/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00041/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00042/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00043/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00044/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00045/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00046/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00047/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00048/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00049/00050 Training error: nan  Test error: nanx (8, 2924)\n",
      "ytrue (2924,)\n",
      "zL (6, 2924)\n",
      "sigma <function logistic at 0x00000127452BB370>\n",
      "zL (1, 2924)\n",
      "sigma <function relu at 0x0000012745CAA290>\n",
      "iL 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "yLm1 (6, 2924) nL-1 x n\n",
      "dJ_db (1, 1) nL x 1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "WL (1, 6) nL x nL-1\n",
      "dJ_dz (1, 2924) nL x n\n",
      "dJ_dy (6, 2924) nL-1 x n\n",
      "dJ_dW (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (1, 6) nL x nL-1\n",
      "bL (1, 1) nL x 1 apres\n",
      "iL 0\n",
      "dJ_dz (6, 2924) nL x n\n",
      "yLm1 (8, 2924) nL-1 x n\n",
      "dJ_db (6, 1) nL x 1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "WL (6, 8) nL x nL-1\n",
      "dJ_dz (6, 2924) nL x n\n",
      "dJ_dy (8, 2924) nL-1 x n\n",
      "dJ_dW (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 avant\n",
      "<class 'numpy.ndarray'>\n",
      "WL (6, 8) nL x nL-1\n",
      "bL (1, 6) nL x 1 apres\n",
      "Wts[0] (6, 8)\n",
      "Wts[1] (1, 6)\n",
      "x_in (8, 2924)\n",
      "x shape: (8, 2924)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 2924\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 2924)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 2924)\n",
      "bL shape: (1, 6)\n",
      "n 2924\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 2924)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 2924)\n",
      "bL shape: (1, 1)\n",
      "n 2924\n",
      "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan]])]\n",
      "ytrainsim 2\n",
      "ytrain (2924,)\n",
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n",
      "Epoch: 00050/00050 Training error: nan  Test error: nan"
     ]
    }
   ],
   "source": [
    "print(\"Learning rate: \", lr, \"  Number of epochs: \", epochs)\n",
    "MSEtrain = np.array([])\n",
    "MSEtest = np.array([])\n",
    "epoch = np.array([])\n",
    "sys.stdout.write('\\r')\n",
    "for iepoch in np.arange(epochs):\n",
    "    epoch = np.append(epoch, iepoch)\n",
    "    ## train the neural network\n",
    "    x_in = xtrain_scl.T \n",
    "    Wts, bias = ANN_backpro(x = x_in, ytrue = ytrain, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun, lr = lr)\n",
    "\n",
    "    print(\"Wts[0]\", Wts[0].shape)\n",
    "    print(\"Wts[1]\", Wts[1].shape)\n",
    "\n",
    "    ## estimate the MSE for the train dataset\n",
    "    x_in = xtrain_scl.T\n",
    "    print(\"x_in\",x_in.shape)\n",
    "    yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "    ytrainsim = yout\n",
    "    print(yout)\n",
    "    ntrain = ytrain.shape[0]\n",
    "    print(\"ytrainsim\",len(ytrainsim))\n",
    "    print(\"ytrain\",ytrain.shape)\n",
    "    Error_train = np.dot((ytrain - ytrainsim[0]).T,(ytrain - ytrainsim[0]))/ntrain\n",
    "    \n",
    "    ## estimate the MSE for the test dataset\n",
    "    x_in = xtest_scl.T\n",
    "    yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "    ytestsim = yout\n",
    "    ntest = ytest.shape[0]\n",
    "    Error_test = np.dot((ytest - ytestsim[0]).T,(ytest - ytestsim[0]))/ntest\n",
    "    \n",
    "    ## keeping track of the errors\n",
    "    MSEtrain = np.append(MSEtrain, Error_train[0,0])\n",
    "    MSEtest = np.append(MSEtest, Error_test[0,0]) \n",
    "    \n",
    "    ## print the evolution\n",
    "    sys.stdout.write('\\r' \"Epoch: \" + str(int(iepoch + 1)).rjust(5,'0') + \"/\"\n",
    "                    + str(int(epochs)).rjust(5,'0') + \" \" +\n",
    "                    \"Training error: \" + str(round(Error_train[0,0],2)) + \n",
    "                    \"  Test error: \" + str(round(Error_test[0,0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab8eb3-e3ca-4573-9041-347e5ab30ce1",
   "metadata": {},
   "source": [
    "Now, we can see the evolution of the training and test errors across the epochs. Note that only the RMSE (in years) is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a917284a-2611-425a-93f6-44e6a4609b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAHkCAYAAAB2aW3RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAACGKUlEQVR4nOzdd1xV5R8H8M9lXZQ9VJQtCm4Z4sSJmlkhAuJIE0tzRo7MlWlurZ/lTs3UylI0cefeKxTcFS5UxMGSIQIyzu8P4saVC97DvRe48nm/XryE85zznO+hJ+XDOed5JIIgCCAiIiIiIiKto1PRBRAREREREVHZMNARERERERFpKQY6IiIiIiIiLcVAR0REREREpKUY6IiIiIiIiLQUAx0REREREZGWYqAjIiIiIiLSUgx0REREREREWoqBjoiIiIiISEsx0BERkcqcnJwgkUjkPqRSKRwcHNC3b1+cOnWqokuUmTlzJiQSCWbOnCm3fcOGDZBIJAgJCdF4Dffu3YNEIoGTk5PGz0VERG82BjoiIlKbdu3aYfDgwRg8eDDefvtt5OfnIywsDB07dsTixYsrurxyUxhw7927V9GlEBHRG06vogsgIqI3x9ChQ+XucGVlZWH48OH46aef8Pnnn+Pdd9+Fq6trxRVYit69e6N169YwMzPT+LlsbW3x999/Q19fX+PnIiKiNxvv0BERkcYYGhpixYoVMDIyQl5eHrZv317RJZXIzMwMDRo0QO3atTV+Ln19fTRo0AAuLi4aPxcREb3ZGOiIiEijjI2N4ebmBgByjyAWvmsHAOvXr0ebNm1gZmZW7FHFR48eYfz48WjYsCGqV68OExMTeHt7Y/ny5cjNzVV4zszMTMycORP169eHVCpF7dq1MXjwYDx48KDEOl/3Dl1cXBwmTpyIpk2bwsTEBEZGRnB1dUVISAjOnj0r18f9+/cBAM7OznLvFR4/flz2fSjtHbqHDx/ik08+Qf369WFoaAgzMzO0a9cOq1evRl5eXqm1Z2RkYMqUKahXrx6kUilsbGwwePBgxMXFlXjtpXn27BlmzJgBd3d3mJiYoHr16mjatCnmzJmDFy9eFNu/6DuKDx48wEcffQR7e3vo6+vLvrchISGQSCTYsGEDrl+/jr59+6J27drQ1dWVe7cxOTkZU6dORePGjWX/7b28vLBo0SJkZmYWO/fx48chkUjQqVMnvHjxAl9++aVs3PB9RSJ6U/GRSyIi0ri0tDQAgFQqLdb2ySefYOXKlWjbti3eeecd3L17Vxb0Tp48CX9/fzx79gxOTk7o1q0bsrOzERERgU8++QS7d+/Gnj175B5dfPHiBXx9fXH+/HkYGRmhe/fuqFatGg4cOIC9e/finXfeEV3/kSNHEBQUhJSUFNSsWRO+vr4wMDDAvXv38OuvvwIA2rZti3r16mHw4MHYtm0bMjIyEBgYCGNjY1k/NjY2rz3XhQsX0KNHDyQnJ8PBwQH+/v5ITU3F8ePHcfbsWYSHh2PXrl0wMDAodmxqairatm2LBw8eoH379mjSpAnOnTuHn376CSdOnMCVK1dEPVL6119/oUePHoiNjUXt2rXh4+MDfX19REREYPr06fj9999x/PhxhX3eunULHh4eMDAwQLt27SAIAqytreX2OXv2LEaMGIHatWujQ4cOyMzMhImJCQDg7t276NKlC+7fv48aNWqgZ8+eyMnJwbFjxzBp0iRs2bIFhw8fhoWFRbFzZ2VloVOnTvjrr7/QoUMHNG/eHElJSUpfNxGRVhGIiIhU5OjoKAAQ1q9fX6ztypUrgo6OjgBA+PHHH2XbAQgABFNTU+HcuXPFjnv8+LFgZWUlSCQSYeXKlUJeXp6sLTExUejSpYsAQPjqq6/kjvvss88EAEKDBg2EuLg42faMjAyhV69esvPOmDFD7rj169cLAITBgwfLbX/w4IFgZmYmABAmT54sZGdny7U/ffpUOHXqlMLvR0xMjKJvlxATEyMAEBwdHeW2Z2VlyY4dMWKE8PLlS1nbnTt3BCcnJwGAMHXqVIW1AxDeeustITU1VdaWnJwsuLu7CwCEefPmKaxHkRcvXgguLi4CAOGLL76Qu+6MjAyhf//+AgBhyJAhcsfNmDFDVsvAgQOFrKysYn0PHjxYts/kyZPl/tsWatWqlQBA8PPzE54/fy7bHh8fL3h6egoAhAEDBsgdc+zYMVm/zZo1Ex4/fqz09RIRaSsGOiIiUpmiQJeSkiLs3btXFgrq1Kkj94N54Q/es2bNUtjnpEmTBADCmDFjFLY/fPhQ0NfXF2rUqCHk5+cLglAQQkxMTAQAwh9//FHsmMePHwuGhoaiAt3YsWMFAMJ7772nxHeiQFkD3c8//yz7XikKQtu2bRMACCYmJkJmZmax2o2MjIRHjx4VO27z5s0CAKFLly5KX8OqVasEAMK7776rsD09PV2oWbOmoKenJyQnJ8u2FwY6S0tLISUlReGxhYHO1dVVyM3NLdZ+6tQpAYBQvXp14cmTJ8XaL168KAAQdHR0hNjYWNn2ooHu5MmTSl8rEZE24zt0RESkNkOGDJG9L2Zubo533nkHd+7cgYuLC/bt2wcjI6NixwQFBSnsa+/evQCAvn37Kmy3tbVF/fr1kZCQgFu3bgEAoqKikJ6eDmtra/To0aPYMTY2Nujevbuoa9q/fz8A4OOPPxZ1XFkUvmPXr18/hY+nBgQEwMLCAunp6YiMjCzW3qJFC4WTujRs2BAARL1H97rvv7GxMVq0aIHc3FxcuHChWHvXrl1f+3inv78/dHV1i20v/D706NEDtWrVKtbu5eWF5s2bIz8/HydOnCjWXrNmTbRv377UcxMRvSn4Dh0REalNu3btUK9ePQCAgYEBatasidatW6NHjx7Q01P8T05Jk1XcvXsXAJT6wTwhIQGurq54+PBhqX0CBROViFE4wUmDBg1EHVcWhYGrpBolEgmcnZ3x7NkzheHMwcFB4XGmpqYACt4tU1bh93/QoEEYNGhQqfsmJCQU26bMJCQl7fO67wMAuLi44MqVKwq/D5wAhYiqEgY6IiJSm1fXoVNGtWrVFG7Pz88HUHAHT9GdvaKsrKxEnfNNpaOjvgdvCr//Jd0lK8rR0bHYtpL+u4rdpyw01S8RUWXEQEdERJWSvb09bt26hUmTJqFFixZKHWNrawtAfnmEV5XWpoiDgwOio6Pxzz//yO4+akph/YV3xxSJiYmR21dT7O3t8c8//+Cjjz4q8bFYTVHm+1DYpunvAxFRZcd36IiIqFJ6++23AQBhYWFKH+Pl5QVjY2MkJibi4MGDxdqfPn2qcHtpCt/FW7t2rdLHFC4pUNI6eSXp1KkTAGDLli0KH48MDw/Hs2fPZOuxaVJZvv/qUvh92L9/P54+fVqs/dKlS7h8+TJ0dHTQoUOHcq6OiKhyYaAjIqJKaeLEiTA3N8fixYvxv//9Dy9fviy2T0xMDH755RfZ19WqVZNNXjJu3Dg8fvxY1paZmYmRI0cqXJC6NOPHj4eJiQl27dqFL774Ajk5OXLt8fHxOH36tNw2Ozs7AMCNGzdEnatPnz5wcHCQLaZeNBDGxMRgwoQJAArW7jM0NBTVt1gff/wxHB0dsXXrVkyaNAnp6enF9nny5ImooKssHx8ftGrVCpmZmRg+fLjcAuaJiYkYPnw4gILJY+zt7dV+fiIibcJAR0RElZKdnR127twJCwsLfPbZZ7C3t4evry8GDhyI9957D/Xq1UPdunWxfPlyueNmzZqFli1b4q+//oKrqyv8/PwQHByMunXr4uTJk/jggw9E1eHg4IBt27bBxMQEc+fOhb29PXr37o3g4GC0atUKdnZ2+OGHH+SOCQwMBAAMHDgQgYGBGDp0KIYOHYro6OhSzyWVSrFt2zZYWlpi1apVqFevHvr164d33nkHjRo1QkxMDN566y3MmDFD1DWUhZGREfbu3QsnJycsWrQIDg4O6NixI95//3307t0bjRs3Rp06dTB9+nSNnP/XX3+Fo6Mjdu7cCWdnZ/Tp0wf+/v5wcXHBhQsX4OnpWey/PRFRVcR36IiIqNLq0KEDbty4geXLl2Pv3r24cOECsrOzUbNmTTg4OMgCU1FGRkY4duwYFixYgF9//RUHDhyAhYUFunbtijlz5mDDhg2i6+jevTuuX7+OxYsXY//+/di/fz/09PRQp04dDBo0CMOGDZPbf+TIkUhPT8cvv/yCffv2yR6fHDhwINzc3Eo9l7e3Ny5fvoyFCxfijz/+QHh4OKRSKTw8PPDBBx9g6NChJc4Yqm6NGzfG1atX8f333yM8PBxXr17FuXPnYG1tDTs7O3z22Wfo3bu3Rs5dt25dREVF4ZtvvsGOHTuwZ88e6OjowM3NDX379kVoaCgnPyEiAiARBEGo6CKIiIiIiIhIPD5ySUREREREpKUY6IiIiIiIiLQUAx0REREREZGWYqAjIiIiIiLSUgx0REREREREWoqBjoiIiIiISEtxHbpKIj8/H48ePYKJiQkkEklFl0NERERERBVEEASkp6ejTp060NEp/R4cA10l8ejRI9jb21d0GUREREREVEnExsbCzs6u1H0Y6CoJExMTAAX/0UxNTSu4GqBv377YsmVLRZdBWoRjhsTimCGxOGZILI4ZEquyjJm0tDTY29vLMkJpGOgqicLHLE1NTStFoNPX168UdZD24JghsThmSCyOGRKLY4bEqmxjRplXsTgpChERERERkZZioCMiIiIiItJSDHRERERERERaioGOiIiIiIhISzHQERERERERaSkGOiIiIiIiIi3FZQuIiIiIqFQ5OTnIy8ur6DJEs7KyQlZWVkWXQVpEE2NGR0cH+vr6Si1BUBYMdERERESkUFpaGhITE5GdnV3RpZRJSEgIYmJiKroM0iKaGjO6urqoXr06atasCQMDA7X2zUBHRERERMWkpaUhLi4OxsbGsLa21ugdBk2RSCRwcnKq6DJIi6h7zAiCgLy8PGRmZiI1NRX37t2DnZ0dqlevrrZzMNARERERUTGJiYkwNjaGnZ2d1gW5Qrq6ujA0NKzoMkiLaGrMGBsbw9LSEvfv30diYiIcHBzU1jcnRSEiIiIiOTk5OcjOzoaZmZnWhjmiykZXVxeWlpbIyMhAbm6u2vploCMiIiIiOYUToOjr61dwJURvFqlUCgBqDXR85JKIiIiIFOLdOaoShHxAyAOEPBjq5RV8LdHMfS9N/D/FQEdERERERNpLEP4NZbmyYCb7PL/I50Xb5bbny7qyNwWQ/xLQ1Z53LxnoiIiIiIioYhW5S1ZiCBPygHxF4Ux9jy8CKDi/rnq71CQGOiIiIiIiUo3sLtmrISz334CmYLtceMt//TnUSaIDSHQBiR6goyv7PCXtOcx1tCjNgYGOiIiIiKhUYt97cnR0xL1799Rag5OTE+7fvw9BEDTXlyCU8rhi0e0lBDOoXpsoEl1AR+/fMPZvOCu2raTtit+RS3h8C+Za9LglwEBHRERERFSqwYMHF9t2+vRp3LlzB82bN4e7u7tcm7W1dTlV9gpBAJBf5I7YK48uFr1TBgBp0UW2V8RdMsl/YeuVO2VyYUzhdt2C44mBjoiIiIioNBs2bCi2LSQkBHfu3IG/vz9mzpypvpPJ7pLJ3/06si8MOS+zgRdxJdwpK9z/9XfJjvy+HDm5uUBOuur1SnT/DVyK7pTpyge2wnbZnTKuoKYODHREREREROpU+C6ZokcTS3unrHC7Ai61dABUAzIfq1yei7Pdf1/I3SV7NXApc6eMd8kqGmMxEREREVFRglAQxvKygdwXQE4a8PIZkJUIZD4puEuWm1Gwb3YSkPoPkHIdeHYFG5bPhERHFzMnj8bNyH3oFxyAWnZ1oVOtBnZs/Ql48Qi3/7qImbPmoU2XANjUbwuDGs1h16gbPhj5BW7evq+wJCcPP0isveW23XvwCBJrb3TqNQKZ2fmYPOd7OHr0grROW9RrGYSFK3+HYFgbMLIHjJ0Bk3qAaQM4eQUW9GXpAVh4AhbNcS/FGBLzRujk9zEydWtj8uxVcGzYFlJzR9Rr0gYLv1sDQc8E0DMCdKX/hr6CMHfixAl06dIFJiYmsLCwQM+ePXHx4kVs2LABEolE9B3M2NhYjBkzBi4uLjA0NISlpSXeffddnD17tti+x48fh0QiQUhICJ48eYKhQ4fCzs4Oenp6+O677wAUvAPp5OSEly9fYtasWWjQoAGkUin8/f3lzjl8+HB06tQJUqkUNWvWREBAAC5cuFDsnPfu3YNEIkGnTp2QlpaG8ePHw9nZGfr6+hg7dqyoa1UH3qEjIiIiojePkA9dHQHIy1I81X1pU+CXcJdMTl7mv39mA7nPi54YABB9+z68uw2GlaUZOrfzwrPUdOjrFfzo/cMvO7Bo2c9o0tAF3u6NIJUa4K/oGPwctg8795/EqT82oVnTxq/cKfv3x3YT1//ulKWaAwBe5huge98J+Ouvv9CpUydkZGTgxIkTmPzlAqRn62LOnDmvFP/vXTVJ8dkcX758ie7duxfva/JkpKenF+tr+/btCA4ORl5eHlq3bg0nJydcu3YNPj4+GDJkyOu/j684d+4c3nnnHTx79gxubm545513kJCQgAMHDmD//v3YtGkT+vbtW+y4hIQEeHt7Izc3Fz4+PsjKykL16tVl7fn5+fD398fJkyfRsWNHNGvWDFZWVgCAa9euoUuXLkhMTISzszMCAgLw4MEDhIeHY/fu3fj111/Rp0+fYufMzMxEx44dcf/+fXTs2BGenp6wsLAQfc2qYqAjIiIiosrntdPgv24R6XzUNUfBnbPyINH597FEAwDA5vCDGDP8A3y3aCZ09aRy75T59/kAw8dMgnPdegXboANIJFi/fj0+/PBDjP3iOxw9erR4/wBgYPrfNp2CH+XPnTuHjh07IiYmBqamBe0XL15E69at8e2332Ly5MkwNjZW6jLE9JWWloZhw4YhLy8PmzZtwoABA2T9fPnll5g9e7aob2FaWhoCAwORlpaGX375Be+//76s7eLFi+jevTuGDh2KLl26oEaNGnLH7tu3D71798avv/4KQ8Pis1TGxsZCKpUiOjoatra2su2CIOD9999HYmIiPv/8c3z00UdwdXUFAPz+++8IDg7Ghx9+CB8fH9SuXVuuz4iICLRp0wZ3796Fubm5qGtVJwY6IiIiIhKlRQvgyRNl9hQUfy6buON17YX0oMqPrTY1c3DxyN+v31HMFPj65gXHVKsFWHr+F7iqRwEAatSogYWLV0G3yF2iQq19fBWefsiQIVi3bh2OHz+O1NRUmJmZKXV9Ojo6WL16tSyAAUCLFi3w9ttvY8+ePbh48SI6deqk9r7CwsKQnJwMX19fuTAHFAS6n376CffvK36EVJEff/wRjx8/xoQJE+TCXGEN06dPx/jx4/HLL79g3Lhxcu1SqRTLli1TGOYKzZ8/Xy7MAQWPbF67dg0ODg6YM2eO3HITgYGB8Pf3x/bt2/Hjjz9i2rRpxfpcunRphYY5gIGOiIiIiEpSOOPiK5N4PHligrg4ZRZflpTweTmT6ADVapc+Nb5ER9wEH//eHStptsauXbvKPfL3qufPn2P37t24fPkykpOTkZOTAwB4/PgxBEHAnTt34OnpqVQpjo6OcHNzK7a98E7T48fKT6Qipq8zZ84AgMLHEfX09BAYGIjFixcrfe6DBw8CAAICAhS2t2/fHkDBnbFXeXp6FgtrRUkkErz33nvFtp86dQoAEBwcDH19/WLtgwYNwvbt22X7FVW7dm20aNGixHOWFwY6IiIiojdZfh6Qk1owqUdOSsGfL58BL1Pk/yzall8dqDkFSH0BvCg+Db6NdUMgv/gPv2ojKSkISor8ISm5/V+5uTnQ09OHjY0eUL3kH/Y1wcHBocS2o0ePol+/fkhISChxn/R05ZcUsLOzU7jdxMQEAJCdna2RvgrDnb29vcJjSvseKFJ4d6xdu3al7peYmCj6XDVr1oRUKi22/dGjRwAKFltXpHB7XFyc6HOWFwY6IiIiosouN/OV0PXqn4ra/t2Wkyb+fAaO/76/pnhNs1IfX3x1GnxRU+Crdxr8W7fuoX79+mrrT4ySHv17/vw5goODkZycjC+//BL9+vWDo6MjqlWrBolEggEDBuC3336DoMR6coV0dNQ3cb06+xIrP79gYfOgoCAYGRmVuF+DBg2KbSvtUUtl2ksiKWU8lrVPdWOgIyIiItI0If+/qe9fF8KK3TFLAfKVv8OiFnomBUFLzxDQ11dysegijy5SiU6dOoWkpCQEBQXhq6++KtZ+9+7dCqiqbAonCYmNjVXYXtL2ktjZ2SE6OhqTJ0+Gl5eXyvUpo06dOgBQ4rt+hXcNS3ucs6Ix0BEREREpIy+75MD1uoCWkwr5CUA0TEcfMLAo+NA3/+9zA/NXtilo0zMFXuYAMTGAiTNQSe5CvCmePXsGQPGjjbdv30ZUVFR5l1Rm7dq1w4YNG/D7779j+PDhcm15eXnYvn27qP66deuGI0eOIDw8vNwCXeF7eVu3bsX8+fOLtf/yyy9y+1VGDHRERERUNQgCkJuuxB2xEkJb4bpj5UXP5JXQZa5EQPt3m241FR9dzFHDBZAihZOLbN++HVOnTpVNv5+SkoKPPvpINjmKNujTpw8mTZqEQ4cOYfPmzejXr5+sbc6cOYiJiRHV3/Dhw7F48WIsWrQIDg4OGDp0qNwjoLm5uThy5AhsbW3RpEkTtVxDp06d0LRpU1y7dg1ffvklBg8eLGsLDw/H9u3bYWxsjA8//FAt59MEBjoiIiLSHvk5JUzo8ZpJPl6mFHwt5JdfrRI9+Ttgxe6KldKmb/bfLIr0RmnRogW6deuGQ4cOwdXVVbYEwPHjx2FtbY1evXph586dFVukkszMzLB27VoEBwejf//+WLp0qWxh8Zs3b+Ljjz/GmjVrYGBgoFR/5ubm2LlzJ9577z0MHz4cc+bMQZMmTWBhYYEnT54gKioKKSkpCA8PV1ugk0gk2LRpEzp37ox58+Zhy5YtaNmyJR48eIAzZ85AT08P69atK7YGXWXCvymIiIio/AgCkJtRyiOKKSW2bQl8AmxW7gdDtdEzKvkRxdcFND0jtU7wQW+OnTt3Yu7cuQgLC8Mff/yBmjVrol+/fpgzZw4mTJhQ0eWJEhAQgMOHD+Orr77ChQsXcOPGDbRu3Rrr1q3DoUOHAABWVlZK99e6dWtcu3YN3377Lfbu3YsTJ04AKHhfr2PHjujduze6du2q1mto2rQpoqKiMGfOHOzZswfbtm2DmZkZ/P39MWXKFLRs2VKt51M3iSBmCh3SmLS0NJiZmSE1NVVuIceK4ufnh127dlV0GaRFOGZILI4ZLZaf+98dr9e+Q/ZqW0rBWmblRaKj/DtkigKajgan5q/EsrKyEBMTA2dn50ozk19Z3Lp1q8JmuSSgR48eOHDgAM6fP49WrVpVdDlK0fSYUfb/LTHZgHfoiIiIqhpBKHgfrLT3xUoLaLnKr4+lFrrVAAMLPHiSBod6zZV7h6zwTz0T3iUj0qC4uDjo6emhVq1asm35+flYsmQJDhw4AFdX10p/h0vbaV2g27p1K1asWIErV67g5cuXqFevHt5//32MGzdO4eruyti5cyfWrVuHiIgIJCcnw9zcHPXq1UOPHj3w5Zdfyu17/PhxdO7cudT+Vq1ahREjRpSpFiIiIqUULhat7LT3rwa0/PKceEFSEK5EvUNW5GvdgsWAx/CuLlGlc+rUKQwcOBAeHh5wdHREdnY2rl+/jnv37qF69er44YcfSl3LjVSnVYFu7NixWLJkCfT09NClSxcYGxvj6NGjmDRpEnbv3o2DBw+iWrVqSvf38uVLDBw4EFu3bkW1atXQpk0b1KpVC0+ePMGNGzewdOnSYoGuUK1atdCjRw+FbW5ubmW6PiIiqmLysoqHMKVmXHxWtsWiVaEjFf8OWeE2fVOuTUb0hvLy8sIHH3yAU6dOITo6GllZWbCxscGgQYMwefJkNGrUqKJLfONpTaDbsWMHlixZAmNjY5w4cQKenp4AgMTERHTp0gWnT5/G9OnT8c033yjd57Bhw7B161b4+/tj7dq1sLa2lrXl5+cjIiKixGMbNGiADRs2lPl6iIjoDSBbLDql5BBW2uOL5b1YtL6ZyHfIinyuq73vURGR5tSvXx8//vhjRZdRpWlNoJs3bx4AYPLkybIwBwDW1tZYuXIl2rdvj+XLl2P69OkwMzN7bX9HjhzBTz/9hCZNmiAsLKzY45o6Ojpo3bq1ei+CiIgqn7yXpS8KXeokH5VhsWjz0t8hK2zTNwN0dMuvViIiKhdaEeji4uJw4cIFAMCAAQOKtfv4+MDe3h6xsbHYt28f+vfv/9o+ly1bBqDgMc6yvntHRESVgGyx6BQlwpiCbRWyWLS5iHfIit4lU3WxaCIietNoRaC7dOkSAMDS0hLOzs4K92nRogViY2Nx6dKl1wa6vLw8HDlyBADQoUMHPHnyBJs3b0Z0dDSkUik8PDwQGBgIY2PjEvt4+vQpZs2ahbi4OBgaGqJBgwZ455134ODgUMarJCKqwvJzRL5DliIf0Mp1sWjdsi0UzcWiiYhIA7TiX5WYmBgAKDUs2dvby+1bmrt37+L58+cAgPPnz2PUqFGyrwtNnDgRmzdvRpcuXRT28c8//2DGjBly2/T09PDJJ59g0aJF0NMr/VubnZ2N7Oz/3p1ISyvnl9uJiNSpcLHo0t4he6VtaY/LwA77gq9zM8q3Xj0j8e+QFbZxsWgiIqpEVA50T58+xZEjRxAVFYWnT5/i2bNnsLCwQK1ateDl5YUuXbrIrUtRFunpBevdGBkZlbhP4d00ZYJRUlKS7POPPvoIbdu2xTfffIMGDRrgzp07mDp1Kvbt24devXohKipKbnFBMzMzjB07Fr1794arqytMTU1x584drF+/HsuXL8e3336L58+fY82aNaXWMH/+fHz11VfFtvft27dSPAIaEREBPz+/ii6DtAjHjPbTkeTDSD8XxgY5MNLPKfjTIKfY13JtRT7X0xH3LpmTOYAXZVvPLC8fyMjRR8ZLfTz/98/Cz58Xfv5SHxlFv/738xc5+sjNVzTj4rN/P17/i0GqGPx7pvxYWVkhJCQEEokEurra++7lixcvcOvWrYoug7SIpsdMXl4e4uPjsWjRIrlM8qqcHOWXlpEIgiD6be6cnBxs2bIFK1askM0EqaibwjUnWrVqhdGjRyM4OLhMYWXevHmYNm0a2rVrh9OnTyvcZ9q0aZg3bx66d++OAwcOlNrfuXPn0LZtWwCAk5MT/vnnH0ilUll7Xl4e3N3dcf36dXz44YdYt26dUnVu374dgYGBAAoeE3V3dy9xX0V36Ozt7ZVaDb48+HGtHxKJY6YSkC0WnVL6hB45JdxBK+fForNzdSA1qVW2ST70jDkNfhXEv2fKT1ZWFmJiYuDs7AxDQ+2d4fTWrVtyv5gneh1Njxll/99KS0uDmZmZUtlA9B26n3/+GVOmTMHjx48hCAJq1KiBNm3aoHHjxrCysoKpqSlSU1ORlJSE69ev49y5czh//jz+/PNPTJ48GfPnz8fAgQNFndPExAQAkJFR8iM5hY9MKhOGCvsDgJCQELkwBwC6uroYPnw4PvnkExw+fFjpOgMCAuDu7o7Lly9j9+7dpQY6qVRa7LxERMjPA3LTlJ/2/tVt+S/LsVjJv9Pgi3mH7L9tfXr34Q/nREREKhIV6Nq0aYOIiAhYW1sjNDQUISEhaN68+WuPu3z5MtavX4/ffvsNgwcPxsqVK3H27Fmlz+vk5AQAiI2NLXGfwrbCfV/Xn0QigSAIqFu3rsJ9Crc/fvxY6ToBoGHDhrh8+TIePnwo6jgieoPkZckHrtdN6CG3TxrKdxp8gyJ3wZR8h0z2NReLJiIiqmiiAt2tW7ewaNEijBkzRtTdJXd3dyxZsgSLFi3C0qVLsXDhQlFFenh4ACh4963wFuWrLl68CABya9SVxNjYGG5ubvjnn3+QmJiocJ/C7aXNdKlI4bOwRe8CEpGWEfKBnPSSQ9jrZmHMyyrfevVNS35E8XUBTa9a+dZKREREaiUq0N29e1el97ukUikmTpyI4cOHizrOzs4O3t7euHDhAn799VdMmzZNrv306dOIjY2FVCpFz549leqzT58+mD17Ng4fPoxx48YVaz906BAAoGXLlkrXGRcXh1OnTok+jog0IO/la6a9L2UWxpzU8p0Gv3CxaLELRcumwdfeCQuIiIhINaICnbom6yhLP1OnTkXv3r2xYMECvP3227I7cUlJSRg1ahQAYMyYMTAzM5MdEx4ejilTpsDW1la27lyh0NBQLF++HPv27cPq1avlQubmzZuxadMm2X5FLVmyBO+//z6sra3ltl+9ehUhISHIzMyEi4sLevXqJfoaiagIQQBynyvxDtm/f+a8si3vRfnWq2es3N0xReuW6VbnNPhERJWYROTf0Y6Ojrh3755miiF6hUrLFjx48AASiUS2Bpwm+fv7IzQ0FEuXLkXr1q3h6+sLIyMjHDlyBCkpKWjXrh1mz54td0xqaiqio6ORlVX88Sdra2ts2bIFfn5+GDFiBJYtW4aGDRvizp07soXMp0+fXuyO34wZMzBhwgS4u7vD2dkZOjo6smPy8/Ph4OCA3bt3c8ITIqDIYtEpyr9DJpuFMQUQ8sqv1sLFosW8QybbZs7FoomI3mCDBw8utu306dO4c+cOmjdvXmwivFd/8a9OEomEgZHkqPQTiJOTE9q0aYMzZ86oq55SLVmyBO3atcOKFStw9uxZ5OTkwMXFBZMnT8a4ceNgYGAgqr9u3brhypUrmDdvHg4fPoydO3fC1NQUPXv2xKefforu3bsXO2batGk4c+YMbty4gUOHDiEjIwOmpqZo27YtevXqheHDh/P9OXpzCELBna7XvkOWgmk+EcChDvIBLfd5+darW138O2SF27hYNBERlWDDhg3FtoWEhODOnTvw9/fHzJkzy70mokIqBTpTU1OFE5RoUnBwMIKDg5XaNyQkBCEhIaXu4+rqqvB/0pJMnDgREydOVHp/ogqXn/dvyEoRMclHka/zlVvYspUdgISnqtUq0fnvjldp75DpK9pmDuiK+6UOERERkbZTKdA1atSo1KUEiEgNBOHfafCVnNDj1W05aeVbr66h0uuQFWvjYtFERPQGyM3Nxdq1a/HTTz/hxo0byMnJgZubG0JCQjBmzBjo6cn/CJ6QkID//e9/2LVrFx48eABdXV3UqlULrVu3xpgxY9CyZUts2LABQ4YMAQDcv39f7r2+jh074vjx40rV9vfff2PhwoU4cuQI4uPjYW5ujs6dO2P69Olo3Lix3L6F55wxYwYGDBiAL7/8EseOHUNCQgK2b98uewWpY8eO2LVrF2bOnInw8HA8fPgQo0ePxnfffQcA+OuvvzB37lwcPXoUSUlJqFGjBnx9fTFt2jS4ubnJnfP48ePo3LkzBg8ejAULFuCLL77A/v378eTJE3zzzTcYO3asuP8YVYBKgW7YsGEYNmwYLly4AG9vb3XVRPTmEfILZk5UJoQpmuSjQhaLNhc1yccHw0Lx02+7CgIdERFRFZWZmYl33nkHx44dg6WlJVq3bg1DQ0P8+eefGDduHI4dO4bw8HDo6BT8AjM9PR2tWrVCTEwM7O3t0a1bN+jp6eHBgwfYvHkz6tati5YtW6JevXoYPHgwNm7cCCMjIwQFBcnO2aBBA6Vq27FjB/r164fs7Gy4u7ujdevWiI2NRVhYGHbv3o0//vgDHTp0KHZcdHQ0vL29YWVlhc6dO+PZs2fQ19eXu+aOHTvi/v376NixIzw9PWFhYQEAOHLkCN577z1kZmbCw8MDnTp1wj///IOff/4Z4eHh2LdvH9q3b1/snAkJCfD29kZubi58fHyQlZWF6tWri/pvUVWoFOiGDBmCS5cuoXv37pg4cSICAwPh5OTECUHozZSXrcSEHiVN8pGKClssWvQkH2VbLDoly5BhjoiIqrzPPvsMx44dQ9++fbF69WrZDOzp6eno168fdu3ahTVr1mDEiBEAgG3btiEmJgZ+fn5yQQ8oCDVPnxa8zuDj4wMfHx9s3LgR1tbWol4ZAoB79+5h4MCB0NfXx549e9C1a1dZ2/79++Hn54eBAwfi9u3bxeal2Lx5M8aMGYPvvvsOurq6cn0CQEREBNq0aYO7d+/C3Nxc1p6RkYH3338fmZmZWL58OUaPHi1r+/bbbzF+/HgMGDAAt27dgqGh/M8Q+/btQ+/evfHrr78WayN5KgW6ov9Bp0+fjunTp5e4r0QiQW5uriqnI1KNIBQ8fijmHbKi2ypisehSF4ouZW0yLhZNRESatL8FkPmkoqt4LafcXOC6HlDNBuhxUePni4+Px9q1a2Fvb4/169ejWrX//j02MTHBunXr4OjoiFWrVskCXUJCAgCgS5cucmEOAGrUqIEaNWqopbbvvvsOGRkZWLZsmVyYA4AePXpg5MiRWLp0Kfbu3YvevXsXq2PhwoVyP/u/aunSpXJhDgDCwsLw9OlTtGnTRi7MAcC4ceOwadMmREZG4vfff8f7778v1y6VSrFs2TKGOSWoFOgEQfk7DmL2JSqRbLHoFPkwlqNEQMtJKd/FoiV6CoLXa94hky0Wbcpp8ImIqPLKfAJkxlV0Fa+lDwDKze2lFsePH0dOTg569OghF+YK2djYoH79+rh27RoyMzNRrVo1eHl5AQC+/vpr1KpVC++8845GZkw/ePAgACAgIEBhe/v27bF06VJEREQUC3Rdu3Yt9XHH2rVro0WLFsW2nzp1CgCKhbVCAwcORGRkJE6dOlVsH09PT9ja2pZ8QSSj0k+M+fnl+MMxvRlki0WnlG2SjwpbLNpc/CQfXCyaiIjeVNVsKroCpeTk5kJfT6/c6i18BHHt2rVYu3ZtqfsmJyfD1tYWvr6+GDduHL777jv0798fenp68PT0RLdu3fDhhx+ibt26aq3tdSEpMTGx2DYHB4dSjymp/dGjRwAKljpTpHB7XFzxXw687pz0H94CIPHyc0uf5r7UgJYCCOX46K1Et8g09+aiJvmAgRmgo19a70RERFVTOTy+qA73bt1C/fr1y+18hTc73N3d0bx581L3LTrnxOLFizF8+HDs3LkThw8fxpkzZxAREYFFixbht99+Q2BgoNpqU7RIelGtWrUqtu11jz2W9bFISSm/+OajlspTKdDt378fPXr0UFctVBlkPgFur8ZHHteBcyGKA1qFLRZtLm6haAPzf6fB510yIiIi0jw7OzsABROYLFu2TNSxbm5u+Pzzz/H5558jKysLy5cvx8SJEzFy5Ei1BDo7OzvcuXMH//vf/2BlZaVyf8qoU6cOgIJlFhRR9q4hlU6lQNezZ0/Ur18fo0aNwpAhQ2BqaqquuqiivHwGXJuJXm4AYmLU1KlEuUcU9RVtM+di0URERKQVOnfuDF1dXezZsweLFy+Wm9pfDENDQ3z22WdYvHgxHj9+jPj4eNSsWRMAoK+vX6aJBrt164Y7d+4gPDwcQ4cOLVNdYrVv3x7r16/Hb7/9hlGjRhVr/+WXX2T7UdmpFOgaNmyIv//+G+PHj8cXX3yBgQMHYvTo0WjSpIm66qPyZmCueHvhYtFKvUP2SiDTN+Fi0URERPTGs7W1xYcffoi1a9eif//+WLFiBWrVqiW3z+3bt3HlyhXZXbcdO3bAxsYGrVu3ltsvMjIST58+hbGxsdzskXXq1EFcXBxSUlKKzSpZmgkTJmDjxo347LPPYGlpWWxylOzsbOzevRutW7eW3WlUVXBwMKZMmYLTp09jzZo1+Pjjj2VtS5cuxcWLF2Fra6uWO5BVmUqB7saNGzh+/DiWL1+OXbt2YfXq1VizZg06dOiAMWPGoHfv3sWmX6VKTmoNdNyLiV/Mw9ffrf0vnHF9MSIiIqLXWrJkCe7du4fff/8d+/fvh7u7OxwcHJCRkYG//voLt2/fRq9evWQh5vjx41iyZAlsbW3h4eEBU1NTPHr0CKdOnUJ+fj6++uoruXXh/Pz8sGzZMnh6eqJt27YwNDSEm5sbJk6cWGpd9erVw2+//YYBAwYgMDAQ9erVQ8OGDWFkZIS4uDhERUUhIyMDly5dUlugMzIywqZNm/Dee+9h+PDhWLNmDVxdXfHPP//g0qVLMDY2xm+//cb35VSk8qQonTp1QqdOnRAXF4fvv/8eP/zwA06cOIGTJ0+iTp06GDFiBIYNGya7TUyVnI4+YNsT0UnfA2YNK7oaIiIiIq1SrVo1/PHHH9i0aRM2btyIy5cvIyIiAjVq1ICjoyMGDRqEfv36yfYPCQmBnp4eTp48iYiICKSmpsLGxgY9e/bEp59+Cl9fX7n+58+fD0EQsHPnTmzZsgW5ubno2LHjawMdAPTq1QtXr17F4sWLcejQIRw6dAj6+vqoU6cO3nvvPQQEBKBRo0Zq/X74+vriwoULmDt3Lo4ePYqrV6/C2toaAwcOxBdffAE3Nze1nq8qkghqXiAuJycH27Ztw/Lly3Hu3DlIJBLo6+ujT58+GDNmjMKZcwhIS0uDmZkZUlNTK8W7iH5+fti1a1dFl0FahGOGxOKYIbE4ZspPVlYWYmJi4OzsrNV3T26V8yyXpP00PWaU/X9LTDZQ+/OQ+vr66N+/P06cOIHJkydDEAS8fPkSmzZtQtu2bdG+fXv8+eef6j4tERERERFRlaP2QPf06VPMnj0bzs7OWLhwIQDAw8MDkyZNgr29Pc6cOQMfHx/+ho2IiIiIiEhFagt0Z8+exYABA+Do6IiZM2fiyZMnCAgIwMmTJxEZGYn58+fj7t27WLFiBQBg5syZ6jo1ERERERFRlaTSpChZWVnYtGkTVqxYgStXrkAQBFhYWGDYsGEYPXo07O3t5fbX0dHByJEjsW/fPhw+fFilwomIiIiIiKo6lQKdra0tUlJSIAgCGjdujNDQUAwcOBDVqlUr9bhatWrh5cuXqpyaiIiIiIioylMp0KWkpODdd99FaGhosSlVS/P5559j0KBBqpyaiIiIiIioylMp0N26dQt169YVfZyrqytcXV1VOTURERERaZiaV7ciqvI08f+USpOilCXMEREREVHlpqurC6BgfWEiUp/s7GwAgJ6eSvfV5IgKdJ9++imSkpJUOmFCQgJCQ0NV6oOIiIiINEdfXx9SqRSpqam8S0ekJnl5eUhOToaRkZFaA52onlasWIH169dj9OjR+PDDD0Wtoh4dHY0ffvgBq1evRmZmJpYuXSq6WCIiIiIqH9bW1oiLi8PDhw9hZmYGfX19SCSSii5LlLy8PGRlZVV0GaRF1D1mBEFAXl4eMjMzkZqaivz8fNSuXVtt/QMiA92FCxfwySefYOHChVi0aBHatGkDX19ftGnTBg0bNoSVlRWMjY3x/PlzJCUl4a+//sK5c+dw6NAhREREQBAEtGvXDsuWLVPrRRARERGRepmamgIAEhMTERcXV8HVlE18fDzvMJIomhozurq6qF69OmrWrAkDAwO19i0q0Hl4eOD06dPYtm0bvv32W5w9exbnzp0r9ZjCb0jbtm0xbtw4BAYGlr1aIiIiIio3pqamMDU1RU5ODvLy8iq6HNEWLVqEVatWVXQZpEU0MWZ0dHQ0eoe7TA9vBgUFISgoCJcvX8aOHTtw9OhRXLp0CRkZGbJ9jIyM4Onpic6dO8Pf3x/u7u7qqpmIiIiIypG+vj709fUrugzRkpKSYGhoWNFlkBbRxjGj0tt47u7ucHd3x8yZMwEAL168QGpqKszNzV+7uDgRERERERGpRn3TqwCoXr06qlevrs4uiYiIiIiIqAQqrUNHREREREREFYeBjoiIiIiISEsx0BEREREREWkpBjoiIiIiIiItxUBHRERERESkpRjoiIiIiIiItBQDHRERERERkZbSSKBbuHAhkpOTNdE1ERERERER/UsjgW7q1Km4ffu2wrasrCykpqZq4rRERERERERVitoCXXx8PDIyMgAAgiCUuN+VK1dgaWmprtMSERERERFVWWoLdKtWrYKZmRlcXV0hkUjw448/Ijw8HHfv3pXb78WLFzAwMFDXaYmIiIiIiKosPXV19PHHH6NBgwa4dOkSFi1ahN9//x1r1qyBRCKBsbExmjZtioYNGyIiIgKNGjVS12mJiIiIiIiqLLUFutq1a6Nv377o27cvwsLCsGXLFtSrVw+XL1+WfVy/fh12dnaYPXu2uk5LRERERERUZakt0BVV9DHLzp07o3Pnzpo4DRERERERUZWmkVkulyxZgpUrV8pty8nJwcuXLzVxOiIiIiIioipJI4Fu9erVePHihezrAwcOwNzcHKampvj0009LnQWTiIiIiIiIlKORQHf//n24u7vLvv7iiy/QvHlz/O9//8Mvv/yCNWvWaOK0REREREREVYpG3qEzMjKSfR4TE4PIyEj8+eef8Pb2Rl5eHlavXo3hw4dr4tRERERERERVhkbu0DVv3hz79+8HAPzyyy+oU6cOvL29ZW23b9/WxGmJiIiIiIiqFI3coZsyZQreeustnD9/HhcuXMDYsWNlbU+fPuXC4kRERERERGqgkUDXpUsXHDx4EJs2bYKPjw9mzJghazt69ChcXV01cVoiIiIiIqIqRSOBDih5/TmJRILg4GBNnZaIiIiIiKjK0FigK8mqVavK+5RERERERERvJI1MigIAu3btgr+/Pzw8PNCpUyfExMRo6lRERERERERVkkYC3Zo1a+Dv7w9BEPDWW2/h1KlTePbsGQBg0qRJOHToUJn73rp1Kzp16gQLCwsYGRmhefPmWLRoEXJycsrc586dO+Hn5wcbGxsYGBigZs2aaNu2LWbNmlXiMbdv30ZISAjs7OwglUphZ2eHkJAQ3L17t8x1EBERERERiaGRQPe///0P06ZNw86dOzFnzhwIgiBrs7S0xLJly8rU79ixYxEcHIwzZ86gZcuW6NGjBx48eIBJkyahS5cuyMzMFNXfy5cvERwcDH9/fxw+fBiNGzdGUFAQmjRpgjt37mDp0qUKjztz5gyaN2+OjRs3wtzcHL1794a5uTk2btyIZs2a4fz582W6PiIiIiIiIjE0EugePHggmxBFIpHItTVs2BCXL18W3eeOHTuwZMkSGBsb488//8SBAwfw+++/49atW2jatClOnz6N6dOni+pz2LBh2Lp1K/z9/fHgwQMcOXIEv/76K44ePYrHjx9jz549xY558eIFgoOD8eLFC0yZMgXXr1/H5s2bcf36dUyZMgUZGRkIDg4WHS6JiIiIiIjE0kigq1evHqKiohS2mZubIyEhQXSf8+bNAwBMnjwZnp6esu3W1tZYuXIlAGD58uVITU1Vqr8jR47gp59+QpMmTRAWFgZra2u5dh0dHbRu3brYcRs2bMCjR4/g6uqKOXPmyLXNmTMHrq6uiI2NxU8//STq+oiIiIiIiMTSSKAbMmQI5syZg3PnzhVri46ORq1atUT1FxcXhwsXLgAABgwYUKzdx8cH9vb2yM7Oxr59+5Tqs/Cxz7Fjx0JfX1/pWsLDwwEA/fr1g46O/LdPR0cHffv2BQBs375d6T6JiIiIiIjKQiPLFowdOxanT59Gp06d4O/vD4lEgri4OKSkpGDu3Lno3r27qP4uXboEoOD9O2dnZ4X7tGjRArGxsbh06RL69+9fan95eXk4cuQIAKBDhw548uQJNm/ejOjoaEilUnh4eCAwMBDGxsYl1tKiRYsS6yi6HxERERERkaZoJNDp6Ohg+/btWLFiBb7++msIgoBevXoBAFq3bl3sUcXXKVzywMHBocR97O3t5fYtzd27d/H8+XMAwPnz5zFq1CjZ14UmTpyIzZs3o0uXLrJt6enpSEpKKrWWwjoSEhKQkZEBIyMjhftlZ2cjOztb9nVaWtpr6yYiIiIiIipKI4Fu4cKFGDZsGEaPHo3Ro0fj5s2bePz4MWrXro369esXmyjlddLT0wGgxHAEQHY3TZlgVBjKAOCjjz5C27Zt8c0336BBgwa4c+cOpk6din379qFXr16IiopC/fr15eoorZaid/XS0tJK3G/+/Pn46quvim3v27evqEdANSUiIgJ+fn4VXQZpEY4ZEotjhsTimCGxOGZIrMoyZsQsyaaRQDd16lR07twZLVu2BAC4urrC1dUVAJCVlYXs7GyYmZlp4tRKKbqMgq2tLQ4cOACpVAoAaN68OXbt2gV3d3dcv34dCxYswLp169Rew5QpUzB+/HjZ12lpabC3t8eWLVtgamqq9vOJ5efnh127dlV0GaRFOGZILI4ZEotjhsTimCGxKsuYSUtLUzovqW1SlPj4eGRkZACQD0yvunLlCiwtLUX1bWJiAgCy/hUpfGRSmTBU2B8AhISEyMJcIV1dXQwfPhwAcPjwYYXHlVRL0Uc3S6tFKpXC1NRU7oOIiIiIiEgMtQW6VatWwczMDK6urpBIJPjxxx8RHh6Ou3fvyu334sULGBgYiOrbyckJABAbG1viPoVthfu+rr/Cxz7r1q2rcJ/C7Y8fP5ZtMzExkYXRBw8elFqHtbV1qY+IEhERERERqUptge7jjz/Gpk2bEBAQAEEQ8PvvvyMwMBD169eHmZkZfHx8MGzYMIwdOxaNGjUS1beHhweAgnffSpr05OLFiwAgt0ZdSYyNjeHm5gYASExMVLhP4fZXZ7os7L/wfKrUQUREREREpAq1BbratWujb9++WLBgAZycnPDHH38gKSkJhw8fxsyZM+Hi4oLr16/Dzs4Oa9euFdW3nZ0dvL29AQC//vprsfbTp08jNjYWUqkUPXv2VKrPPn36AJB/pLKoQ4cOAYDsPcBCvXv3BgBs3rwZ+fn5cm35+fnYsmULACAgIECpOoiIiIiIiMpKIwuL3717Fy1atICFhQU6d+6McePGYePGjTh37hz27t1bprtXU6dOBQAsWLAAUVFRsu1JSUkYNWoUAGDMmDFyLw+Gh4ejQYMG8PX1LdZfaGgoLCwssG/fPqxevVqubfPmzdi0aZNsv6JCQkJQp04d3Lx5E9OnT5drmz59Om7evAk7Ozt88MEHoq+RiIiIiIhIDFGBztbWFt99952GSimdv78/QkND8fz5c7Ru3Rpvv/02goKCUK9ePVy7dg3t2rXD7Nmz5Y5JTU1FdHQ07ty5U6w/a2trbNmyBYaGhhgxYgSaNGmCPn36wNPTE/3794cgCJg+fXqxO37Vq1dHWFgYqlevjnnz5qFp06bo378/mjZtinnz5sHIyAhbt25FtWrVNPr9ICIiIiIiEhXoHj9+jKtXrypsO3TokNw6bZqwZMkSbNmyBW3atMHZs2exb98+2NnZYcGCBTh69KjoENWtWzdcuXIFgwcPRkpKCnbu3IkHDx6gZ8+eOHDgAGbNmqXwuHbt2uHKlSv44IMPkJycjN9//x3Jycn44IMPcOXKFbRu3Vodl0tERERERFQqta1D99Zbb2HIkCEK12yLiIiAsbGx6MlQFAkODkZwcLBS+4aEhCAkJKTUfVxdXbFhwwbRddSrVw8bN24UfRwREREREZG6qPUdupLWn1u1ahWaNm2qzlMRERERERFVeRqZFIWIiIiIiIg0j4GOiIiIiIhISzHQERERERERaSkGOiIiIiIiIi0lOtBdvHgRK1aswIkTJ5CUlKSJmoiIiIiIiEgJopctuH79OkJDQ2Vf16xZE02aNAEAPHnyBA8fPoSdnZ36KiQiIiIiIiKFRAW6sLAwREZGIjIyElFRUUhOTsbTp0/x9OlTAMCBAwfg6OgIKysreHl5wcvLC56enkhOTtZI8URERERERFWZqEAXFBSEoKAg2df379+XBbzCkJeYmIjExEQcOHAABw8eVHvBREREREREVED0I5dFOTo6wtHREQEBAbJtsbGxcgEvMjIS8fHxkEgkKhdLRERERERE/1Ep0Clib28Pe3t7+Pv7y7bFxcUhMjJS3aciIiIiIiKq0kQFuri4OERHR8Pb2xsmJiZKH2drawtbW1vRxREREREREVHJRC1bsGnTJnTr1g3z5s0rcZ+YmBjs2rULR48eRVZWlsoFEhERERERkWKiAt2xY8cAAMOHD1fYPmnSJNSvXx+9e/dGt27dYGNjg9WrV6teJRERERERERUjKtDdvn0bbm5ucHJyKta2Z88efP3118jPz4cgCBAEAWlpaRg1ahRWrFihrnqJiIiIiIjoX6ICXUJCAho1aqSwrTC0NW7cGOfPn0dcXByWL18OqVSKzz//HA8fPlS9WiIiIiIiIpIRFeiysrJgYGBQbHtGRgaOHj0KiUSCRYsWoWXLlqhduzZGjRqFRYsWITMzEz/88IPaiiYiIiIiIiKRgc7KykrhnbZz584hJycH1atXR7du3eTahg4dCktLSxw6dEi1SomIiIiIiEiOqEDn7u6OS5cuISUlRW774cOHAQBt27aFnp78SgiGhoZo2rQpbt68qVqlREREREREJEdUoAsODkZGRgbmzp0r25afn4+tW7dCIpGga9euCo+rU6cO0tLSVKuUiIiIiIiI5IhaWPz999/HN998g8WLFyM6Oho+Pj44efIkYmJiIJFIEBAQoPC4jIwMmJqaqqVgIiIiIiIiKiAq0Onp6WH79u3o3Lkz9uzZg71798raevXqBRcXF4XHXbt2DTY2NqpVSkRERERERHJEPXIJAPXr18f169cxZcoUtGjRAo0bN8aoUaOwYcMGhfv/9ddfiImJQb169VStlYiIiIiIiIoQdYeukLm5OebOnSv3Ll1Jli1bBolEAl9f37KcioiIiIiIiEog+g6dWJaWlvDx8UH37t01fSoiIiIiIqIqRdQdujlz5sDT0xNeXl6oVauWUscocxePiIiIiIiIxBMV6L788ktIJBIAgI2NDby8vGQBz9PTE7a2thopkoiIiIiIiIor0zt0APD48eNiM13WqFFDLuB5eXnBwcFBLYUSERERERGRPFGBrlq1asjKyoKnpyeGDBmC3NxcREZGIioqCv/88w/i4+Oxf/9+HDhwQHaMpaWlLNzNmzdP7RdARERERERUVYmaFCU6Ohp9+vRBZGQkPv30U9y8eRPffvstrl27hvT0dJw7dw4rVqzAhx9+iObNm0NPTw9JSUk4dOgQFi5cqKlrICIiIiIiqpJE3aGzs7PD5s2bMXr0aISGhmLlypXYsmULZs6ciZEjR6JVq1Zo1aqVbP+cnBxcu3YNUVFRiIqKUnvxREREREREVVmZli1o3749oqKisGrVKgBAaGgo3N3dcezYMbn99PX14enpiaFDh2LlypWqV0tEREREREQyZV6HTiKRYPjw4bh9+zZGjx6Nf/75B127dkVgYCBiYmLUWSMREREREREpoPLC4mZmZli6dCkuXbqEzp07Izw8HI0bN8YXX3yBFy9eqKNGIiIiIiIiUkDlQFeocePGOHz4MLZv347atWtj/vz58Pb2Vlf3RERERERE9Ioyr0NXKDc3F3///TeuXbsm+8jOzoYgCEhISFBHjURERERERKSAqEB37949XL9+XS683bx5E7m5uQAAQRAAAHXq1EHPnj3Rtm1b9VdMREREREREAEQGurp160IikQAoCG+6urpwc3ODu7u73Ie1tbVGiiUiIiIiIqL/lOmRy9atW+PDDz9E3759YWxsrO6aiIiIiIiISAmiA50gCDh//jzOnz+PTz75BM2aNYOHhwc8PT3h6emJZs2aQV9fXxO1EhERERERURGiAl1YWBgiIyMRGRmJqKgoJCcnIyIiAhEREbJHMfX19dGoUSN4enrCy8sLnp6eaN68OQwNDTVyAURERERERFWVqEAXFBSEoKAg2df379+XBbzCkJeYmIjLly/j8uXLWL9+PQBAV1cXDRo0wNWrV9VbPRERERERURWm0rIFjo6OcHR0REBAgGxbbGysXMCLjIxEfHw8bty4oXKxRERERERE9B+V16F7lb29Pezt7eHv7y/bFhcXh8jISHWfioiIiIiIqEpTe6BTxNbWFra2tuVxKiIiIiIioipDp6ILICIiIiIiorIp0x26e/fu4fLlyzAwMECrVq1gZWX12mPi4+NRs2bNspyOiIiIiIiIFBB1h04QBIwZMwb16tVDYGAg3nvvPdjZ2eHLL79UuP+9e/ewePFitG/fHnZ2dmopmIiIiIiIiAqIukO3bt06rFy58r+D9fSQnZ2NuXPnolq1apgyZQpSU1Pxww8/YNOmTbhy5QqAgiBYuE4dERERERERqYeoO3Q//vgjJBIJRowYgSdPnuDly5e4e/cuQkJCMHfuXBw/fhwuLi74/PPPcfnyZQiCADs7O4wdOxanT5/W1DUQERERERFVSaLu0N24cQOurq5yd+mcnJywbt065OTkwN/fH2lpaZBKpRg4cCCGDh2KVq1aqb1oIiIiIiIiEnmHLj09HR4eHgrbJk+ejLS0NOjp6eH48eNYu3atRsLc1q1b0alTJ1hYWMDIyAjNmzfHokWLkJOTI6qfDRs2QCKRlPqxf//+Ysfdu3fvtcdNnjxZXZdLRERERERUItGzXBoaGirc7ubmBgDo0KGDxu7KjR07FkuWLIGenh66dOkCY2NjHD16FJMmTcLu3btx8OBBVKtWTVSfLi4u8PHxUdhW2tp5RkZGCAoKUtjm5eUlqgYiIiIiIqKyUNvC4rq6ugCgsdksd+zYgSVLlsDY2BgnTpyAp6cnACAxMRFdunTB6dOnMX36dHzzzTei+vXx8cGGDRtE12NtbV2m44iIiIiIiNRF9MLif/75JxYvXoyDBw/i8ePHxTvU0cxa5fPmzQNQ8GhnYZgDCoJV4Tt9y5cvR2pqqkbOT0REREREVNmIvkP3999/Y+LEibKvLSws0KRJEzRp0gQAkJmZqb7q/hUXF4cLFy4AAAYMGFCs3cfHB/b29oiNjcW+ffvQv39/tddARERERERU2YgKdGFhYYiMjERkZCSioqKQnJyM5ORknDx5EqdOnYJEIkFYWBj2798PLy8vtGzZUvZRp06dMhd56dIlAIClpSWcnZ0V7tOiRQvExsbi0qVLogLd7du38cUXXyA+Ph7GxsZo0qQJ/Pz8YG1tXepxGRkZWLBgAe7duwd9fX24uLjg7bfflr1LSEREREREpGmiAl1QUJDcRCD379+XBbzCkJeYmIjU1FQcPXoUx44dk+1bp04dxMbGlqnImJgYAICDg0OJ+9jb28vtq6wzZ87gzJkzctsMDQ0xc+ZMTJo0qcTjEhMTMWXKFLlt48ePx/vvv49Vq1bB2NhYVB1ERERERERiqTQpiqOjIxwdHREQECDbFhsbKxfwIiMjER8fj0ePHpX5POnp6QAKZpYsSWGASktLU6pPGxsbTJs2DX5+fqhbty6kUimio6OxbNky/Pzzz5g8eTLy8vIwdepUueOkUimGDRuGoKAgNGrUCFZWVoiJicH27duxYMEC/PLLL4iPj8f+/fshkUhKPH92djays7NlXytbNxERERERUSGJIAiCpk8SFxeHyMhI+Pn5len4efPmYdq0aWjXrh1Onz6tcJ9p06Zh3rx56N69Ow4cOKBKuVi8eDEmTJgAqVSK+/fvo1atWkodFxERgbZt2yIvLw/h4eHw9/cvcd+ZM2fiq6++Kra9R48e0NfXL2vpahMREYGWLVtWdBmkRThmSCyOGRKLY4bE4pghsSrLmMnJycH+/fuRmpoKU1PTUvdV27IFpbG1tS11TbfXMTExAVDw3lpJnj9/DgCvvWBlfPrpp5g/fz4SExNx8OBBDBo0SKnjWrZsiffeew87duzA7t27Sw10U6ZMwfjx42Vfp6Wlwd7eHlu2bFHLNajKz88Pu3btqugySItwzJBYHDMkFscMicUxQ2JVljGTlpYGMzMzpfbVzBoDaubk5AQApb6DV9hWuK8qdHV1Ub9+fQDAw4cPRR3bsGFDpY6TSqUwNTWV+yAiIiIiIhJDKwKdh4cHACApKanESU8uXrwIAHJr1KkiKSkJwH93BzV9HBERERERkVhaEejs7Ozg7e0NAPj111+LtZ8+fRqxsbGQSqXo2bOnyueLiorCzZs3AUDUM7QZGRnYvXu36OOIiIiIiIjKQisCHQDZbJMLFixAVFSUbHtSUhJGjRoFABgzZozcs6bh4eFo0KABfH195fp68eIFVqxYIZs9s6iTJ08iMDAQQMGC5a8GszVr1ih89DMmJga9evXC48ePYW5ujg8//LCMV0pERERERKSccpkURR38/f0RGhqKpUuXonXr1vD19YWRkRGOHDmClJQUtGvXDrNnz5Y7JjU1FdHR0cjKypLb/vLlS4wZMwYTJkyAh4cHHBwckJubi5s3b+L69esAgKZNmyIsLKxYHStXrsSIESPQuHFjuLq6wsDAADExMbh8+TKys7NhZWWF7du3v3ZhciIiIiIiIlVpTaADgCVLlqBdu3ZYsWIFzp49i5ycHLi4uGDy5MkYN24cDAwMlOqnevXqmD59Oi5evIh//vkHN27cQGZmJiwsLNC1a1f06dMHISEhCvsLDQ3FgQMHcPXqVRw/fhxpaWkwNjZGs2bN0LNnT4waNQo1a9ZU96UTEREREREVo1WBDgCCg4MRHBys1L4hISEICQkptt3AwACzZs0q0/k//PBDPk5JRERERESVgta8Q0dERERERETyNB7o6tatiwYNGuDrr79Gdna2pk9HRERERERUZWg80N27dw83b97E5MmTUb9+faxfv17TpyQiIiIiIqoSNB7o1q9fj3Xr1mHkyJGQSqUYOnSopk9JRERERERUJWh8UpTBgwcDAIYMGQIAuH//vqZPSUREREREVCWU+6Qojo6O5X1KIiIiIiKiNxJnuSQiIiIiItJSDHRERERERERaSuV36J4+fYojR44gKioKT58+xbNnz2BhYYFatWrBy8sLXbp0Qa1atdRRKxERERERERVRpkCXk5ODLVu2YMWKFYiIiAAACIJQbD+JRAIAaNWqFUaPHo3g4GDo6+urUC4REREREREVEh3ofv75Z0yZMgWPHz+GIAioUaMG2rRpg8aNG8PKygqmpqZITU1FUlISrl+/jnPnzuH8+fP4888/MXnyZMyfPx8DBw7UxLUQERERERFVKaICXZs2bRAREQFra2uEhoYiJCQEzZs3f+1xly9fxvr16/Hbb79h8ODBWLlyJc6ePVvmoomIiIiIiEjkpCi3bt3CokWL8ODBA3z77bdKhTkAcHd3x5IlSxAbG4sFCxbg5s2bZSqWiIiIiIiI/iPqDt3du3dhampa5pNJpVJMnDgRw4cPL3MfREREREREVEDUHbpXw9yDBw8QGxsr+qSqhEIiIiIiIiIqoNI6dE5OTujXr5+6aiEiIiIiIiIRVAp0pqamcHZ2VlctREREREREJIJKga5Ro0ZleuSSiIiIiIiIVKdSoBs2bBjOnDmDCxcuqKseIiIiIiIiUpJKgW7IkCEYNWoUunfvjnnz5iE6OhrZ2dnqqo2IiIiIiIhKIWrZglfp6urKPp8+fTqmT59e4r4SiQS5ubmqnI6IiIiIiIiKUCnQCYKgkX2JiIiIiIjo9VQKdPn5+eqqg4iIiIiIiERS6R06IiIiIiIiqjgqBbr9+/erqw4iIiIiIiISSaVA17NnT7i5uWHJkiVIS0tTV01ERERERESkBJUCXcOGDXHr1i2MHz8etra2GDlyJK5fv66u2oiIiIiIiKgUKgW6Gzdu4OjRo+jduzeys7OxevVqNG/eHJ07d8bvv//OSVOIiIiIiIg0SOVJUTp16oRt27YhJiYG06ZNQ82aNXHixAkEBwfD0dERc+fORXx8vDpqJSIiIiIioiLUNsulra0tZs+ejQcPHmDTpk1o3bo14uLi8OWXX8LBwQGDBg3Cn3/+qa7TERERERERVXlqX7ZAX18f/fv3x4kTJzB58mQIgoCXL19i06ZNaNu2Ldq3b89gR0REREREpAZqD3RPnz7F7Nmz4ezsjIULFwIAPDw8MGnSJNjb2+PMmTPw8fHBrl271H1qIiIiIiKiKkVtge7s2bMYMGAAHB0dMXPmTDx58gQBAQE4efIkIiMjMX/+fNy9excrVqwAAMycOVNdpyYiIiIiIqqS9FQ5OCsrC5s2bcKKFStw5coVCIIACwsLDBs2DKNHj4a9vb3c/jo6Ohg5ciT27duHw4cPq1Q4ERERERFRVadSoLO1tUVKSgoEQUDjxo0RGhqKgQMHolq1aqUeV6tWLbx8+VKVUxMREREREVV5KgW6lJQUvPvuuwgNDYWvr6/Sx33++ecYNGiQKqcmIiIiIiKq8lQKdLdu3ULdunVFH+fq6gpXV1dVTk1ERERERFTlqTQpSlnCHBEREREREamH2pctICIiIiIiovIhKtB9+umnSEpKUumECQkJCA0NVakPIiIiIiIiEhnoVqxYAWdnZ0yZMgW3bt0SdaLo6GhMnDgRLi4uWLVqlahjiYiIiIiIqDhRk6JcuHABn3zyCRYuXIhFixahTZs28PX1RZs2bdCwYUNYWVnB2NgYz58/R1JSEv766y+cO3cOhw4dQkREBARBQLt27bBs2TJNXQ8REREREVGVISrQeXh44PTp09i2bRu+/fZbnD17FufOnSv1GEEQAABt27bFuHHjEBgYWPZqiYiIiIiISKZMyxYEBQUhKCgIly9fxo4dO3D06FFcunQJGRkZsn2MjIzg6emJzp07w9/fH+7u7uqqmYiIiIiIiKDiOnTu7u5wd3fHzJkzAQAvXrxAamoqzM3NUa1aNXXUR0RERERERCVQKdC9qnr16qhevbo6uyQiIiIiIqIScB06IiIiIiIiLcVAR0REREREpKUY6IiIiIiIiLQUAx0REREREZGWYqAjIiIiIiLSUgx0REREREREWkrrAt3WrVvRqVMnWFhYwMjICM2bN8eiRYuQk5Mjqp8NGzZAIpGU+rF///4Sj3/69CnGjBkDZ2dnSKVS1KpVC3369EFUVJSql0hERERERKQUta5Dp2ljx47FkiVLoKenhy5dusDY2BhHjx7FpEmTsHv3bhw8eFD0guYuLi7w8fFR2GZra6tw+82bN9G+fXvEx8ejbt268Pf3R0xMDLZt24YdO3YgLCwMvXv3Fn19REREREREYmgk0D1+/BizZ8/GzZs3UaNGDTRt2hTu7u5wd3dHnTp1ytTnjh07sGTJEhgbG+PEiRPw9PQEACQmJqJLly44ffo0pk+fjm+++UZUvz4+PtiwYYPS+wuCgH79+iE+Ph6DBg3C+vXroaurCwBYs2YNhg8fjg8++AC3bt2CjY2NqFqIiIiIiIjE0Mgjl++//z5+/vlnWFlZITExEUuXLsW7774Le3t71KpVC2+99ZboPufNmwcAmDx5sizMAYC1tTVWrlwJAFi+fDlSU1PVcxEl+OOPP3Dp0iWYm5tj5cqVsjAHAB9//DF8fX3x/PlzLFmyRKN1EBERERERaSTQ/fnnn1i1ahW2bNmCQ4cO4cmTJ3j06BH27t2LsWPHwtLSUlR/cXFxuHDhAgBgwIABxdp9fHxgb2+P7Oxs7Nu3Ty3XUJLw8HAAgJ+fH4yNjYu1F9a3fft2jdZBRERERESkkUcubW1tYW1tLbfNxsYGPXr0QI8ePUT3d+nSJQCApaUlnJ2dFe7TokULxMbG4tKlS+jfv7/Sfd++fRtffPEF4uPjYWxsjCZNmsDPz69Y/a/W0qJFixLrAIBbt24hIyMDRkZGStdCREREREQkhkYC3ciRIxEWFlam8KZITEwMAMDBwaHEfezt7eX2VdaZM2dw5swZuW2GhoaYOXMmJk2aJLqWwjoEQcC9e/fQuHFjUfUQEREREREpSyOPXKanp+PEiRP47LPPkJycrJb+AJR6t6vw8ce0tDSl+rSxscG0adPw559/IiEhAWlpabhw4QI++OADZGdnY/LkybL39sTUUvQxzNJqyc7ORlpamtwHERERERGRGBq5Q/f999/jyZMnWLx4MZYuXQoPDw+5j2bNmsHQ0FATp1aaosc/W7RogY0bN6J58+aYMGECZs2ahY8++gi1atVS+/nnz5+Pr776qtj2vn37Ql9fX+3nEysiIgJ+fn4VXQZpEY4ZEotjhsTimCGxOGZIrMoyZsSssa2RQPfo0SMkJCTgypUruHr1Kq5cuYLz589jw4YNePnyJXR1dUUVaWJiAgDIyMgocZ/nz58DAExNTVUrHsCnn36K+fPnIzExEQcPHsSgQYPkaklOTi6xlsI6XlfLlClTMH78eNnXaWlpsLe3x5YtW9RyDary8/PDrl27KroM0iIcMyQWxwyJxTFDYnHMkFiVZcykpaXBzMxMqX01trB4jRo10LVrV3Tt2lW2LTc3F3/99ReuXr0qqi8nJycAQGxsbIn7FLYV7qsKXV1d1K9fH4mJiXj48GGxWpKTk/HgwYNS65BIJHB0dCzxHFKpFFKpVOVaiYiIiIio6lLbO3Tx8fGv3UdPTw/NmjXDwIEDRfXt4eEBAEhKSipx0pOLFy8CgNwadapISkoC8N/dwUKF/Reer6Q66tevr3BZAyIiIiIiInVRW6CrU6eObEr/WbNmYffu3bh//75a+razs4O3tzcA4Ndffy3Wfvr0acTGxkIqlaJnz54qny8qKgo3b94EALRs2VKurXfv3gCAXbt2KXzssrC+gIAAlesgIiIiIiIqjdoC3c6dO1G7dm0AwNq1a9GrVy/UrVsXFhYW6NixI0JDQ7Fu3TpcvHgRWVlZovufOnUqAGDBggWIioqSbU9KSsKoUaMAAGPGjJF71jQ8PBwNGjSAr6+vXF8vXrzAihUrZDNWFnXy5EkEBgYCKFiw/NVA9/bbb8PDwwMpKSkYNWoU8vLyZG1r1qzBkSNHYGxsjE8//VT0NRIREREREYmhtnfo3nnnHdnnsbGxSE5OxuXLl2WTopw+fRpr1qwp06QoAODv74/Q0FAsXboUrVu3hq+vL4yMjHDkyBGkpKSgXbt2mD17ttwxqampiI6OLhYgX758iTFjxmDChAnw8PCAg4MDcnNzcfPmTVy/fh0A0LRpU4SFhRWrQyKR4LfffkP79u3x008/4fTp0/D29kZMTAwiIiKgp6eHn376CTY2NqKuj4iIiIiISCyNTYpiaWmJLl26oEuXLrJtubm5+Pvvv0VPilJoyZIlaNeuHVasWIGzZ88iJycHLi4umDx5MsaNGwcDAwOl+qlevTqmT5+Oixcv4p9//sGNGzeQmZkJCwsLdO3aFX369EFISEiJ/bm5ueHq1auYM2cO9uzZg/DwcJiZmSEgIADTpk1T23t8REREREREpdFYoIuIiMC5c+dgbGyMJk2aoFmzZqhWrRqaNm2Kpk2blrnf4OBgBAcHK7VvSEgIQkJCim03MDDArFmzylwDULAw+fLly7F8+XKV+iEiIiIiIiorjQS6H3/8EcOGDYOVlZVstkgdHR3Ur18f7u7u8PDwwMSJEzVxaiIiIiIioipDbZOiFPX1119j/PjxePToEQRBwPLlyzF27FjExsbiyJEjWLhwoSZOS0REREREVKVoJNDdv38f77zzDnR0Crr39vbG119/jStXrsDMzAxHjx7VxGmJiIiIiIiqFI0EOlNTU+Tm5kJHRwdmZmZITk4GANStWxfjx4/H559/ronTEhERERERVSkaCXTNmjXD7du3AQAuLi6IjIyUtbm6uuLMmTOaOC0REREREVGVopFAN2rUKCQmJgIAhgwZgm+//RaHDh3CvXv3sGzZMlhZWWnitERERERERFWKRma59Pf3h7+/PwDg448/xh9//IG33noLEokEurq6+PHHHzVxWiIiIiIioipF7YEuPz8f6enpMDMzAwDo6+tjz549uHDhAmJjY+Hh4QFnZ2d1n5aIiIiIiKjK0cgjl1ZWVti7d6/cNm9vbwQEBDDMERERERERqYnaA52Ojg7s7OwgkUjU3TUREREREREVoZE7dMOGDcOGDRs00TURERERERH9SyOBLjMzE2fPnsWQIUPw9OlTTZyCiIiIiIioytPILJcbN27Eo0ePsHHjRvz8889wd3dHixYt4OHhAQ8PDzRr1gyGhoaaODUREREREVGVoZFAFxsbi+TkZFy5ckX2ERERgQ0bNuDly5fQ1dVFTk6OJk5NRERERERUZWgk0C1cuBDDhg1D586d0blzZ9n23Nxc/P3337h69aomTktERERERFSlaOQduqlTp+L27dvFtuvp6aF+/fp49913NXFaIiIiIiKiKkVtgS4+Ph4ZGRkAAEEQStzvypUrsLS0VNdpiYiIiIiIqiy1BbpVq1bBzMwMrq6ukEgk+PHHHxEeHo67d+/K7ffixQsYGBio67RERERERERVltreofv444/RoEEDXLp0CYsWLcLvv/+ONWvWQCKRwNjYGE2bNkXDhg0RERGBRo0aqeu0REREREREVZbaAl3t2rXRt29f9O3bF2FhYQgLC4OLiwsuX74s+7h+/Trs7Owwe/ZsdZ2WiIiIiIioytLILJdFH7N8daZLIiIiIiIiUg9R79DZ2triu+++01ApREREREREJIaoQPf48eMS15A7dOgQ0tPT1VIUERERERERvZ7aZrl86623MHbsWIVtERER+Ouvv9R1KiIiIiIiIoKaFxYvaf25VatWoWnTpuo8FRERERERUZWn1kBHRERERERE5YeBjoiIiIiISEsx0BEREREREWkpBjoiIiIiIiItJTrQXbx4EStWrMCJEyeQlJSkiZqIiIiIiIhICXpiD7h+/TpCQ0NlX9esWRNNmjQBADx58gQPHz6EnZ2d+iokIiIiIiIihUQFurCwMERGRiIyMhJRUVFITk7G06dP8fTpUwDAgQMH4OjoCCsrK3h5ecHLywuenp5ITk7WSPFERERERERVmahAFxQUhKCgINnX9+/flwW8wpCXmJiIxMREHDhwAAcPHlR7wURERERERFRA9COXRTk6OsLR0REBAQGybbGxsXIBLzIyEvHx8ZBIJCoXS0RERERERP9RKdApYm9vD3t7e/j7+8u2xcXFITIyUt2nIiIiIiIiqtJEBbq4uDhER0fD29sbJiYmSh9na2sLW1tb0cURERERERFRyUQtW7Bp0yZ069YN8+bNK3GfmJgY7Nq1C0ePHkVWVpbKBRIREREREZFiogLdsWPHAADDhw9X2D5p0iTUr18fvXv3Rrdu3WBjY4PVq1erXiUREREREREVIyrQ3b59G25ubnBycirWtmfPHnz99dfIz8+HIAgQBAFpaWkYNWoUVqxYoa56iYiIiIiI6F+iAl1CQgIaNWqksK0wtDVu3Bjnz59HXFwcli9fDqlUis8//xwPHz5UvVoiIiIiIiKSERXosrKyYGBgUGx7RkYGjh49ColEgkWLFqFly5aoXbs2Ro0ahUWLFiEzMxM//PCD2oomIiIiIiIikYHOyspK4Z22c+fOIScnB9WrV0e3bt3k2oYOHQpLS0scOnRItUqJiIiIiIhIjqhA5+7ujkuXLiElJUVu++HDhwEAbdu2hZ6e/EoIhoaGaNq0KW7evKlapURERERERCRHVKALDg5GRkYG5s6dK9uWn5+PrVu3QiKRoGvXrgqPq1OnDtLS0lSrlIiIiIiIiOSIWlj8/fffxzfffIPFixcjOjoaPj4+OHnyJGJiYiCRSBAQEKDwuIyMDJiamqqlYCIiIiIiIiogKtDp6elh+/bt6Ny5M/bs2YO9e/fK2nr16gUXFxeFx127dg02NjaqVUpERERERERyRD1yCQD169fH9evXMWXKFLRo0QKNGzfGqFGjsGHDBoX7//XXX4iJiUG9evVUrZWIiIiIiIiKEHWHrpC5uTnmzp0r9y5dSZYtWwaJRAJfX9+ynIqIiIiIiIhKIPoOnViWlpbw8fFB9+7dNX0qIiIiIiKiKkXUHbo5c+bA09MTXl5eqFWrllLHKHMXj4iIiIiIiMQTFei+/PJLSCQSAICNjQ28vLxkAc/T0xO2trYaKZKIiIiIiIiKK9M7dADw+PHjYjNd1qhRQy7geXl5wcHBQS2FEhERERERkTxR79BVq1YNAODp6Ynly5fju+++w8CBA9GoUSPo6OggPj4e+/fvx7x58xAUFARnZ2fUqFEDb731FqZOnaqWgrdu3YpOnTrBwsICRkZGaN68ORYtWoScnByV+963bx8kEkmpi6QfP35ctk9JH99//73KtRAREREREb2OqDt00dHR+OyzzxAWFobLly9j+PDh+Pbbb2FpaYmsrCxcuXIFUVFRiIqKQmRkJG7cuIGkpCQcOnQIhw8fxrx581QqduzYsViyZAn09PTQpUsXGBsb4+jRo5g0aRJ2796NgwcPykKnWM+ePcOwYcMgkUggCMJr969VqxZ69OihsM3Nza1MNRAREREREYkhKtDZ2dlh8+bNGD16NEJDQ7Fy5Ups2bIFM2fOxMiRI9GqVSu0atVKtn9OTg6uXbsmC3mq2LFjB5YsWQJjY2OcOHECnp6eAIDExER06dIFp0+fxvTp0/HNN9+Uqf9PPvkET58+xYgRI7Bq1arX7t+gQYMS194jIiIiIiIqD2VatqB9+/aIioqSBZ/Q0FC4u7vj2LFjcvvp6+vD09MTQ4cOxcqVK1UqtPDu3uTJk2VhDgCsra1lfS9fvhypqami+w4PD8emTZswfvx4tGzZUqU6iYiIiIiIykuZ16GTSCQYPnw4bt++jdGjR+Off/5B165dERgYiJiYGHXWiLi4OFy4cAEAMGDAgGLtPj4+sLe3R3Z2Nvbt2yeq78TERIwYMQJubm6YNWuWWuolIiIiIiIqDyovLG5mZoalS5fi0qVL6Ny5M8LDw9G4cWN88cUXePHihTpqxKVLlwAULFLu7OyscJ8WLVrI7auskSNHIjExEevWrYOhoaHSxz19+hSzZs3C8OHD8emnn2LVqlV48OCBqHMTERERERGposzLFryqcePGOHz4MHbs2IEJEyZg/vz5CA8Px40bN1Tuu/COX2lLINjb28vtq4zNmzdj27Zt+PTTT9GuXTtRNf3zzz+YMWOG3DY9PT188sknWLRoEfT0Sv/WZmdnIzs7W/Z1WlqaqPMTERERERGpHOhyc3Px999/49q1a7KP7OxsCIKAhIQEddSI9PR0AICRkVGJ+xgbGwNQPhg9efIEo0ePhouLi6jZN83MzDB27Fj07t0brq6uMDU1xZ07d7B+/XosX74c3377LZ4/f441a9aU2s/8+fPx1VdfFdvet29f6OvrK12PpkRERMDPz6+iyyAtwjFDYnHMkFgcMyQWxwyJVVnGjJgl2UQFunv37uH69ety4e3mzZvIzc0FANl0/3Xq1EHPnj3Rtm1bMd2Xq48//hjPnj3D77//jurVqyt9nIeHBzw8POS2NW3aFIsXL4aPjw8CAwOxdu1ajBo1Cu7u7iX2M2XKFIwfP172dVpaGuzt7bFlyxaYmpqKvh518/Pzw65duyq6DNIiHDMkFscMicUxQ2JxzJBYlWXMpKWlwczMTKl9RQW6unXrQiKRACgIb7q6unBzc4O7u7vch7W1tfiqS2FiYgIAyMjIKHGf58+fA4BSYWjjxo3YvXs3Ro4ciU6dOqmlRgAICAiAu7s7Ll++jN27d5ca6KRSKaRSqdrOTUREREREVU+ZHrls3bo1PvzwQ/Tt21f2qKMmOTk5AQBiY2NL3KewrXDf0oSHhwMALly4UCzQPXnyBAAQGRkpa9u8eTNsbGyUqrVhw4a4fPkyHj58qNT+REREREREZSU60AmCgPPnz+P8+fP45JNP0KxZM3h4eMDT0xOenp5o1qyZ2t8BK3zEMSkpCTExMQpnurx48SIAyK1R9zqFxyiSkpKCEydOAACysrKU7jMpKQnAf3cViYiIiIiINEVUoAsLC0NkZCQiIyMRFRWF5ORkREREICIiQvYopr6+Pho1agRPT094eXnB09MTzZs3F7UkwKvs7Ozg7e2NCxcu4Ndff8W0adPk2k+fPo3Y2FhIpVL07Nnztf3t2LGjxLYNGzZgyJAh8PX1xeHDh0XVGRcXh1OnTgEAFygnIiIiIiKNE7UOXVBQEObPn4+DBw8iMTERMTEx2LZtG6ZMmYJu3brBysoKL1++xOXLl/Hjjz9izJgxaNu2LUxNTdGsWTOVCp06dSoAYMGCBYiKipJtT0pKwqhRowAAY8aMkXt5MDw8HA0aNICvr69K5y5qyZIlSExMLLb96tWreO+995CZmQkXFxf06tVLbeckIiIiIiJSRKVlCxwdHeHo6IiAgADZttjYWLm7eJGRkYiPj1d5PTp/f3+EhoZi6dKlaN26NXx9fWFkZIQjR44gJSUF7dq1w+zZs+WOSU1NRXR0tKhHJl9nxowZmDBhAtzd3eHs7AwdHR3cuXMHly5dQn5+PhwcHLB7925OeEJERERERBqntoXFC9nb28Pe3h7+/v6ybXFxcYiMjFS57yVLlqBdu3ZYsWIFzp49i5ycHLi4uGDy5MkYN24cDAwMVD7H60ybNg1nzpzBjRs3cOjQIWRkZMDU1BRt27ZFr169MHz4cL4/R0RERERE5ULtgU4RW1tb2NraqqWv4OBgBAcHK7VvSEgIQkJCRPX/umMmTpyIiRMniuqTiIiIiIhIE8oU6O7du4fLly/DwMAArVq1gpWV1WuPiY+PR82aNctyOiIiIiIiIlJA1KQogiBgzJgxqFevHgIDA/Hee+/Bzs4OX375pcL97927h8WLF6N9+/aws7NTS8FERERERERUQNQdunXr1mHlypX/Haynh+zsbMydOxfVqlXDlClTkJqaih9++AGbNm3ClStXABQEwcJlDYiIiIiIiEg9RN2h+/HHHyGRSDBixAg8efIEL1++xN27dxESEoK5c+fi+PHjcHFxweeff47Lly9DEATY2dlh7NixOH36tKaugYiIiIiIqEoSdYfuxo0bcHV1lbtL5+TkhHXr1iEnJwf+/v5IS0uDVCrFwIEDMXToULRq1UrtRRMREREREalCEIDsbOD5cyAjo+DPZ8/qIysLMDSs6OqUJyrQpaenw8PDQ2Hb5MmT8csvv0BPTw/Hjx9nkCMiIiIiIpUpCl5FP1e0Tdn2vLxXz/Y/3LsHNGhQARdaRqJnuTQsIa66ubkBADp06MAwR0RERERUxbwueIkJYa/uWzx4ac7z5+V3LnVQ2zp0urq6AMDZLImIiIiIKrGiwUtdd7oKP8/Pr+irK5meHmBiAhgZAcbG//1Z9PNjx3bBysqvoksVRXSg+/PPP7F48WI0adIETZs2Re3ateXadXREzbNCREREREQKlBS81BHCtCl4FQ1ciraV9Pmr2wwMXn9uP78f4Oz8hge6v//+GxMnTpR9bWFhgSZNmqBJkyYAgMzMTPVVR0RERERUySkTvMoawipz8NLXL1uwet2+ygQv+o+oQBcWFobIyEhERkYiKioKycnJSE5OxsmTJ3Hq1ClIJBKEhYVh//798PLyQsuWLWUfderU0dQ1EBERERG9liAAWVllf4+rtHZtCF5iQhaDl/YQFeiCgoIQFBQk+/r+/fuygFcY8hITE5GamoqjR4/i2LFjsn3r1KmD2NhY9VVORERERG+kV4NXWUPW1avfwtVVu4OXqiGMwevNp9KkKI6OjnB0dERAQIBsW2xsrFzAi4yMRHx8PB49eqRysURERERUeSgbvMryuKF6gpcL0tLU0Y+81wWvsjxuyOBFZaW2WS4L2dvbw97eHv7+/rJtcXFxiIyMVPepiIiIiEgJhcFL3TMaZmRU7jteOjo5MDPTV+vjhgxeVNmoPdApYmtrC1tb2/I4FREREZHWUhS81BXCKnPwMjBQzztdrwavoKBA7Nq1q6Ivj0ijyiXQEREREb1JXhe8VFlAWduDl9gQxjteRKphoCMiIqI3VtHgpc7HDbUheKlr7a6in+vrV/SVEdGrGOiIiIiowpUUvIp+fv/+W1i8WFwI07bgpa4QxuBFVHUw0BEREZHSlAleZbkTplzwGo0JE8rjKosrLXipEsIYvIhIVQx0REREbyBBADIzNbOAsiBU9NWVrDB4qfNxQwYvIqrMGOiIiIgq0KvBS52zG2pb8HpdsFq3bgmmTv20xH0ZvIioKmKgIyIiUoKywassIawyBy+pVD1rd706nXxZgtehQ0cQHPyp+i+SiEiLMdAREdEbpTB4qTJtfEnt2hK81LmAMu94ERFVbgx0RERUIQQBePFCfWt3aXvwUnUBZQYvIqKqiYGOiIhKpeiOlzpC2PPnO2BkVNFXVzJlgldZFlBm8CIiInVioCMiekMUDV7qXkBZM3e8dNTSi1SqmQWU9fgvJBERaQH+c0VEVM5KCl6qhjBteNSwaHB69CgaHh5uKocwBi8iIqrK+M8gEVEJlAleZXncUNuCl7oWUH41ePn5TcSuXbsq5iKJiIjeEAx0RKT1CifXUNedLm0LXupeQJl3vIiIiLQH/9kmonJTWvBSJYRpY/BSNYQxeBERERHAQEdECigbvIp+fvXqCHzwQekhrLIHL0NDzSygzOBFREREmsIfM4i0mKJ1vNSxgPKLF2UJXj3x88+auMriigYvdS6gzOBFRERE2oY/vhCVA00toFy24FV+Sgpeqi6gzOBFREREVIA/FhEVkZ+v/ALKYgLZmxC8XhfCvvhiLNau/U62vXp1Bi8iIiIiTeOPW6SVigYvdT5umJFR0VdWOkND9d3pUnfwMjO7C1dX1fshIiIiIuUx0JFGlRS8VA1h2hy8yhrCeMeLiIiIiF7FHw9JzsuXQGwskJbmhLNn1bOAcmWmbPASG8J0dSv6yoiIiIioKmCgIzk3bwJNmwLAUrRrV9HV/KcweKl7AWUGLyIiIiLSZgx0JMfYWLXjFQUvdSygzOBFRERERFQcAx3JsbAABgwATp/ej8DAHqIXUGbwIiIiIiIqPwx0JMfMDNi0CfDzW4nFi3tUdDlERERERFQKnYougIiIiIiIiMqGgY6IiIiIiEhLMdARERERERFpKQY6IiIiIiIiLcVAR0REREREpKUY6IiIiIiIiLQUAx0REREREZGWYqAjIiIiIiLSUgx0REREREREWoqBjoiIiIiISEsx0BEREREREWkpBjoiIiIiIiItxUBHRERERESkpRjoiIiIiIiItJReRRdABQRBAACkpaVVcCUFcnJyKk0tpB04ZkgsjhkSi2OGxOKYIbEqy5gprKEwI5RGIiizF2ncw4cPYW9vX9FlEBERERFRJREbGws7O7tS92GgqyTy8/Px6NEjmJiYQCKRVGgtaWlpsLe3R2xsLExNTSu0FtIOHDMkFscMicUxQ2JxzJBYlWnMCIKA9PR01KlTBzo6pb8lx0cuKwkdHZ3Xpu/yZmpqWuGDmbQLxwyJxTFDYnHMkFgcMyRWZRkzZmZmSu3HSVGIiIiIiIi0FAMdERERERGRlmKgo2KkUilmzJgBqVRa0aWQluCYIbE4ZkgsjhkSi2OGxNLWMcNJUYiIiIiIiLQU79ARERERERFpKQY6IiIiIiIiLcVAR0REREREpKUY6KqArVu3olOnTrCwsICRkRGaN2+ORYsWIScnp0z9RUZGok+fPqhVqxYMDQ3h7OyMTz75BPHx8WqunCqKusbMpUuXMH/+fPj6+qJWrVrQ19eHhYUF2rdvjxUrVpR5DFLlo+6/Z4rat28fJBIJJBIJunbtqoZqqTLQxJjZuXMn/Pz8YGNjAwMDA9SsWRNt27bFrFmz1Fg5VRR1jpmMjAzMnz8fLVq0gKmpKfT19WFjY4N3330Xu3bt0kD1VJ6io6OxbNkyhISEoGnTptDT04NEIsGcOXNU6vfw4cPo2bMnrK2tUa1aNTRo0ADTpk3D8+fP1VR5GQn0Rvv0008FAIKenp7QvXt3ISAgQDA3NxcACD4+PsKLFy9E9bd161ZBT09PACB4e3sLwcHBQt26dQUAQq1atYRbt25p6EqovKhrzOTk5AgABACCsbGx0LlzZ6Ffv36Cj4+PoKurKwAQWrZsKTx79kyzF0Qap+6/Z4pKTk4W6tSpI0gkEgGA4Ovrq8bKqaKoe8xkZ2cLffr0EQAI1apVE7p06SL0799f6Ny5s1CzZk3ByspKQ1dC5UWdYyYxMVFo1KiR7N+n7t27C8HBwYKnp6fs363Q0FANXg1pWuF4efVj9uzZZe5z8eLFAgBBIpEIHTp0EPr06SPY2NgIAAQ3NzchISFBjVcgDgPdGyw8PFz2l1VkZKRse0JCgtC0aVMBgDBhwgSl+4uLixOqV68uABBWr14t256bmysMHDhQFvLy8/PVeh1UftQ5ZnJycgQvLy8hLCxMyMrKkmu7evWqULt2bQGAMGTIELVeA5Uvdf8986r3339f0NXVFUaOHMlA94bQxJj54IMPBACCv79/sR+q8vLyhHPnzqmldqoY6h4zoaGhAgDBy8tLSEpKkmvbu3ev7BfXHDfaa+3atcJnn30mbNq0Sfj777+FQYMGqRTooqKiBIlEIujq6gr79u2Tbc/IyBB8fX0FAEJgYKC6yheNge4N5u3tLQAQ5syZU6zt1KlTAgBBKpUKKSkpSvU3ceJEAYDQtWvXYm3p6emCmZmZAEDYv3+/yrVTxVD3mCnNzz//LPtt+suXL1XujyqGJsfM9u3bBQDCxIkThfXr1zPQvSHUPWYOHz4sABCaNGnCv0veUOoeM02aNBEACGFhYQrbu3XrJgAQFi9erFLdVHkMHjxYpUBX+ATA0KFDi7Xdu3dP0NHREQAIf//9t6qllgnfoXtDxcXF4cKFCwCAAQMGFGv38fGBvb09srOzsW/fPqX6DA8PL7E/Y2Nj+Pn5AQC2b99e1rKpAmlizJTGw8MDAJCZmYnExESV+6Pyp8kxk5iYiBEjRsDNzY3vP71BNDFmli1bBgAYO3Ys9PX11VcsVQqaGDOGhoZK7Wdtba18ofTGevnyJfbu3QtA8Rh0dHREu3btAPz3s3J5Y6B7Q126dAkAYGlpCWdnZ4X7tGjRQm7f0qSnp+P27dtyx6nSH1U+6h4zr3Pr1i0AgIGBASwtLVXuj8qfJsfMyJEjkZiYiHXr1in9wxdVfuoeM3l5eThy5AgAoEOHDnjy5Am+++47jBw5EmPHjsXGjRsrfrICUokm/p55++23AQALFy5EcnKyXNu+fftw7Ngx2NjYyH5RTVXbzZs38eLFCwCV92dgvQo5K2lcTEwMAMDBwaHEfezt7eX2Lc29e/dkn5fUp5j+qPJR95gpjSAIWLRoEQDg3XffhVQqVak/qhiaGjObN2/Gtm3b8Omnn8p+60lvBnWPmbt378oC2/nz5zFq1KhiAW7ixInYvHkzunTpUtayqQJp4u+ZSZMmISIiAgcOHJDdXTE3N8ft27cRGRmJdu3aYd26dTAzM1P9AkjrFY4rc3NzmJiYKNynon8G5h26N1R6ejoAwMjIqMR9jI2NAQBpaWlK91dan2L6o8pH3WOmNF999RXOnTsHY2NjLFiwQKW+qOJoYsw8efIEo0ePhouLC+bNm6d6kVSpqHvMJCUlyT7/6KOP4OXlhQsXLiA9PR2XL19Gz549kZCQgF69esmeCiDtoom/Z4yMjLB792589tlnyMjIwIEDB7BlyxZERkbCysoKXbt2ha2trerF0xuhPH8+KisGOiIqVz/99BNmzZoFHR0d/Pjjj6hfv35Fl0SVyMcff4xnz57hhx9+QPXq1Su6HKrkBEGQfW5ra4sDBw6gRYsWMDY2RvPmzbFr1y40adIEz58/5y+PSObx48do164dli1bhjlz5sju9EZERMDLywtfffUVfHx85H6ZTVSZMdC9oQpvCWdkZJS4T+FjKaampkr3V1qfYvqjykfdY0aRrVu34sMPPwQArF27Fn369ClTP1Q5qHvMbNy4Ebt378aIESPQqVMntdRIlYsm/20KCQkp9vi2rq4uhg8fDqBgQWDSPpr4t2nw4MG4cOECZs+ejalTp8LZ2RlGRkbw9vbGnj170LRpU1y5cgXffPON6hdAWq88fj5SFd+he0M5OTkBAGJjY0vcp7CtcN/SODo6yj5/8OABmjZtqlJ/VPmoe8y8avv27RgwYADy8/OxevVqWbAj7aXuMVM4O9iFCxeKBbonT54AACIjI2Vtmzdvho2NjbiiqUKpe8w4OTlBIpFAEATUrVtX4T6F2x8/fiyuWKoU1D1m4uLicOjQIQBA//79i7Xr6+sjKCgI165dw+HDh/HVV1+JL5reKIXjKiUlBenp6Qrfo6von4F5h+4NVTglfFJSUokvaF68eBEA4Onp+dr+TE1NUa9ePbnjVOmPKh91j5miduzYgX79+iEvLw+rVq3CsGHDVCuWKgVNjZmLFy/ixIkTch/R0dEACv5BLdyWlZWl4hVQeVP3mDE2NoabmxsAlLj8SeH2wndcSLuoe8w8ePBA9nlJd1MKJ0N5dQZMqprc3NxkrwBU1p+BGejeUHZ2dvD29gYA/Prrr8XaT58+jdjYWEilUvTs2VOpPnv37l1if8+fP8fu3bsBAAEBAWUtmyqQJsYMAOzevRvBwcHIzc3FqlWrZI8/kfZT95jZsWMHBEFQ+LF+/XoAgK+vr2wbnwbQPpr4e6bw0e2SHqksvBvTsmXLspRMFUzdY6boZCd//vmnwn3Onz8PACUuk0BVi4GBAd555x0Aisfg/fv3cfbsWQD//axc7ipkOXMqF+Hh4QIAwdjYWIiMjJRtT0xMFJo2bSoAECZMmCB3zPbt2wU3NzehS5cuxfqLi4sTqlevLgAQ1qxZI9uem5srDBo0SAAgeHt7C/n5+Zq7KNIodY+ZvXv3CgYGBoJEIhFWr16t8fqp/Kl7zJRk/fr1AgDB19dXbbVTxVD3mElISBAsLCwEAML3338v1/bbb78JEolEACDs3btXMxdEGqfuMePt7S0AEBo2bCjExMTItf3888+yMfPzzz9r5Hqo/A0ePFgAIMyePbvEfZYtWya4ubkJgwYNKtYWGRkpSCQSQVdXV/jjjz9k2zMyMgRfX18BgBAYGKiR2pXBQPeGCw0NFQAI+vr6Qo8ePYTAwEDB3NxcACC0a9dOePHihdz+hT80OTo6KuwvLCxM0NXVFQAIrVq1Evr27SvUrVtXACDUqlVLuHXrVjlcFWmSusbM06dPBalUKgAQ7OzshMGDB5f4kZCQUI5XSOqm7r9nFGGge7Ooe8wcPHhQMDQ0FAAIjRs3FoKCggQPDw8BgABAmD59ejlcFWmSOsfMtWvXBGtrawGAYGhoKHTq1EkICgoSGjduLBszAwcO5C+otVhkZKTQqlUr2Ufhf287Ozu57Y8ePZIdM2PGDAGA0LFjR4V9Ll68WAAgSCQSoVOnTkJwcLBQu3ZtAYDg5uZWoT/LMNBVAVu2bBE6dOggmJqaCtWqVROaNGkiLFiwQMjOzi62rzI/aF28eFEICAgQatSoIRgYGAiOjo7C6NGjhSdPnmjwKqg8qWPMxMTEyP5hfN3Hq78hJe2j7r9nSjqGge7Noe4xEx0dLQwePFiwtbUV9PX1BSsrK6Fnz57CgQMHNHgVVJ7UOWaePHkiTJo0SWjWrJlgZGQk6OnpCTVq1BDeeustYcuWLRq+EtK0Y8eOif7543WBThAE4dChQ0KPHj0ES0tLQSqVCvXr1xemTJkipKWlaf6iSiERhCKLuBAREREREZHW4KQoREREREREWoqBjoiIiIiISEsx0BEREREREWkpBjoiIiIiIiItxUBHRERERESkpRjoiIiIiIiItBQDHRERERERkZZioCMiIiIiItJSDHRERERERERaioGOiIjoDbdhwwZIJBKEhIRUdClERKRmDHRERERERERaioGOiIiIiIhISzHQERERERERaSkGOiIiIgUyMzPxv//9D61bt4a5uTkMDQ3h5uaGzz//HElJSXL7Fn1HLSkpCaNHj4aDgwOkUikcHR0xbtw4PHv2rMRzRUREIDg4GHXq1IGBgQFq1qyJ9957D4cOHSq1xqNHj6JPnz6ws7ODVCpFjRo14O3tjRkzZhSrsVBGRgamTJmCevXqQSqVwsbGBoMHD0ZcXJz4bxIREVU4iSAIQkUXQUREVJk8evQIPXr0wLVr12BpaQlPT0+YmJggKioK9+/fh5OTE44fPw5HR0cABYFuyJAh8PPzw40bN5CUlIROnTpBIpHg+PHjePbsGdzc3HDq1CnUqFFD7lxr167FiBEjkJ+fDw8PDzRo0AD379/H2bNnAQAzZ87EjBkzitUYGhqKZcuWAQDc3d3RoEEDpKamIjo6Gnfv3sWxY8fQqVMnufr8/f1x9+5dPHjwAO3bt4eOjg7OnTuH+Ph4ODo64sqVKzAzM9Pgd5aIiNROICIiIpn8/HyhXbt2AgDho48+EtLS0mRtOTk5woQJEwQAQufOnWXb169fLwAQAAitW7cWkpKSZG3Pnj0T2rZtKwAQ+vXrJ3euq1evCnp6eoJEIhF++uknubZ9+/YJBgYGAgDh4MGDcm1Lly4VAAhWVlbC0aNHi13Dn3/+KTx48EBhfW+99ZaQmpoqa0tOThbc3d0FAMK8efNEfreIiKii8ZFLIiKiIg4cOIAzZ87A3d0d33//PUxMTGRtenp6WLRoEZo0aYJjx47h+vXrxY5ftWoVLC0tZV+bm5vj+++/h0QiQVhYGB4+fChrW7JkCXJzc9G7d28MGjRIrp+3334bH3/8MQDg66+/lm3Pzc3F7NmzAQBr1qxB586di9XQsmVL2NvbF9tuZGSE9evXw9TUVLbNwsICkydPBgAcPny49G8OERFVOgx0RERERezduxcAEBgYCD09vWLtOjo66NChAwDIHoss1Lx5c7i7uxc7pmnTpvDw8EB+fj5Onjwp2378+HEAKHF9uI8++ggAcOrUKeTl5QEAIiMjkZCQAGtra/Tu3VvUtbVo0QK1a9cutr1hw4YAwPfoiIi0EAMdERFREXfv3gUATJ8+HRKJROHHypUrAQAJCQlyxzo7O5fYb2Fb0Tt0hQGqpONcXFwAAFlZWbJJTu7fvw8AcHNzg0QiEXVtDg4OCrcX3rHLysoS1R8REVW84r96JKL/t3f3Lq2DYRiH75YO3fxAEKoICuLgRxF0U8RBQYVmcFVql2KmLG4i+gcILopxkQouokMWF2eHUMEhVlDB6qyg6ChqnFqa0+PQQTzp+V3jG940dLvz5HkfAP+xz89PSdLIyEg5UH2nt7e35vv7v3gWWTTKe1wAqDcEOgAAKpR6zwzD0NLSUk177+7uvr12f38vSWpvby+vtbW16fb2VsViUX19fVV7StXCeDxe7ssrVdlubm7k+37NVToAQH3hVR0AABWmpqYkSYeHhzVX0zzPk+d5VeuXl5c6Pz8P9N9JCowV+Jvd3V1J0ujoaLmfb2hoSC0tLXp8fJTjODU9HwCg/hDoAACoYBiGhoeHlc/nlclkqvrkJOn5+Vm2bev9/T2w7vu+TNMMDBF/eXmRaZryfV+zs7OB0ycty1IsFpPjONrf3w/c6+TkRDs7O5IUqBTGYjEtLy9LkrLZbOCQlZKzs7NArx4AoH7xySUAABWi0agcx9HMzIz29vZ0dHSkZDKpjo4Ovb29qVgs6uLiQh8fH1pYWAichJlKpVQoFNTV1aXx8fHyYPGnpyd1d3drc3Mz8Fv9/f3a2tqSaZqan5/XxsZGYLC47/taW1vT5ORkYJ9lWbq+vpZt2xobG9Pg4KB6enr0+vqqq6ur8mDxys87AQD1iUAHAMAfEomEXNdVLpfTwcGBPM9TPp9Xc3OzEomEFhcXlUqlFI/HA/uamprkuq5WVlZ0fHysh4cHtba2am5uTqurq4H5dCXZbFbJZFLr6+s6PT2V53lqaGjQ9PS0LMvSxMRE1Z5IJKLt7W0ZhiHbtuW6rgqFghobG9XZ2al0Oq2BgYEf+38AAP+OiP+bx20BAFAHcrmcMpmM0un0t/1wAAD8BHroAAAAACCkCHQAAAAAEFIEOgAAAAAIKXroAAAAACCkqNABAAAAQEgR6AAAAAAgpAh0AAAAABBSBDoAAAAACCkCHQAAAACEFIEOAAAAAEKKQAcAAAAAIUWgAwAAAICQ+gLN9JyB714JMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## initialization of the plot\n",
    "plt.grid(color='black', axis='y', linestyle='-', linewidth=0.5)    \n",
    "plt.grid(color='black', axis='x', linestyle='-', linewidth=0.5)   \n",
    "plt.grid(which='minor',color='grey', axis='x', linestyle=':', linewidth=0.5)     \n",
    "plt.grid(which='minor',color='grey', axis='y', linestyle=':', linewidth=0.5)    \n",
    "plt.xticks(fontsize=16); plt.yticks(fontsize=16)   \n",
    "plt.xlabel('epoch',fontsize=16 )\n",
    "plt.ylabel(r'$RMSE_{train}$ (yr), $RMSE_{test}$ (yr)', size = 16)\n",
    "## plotting the data\n",
    "plt.plot(epoch, MSEtrain**0.5, color = \"blue\", linewidth = 2., label = \"Training error\")\n",
    "plt.plot(epoch, MSEtest**0.5, color = \"orange\", linewidth = 2., label = \"Test error\")\n",
    "plt.title(\"Prediction error\", fontsize = 16)\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.legend(loc=\"upper right\", prop={'size': 15})\n",
    "plt.savefig(\"fig01.png\", dpi = 300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbd929-abc0-45f4-bc9b-a37a9959e4c5",
   "metadata": {},
   "source": [
    "<strong>Task 12:</strong> Complete the code to compute the outputs on the test dataset using the optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62ddcb1a-64d8-4921-aadd-92a9de3edf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (8, 1253)\n",
      "NpL: [6, 1]\n",
      "Nfx: 8\n",
      "Wts: 2\n",
      "n vaut: 1253\n",
      "NL vaut: 2\n",
      "WL shape: (6, 8)\n",
      "yLm1 shape: (8, 1253)\n",
      "bL shape: (1, 6)\n",
      "zL shape: (6, 1253)\n",
      "bL shape: (1, 6)\n",
      "n 1253\n",
      "WL shape: (1, 6)\n",
      "yLm1 shape: (6, 1253)\n",
      "bL shape: (1, 1)\n",
      "zL shape: (1, 1253)\n",
      "bL shape: (1, 1)\n",
      "n 1253\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$Age_\u001b[39m\u001b[38;5;132;01m{sim}\u001b[39;00m\u001b[38;5;124m$ (yr)\u001b[39m\u001b[38;5;124m'\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## plotting the data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytestsim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m30.\u001b[39m], [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m30.\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mgcf()\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\matplotlib\\pyplot.py:2798\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[0;32m   2794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[0;32m   2795\u001b[0m         x, y, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2796\u001b[0m         vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2797\u001b[0m         edgecolors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, plotnonfinite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2798\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[0;32m   2799\u001b[0m         x, y, s\u001b[38;5;241m=\u001b[39ms, c\u001b[38;5;241m=\u001b[39mc, marker\u001b[38;5;241m=\u001b[39mmarker, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   2800\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin, vmax\u001b[38;5;241m=\u001b[39mvmax, alpha\u001b[38;5;241m=\u001b[39malpha, linewidths\u001b[38;5;241m=\u001b[39mlinewidths,\n\u001b[0;32m   2801\u001b[0m         edgecolors\u001b[38;5;241m=\u001b[39medgecolors, plotnonfinite\u001b[38;5;241m=\u001b[39mplotnonfinite,\n\u001b[0;32m   2802\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2803\u001b[0m     sci(__ret)\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\matplotlib\\__init__.py:1433\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1433\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1435\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1436\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1437\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\matplotlib\\axes\\_axes.py:4524\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;66;03m# np.ma.ravel yields an ndarray, not a masked array,\u001b[39;00m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;66;03m# unless its argument is a masked array.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(x)\n\u001b[1;32m-> 4524\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\numpy\\ma\\core.py:6840\u001b[0m, in \u001b[0;36m_frommethod.__call__\u001b[1;34m(self, a, *args, **params)\u001b[0m\n\u001b[0;32m   6837\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m   6838\u001b[0m     a, args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], a\n\u001b[1;32m-> 6840\u001b[0m marr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6841\u001b[0m method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m   6842\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(marr), method_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\numpy\\ma\\core.py:8120\u001b[0m, in \u001b[0;36masanyarray\u001b[1;34m(a, dtype)\u001b[0m\n\u001b[0;32m   8118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, MaskedArray) \u001b[38;5;129;01mand\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m a\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   8119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[1;32m-> 8120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmasked_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\numpy\\ma\\core.py:2820\u001b[0m, in \u001b[0;36mMaskedArray.__new__\u001b[1;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;124;03mCreate a new masked array from scratch.\u001b[39;00m\n\u001b[0;32m   2813\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2817\u001b[0m \n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;66;03m# Process data.\u001b[39;00m\n\u001b[1;32m-> 2820\u001b[0m _data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m                 \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2822\u001b[0m _baseclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_baseclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(_data))\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;66;03m# Check that we're not erasing the mask.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Showing the results for the test dataset\n",
    "x_in = xtest_scl.T\n",
    "yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "ytestsim = yout\n",
    "Error_test = np.dot((ytest - ytestsim[0]).T,(ytest - ytestsim[0]))/ntest\n",
    "## Making a scatter plot\n",
    "## initialization of the plot\n",
    "plt.grid(color='black', axis='y', linestyle='-', linewidth=0.5)    \n",
    "plt.grid(color='black', axis='x', linestyle='-', linewidth=0.5)   \n",
    "plt.grid(which='minor',color='grey', axis='x', linestyle=':', linewidth=0.5)     \n",
    "plt.grid(which='minor',color='grey', axis='y', linestyle=':', linewidth=0.5)    \n",
    "plt.xticks(fontsize=16); plt.yticks(fontsize=16)   \n",
    "plt.xlabel(r'$Age_{obs}$ (yr)',fontsize=16 )\n",
    "plt.ylabel(r'$Age_{sim}$ (yr)',fontsize=16 )\n",
    "## plotting the data\n",
    "plt.scatter(ytest, ytestsim, color = \"red\", marker = \"o\")\n",
    "plt.plot([0., 30.], [0., 30.], color='k', linestyle='-', linewidth=2)\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.savefig(\"fig02.png\", dpi = 300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf23d2a-355f-4269-9007-e059cfe4a7d1",
   "metadata": {},
   "source": [
    "## <strong>Part 3: Make your life easier with Keras</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fac845-9b3c-46a7-b3d1-cebf25f94e0a",
   "metadata": {},
   "source": [
    "In this part, an implementation of a DNN using functions from Keras is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff88e48-d89c-410b-b456-50399d21c3b5",
   "metadata": {},
   "source": [
    "### Step 1: Build the ANN architecture using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759ba1c-824b-4b79-9246-39f3cf54ec70",
   "metadata": {},
   "source": [
    "<strong>Task 13:</strong> Modify the following function to implement the same ANN as the one implemented in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d365ca70-94e2-4806-8636-ae1aba182817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_keras(shape):\n",
    "  \n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "  model.add(keras.layers.Dense(3, activation='logistic', name='HiddenLayer01'))\n",
    "  model.add(keras.layers.Dense(1, activation='logistic', name='HiddenLayer02'))\n",
    "  model.add(keras.layers.Dense(1, name='Output'))\n",
    "  \n",
    "  model.compile(optimizer = 'adam',\n",
    "                loss      = 'mse',\n",
    "                metrics   = ['mae', 'mse'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13627a-f857-49fd-bd4e-2e658d475628",
   "metadata": {},
   "source": [
    "The following lines help create and instantiate an ANN using the function ```ANN_keras```.\n",
    "\n",
    "<strong>Task 14:</strong> Use ```Nfx``` to create a \"copy\" of the ANN defined using ```ANN_keras```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b572375-3211-406c-830e-66b6ed49c0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function: 'logistic'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mANN_keras\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mNfx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mANN_keras\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogistic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHiddenLayer01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHiddenLayer02\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\keras\\layers\\core\\dense.py:125\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived an invalid value for `units`, expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma positive integer. Received: units=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias \u001b[38;5;241m=\u001b[39m use_bias\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer)\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\keras\\activations.py:609\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m callable(identifier):\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m identifier\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\keras\\activations.py:568\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# we put 'current_module' after 'activation_layers' to prefer the local one\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# if there is a collision\u001b[39;00m\n\u001b[0;32m    562\u001b[0m generic_utils\u001b[38;5;241m.\u001b[39mpopulate_dict_with_module_objects(\n\u001b[0;32m    563\u001b[0m     activation_functions,\n\u001b[0;32m    564\u001b[0m     (activation_layers, current_module),\n\u001b[0;32m    565\u001b[0m     obj_filter\u001b[38;5;241m=\u001b[39mcallable,\n\u001b[0;32m    566\u001b[0m )\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ENSEEIHT\\BigData\\tp_ann\\lib\\site-packages\\keras\\saving\\legacy\\serialization.py:557\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    555\u001b[0m     obj \u001b[38;5;241m=\u001b[39m module_objects\u001b[38;5;241m.\u001b[39mget(object_name)\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# Classes passed by name are instantiated with no args, functions are\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# returned as-is.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown activation function: 'logistic'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "model= ANN_keras( (Nfx,) )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279dd64-51f1-443e-be11-744617509b70",
   "metadata": {},
   "source": [
    "### Step 2: Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df30be7-f04e-45de-8ce4-80298793b713",
   "metadata": {},
   "source": [
    "Now, the model is ready for training. The following function launches the training of the ANN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d22521-ceb5-4f33-94f9-21abf60d973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x train,\n",
    "                    y train,\n",
    "                    epochs          = 50,\n",
    "                    batch_size      = length of training dataset,\n",
    "                    validation_data = (x test, y test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f2dab-fec3-4497-94a2-a1460c8f02dc",
   "metadata": {},
   "source": [
    "The test scores are computed using the optimized parameter set on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd647516-9782-44a6-831a-f3d1d220d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(xtest_scl, ytest, verbose=0)\n",
    "\n",
    "print('test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe857-d3fd-48da-a9dc-a75e686f8b68",
   "metadata": {},
   "source": [
    "We can make a prediction with the Keras-built ANN and compare it with that from your own ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b3e22-cd95-4261-bd58-acef44622bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [ 1, 0.3, 0.1, 0.2, 0.2, 0.5, 0.7, -1. ]\n",
    "mydata = np.array(mydata).reshape(1,8)\n",
    "print(mydata.shape)\n",
    "predictions = model.predict(mydata)\n",
    "print(\"predicted age using Keras-built network: \", round(predictions[0,0],2), \"yr\")\n",
    "yout, zout = ANN(x = mydata.T, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "print(\"predicted age using your neural network: \", round(yout[NL-1][0,0], 2), \"yr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e237b49-85bd-475d-b536-3cfc4f1f67b0",
   "metadata": {},
   "source": [
    "Finally, we can use parameters estimated by Keras and your neural network to test whether you get the same estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813e857-e6b8-4da4-92df-d8f406849736",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keras = model.get_weights()\n",
    "len_par = len(param_keras)\n",
    "wts_keras = [param_keras[i].T for i in np.arange(0, len_par, 2)]\n",
    "bias_keras = [param_keras[i].reshape(param_keras[i].shape[0], 1) for i in np.arange(1, len_par, 2)]\n",
    "ActivFun_Keras = ['relu', 'relu', 'relu', 'linear']\n",
    "NpL_Keras = [16,32,16,1]\n",
    "Nfx = 8\n",
    "yout_k, zout_k = ANN(x = mydata.T, NpL = NpL_Keras, Nfx = Nfx, Wts = wts_keras, bias = bias_keras, ActivFun = ActivFun_Keras)\n",
    "print(\"predicted age using Keras-built network: \", round(predictions[0,0],2), \"yr\")\n",
    "print(\"predicted age using your neural network and Keras-estimated parameters: \", round(yout_k[len(NpL_Keras)-1][0,0], 2), \"yr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
