{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad1ce57-67fb-4b22-bc02-8bd8b7aad9eb",
   "metadata": {
    "tags": []
   },
   "source": [
    " # <strong> Introduction to artificial neural networks </strong>\n",
    " Spring 2023 - Toulouse INP/ENSEEIHT<br /> \n",
    " by Ruming PAN & Mohamed SAADI<br /> \n",
    " Last update: 20-01-2023<br /> \n",
    " \n",
    " This notebook contains three parts:<br />\n",
    " <ol>\n",
    "  <li><strong>Part 1: Building a neural network</strong>, in which a function ```ANN``` builds a DNN (dense neural network) knowing the number of layers (hidden + output), the number of neurons per layer, and given the parameters (weights and biases) for each layer. You are asked to implement an additional activation function (e.g., linear, ReLU) following the example given for the logistic function and test them in new ANN.</li>\n",
    "  <li><strong>Part 2: Training a neural network</strong>, where a set of data \"abalone_data.xlsx\" is given to train a DNN by back-propagating the gradient of a loss function with respect to the network parameters. You are required to do three things: (1) define the training and test datasets, (2) scale the features, and most importantly (3) complete the function ```ANN_backpro``` to successfully run the algorithm. </li>\n",
    "  <li><strong>Part 3: Make your life easier with Keras</strong>, where you are asked to implement a similar DNN using ```keras```, which is a high-level interface for ```TensorFlow```, a famous framework for deep machine learning. The objective here is to present one of the most used libraries in python-based machine learning eco-system. As you will notice, the implementation is very friendly and easy.</li>\n",
    "</ol> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d764ef-555f-4788-a195-a33fdd3767bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <strong>Part 1: Building a neural network</strong>\n",
    " In this part, you are required to understand the structure of a neural network and build one given a number of layer ```NL``` and a list of number of neurons per layer ```NpL```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aae1b-d6d9-4cb8-97a9-c9fae30f3fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Import libraries, define global functions and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14d628-1419-4e93-9930-a70e5f6f87e3",
   "metadata": {},
   "source": [
    "<strong>Task 1:</strong> Based on the example provided for the logistic function, build new activation functions (tanh, ReLU, and linear) in order to use them later for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3523df9-46de-4f00-95be-12eb6bf168d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "## a function that takes a dataframe and returns some basic stastics of its columns\n",
    "def datasum(df):\n",
    "    dfmin = df.apply(np.min, 0)\n",
    "    dfmedian = df.apply(np.median, 0)\n",
    "    dfmean = df.apply(np.mean,0)\n",
    "    dfmax = df.apply(np.max, 0)\n",
    "    dfstd = df.apply(np.std, 0)\n",
    "    dfsummary = pd.DataFrame(np.array([dfmean, dfstd, dfmin, dfmedian, dfmax]), columns=list(df.columns[:]))\n",
    "    dfsummary[\"Statistic\"] = [\"mean\", \"std\", \"min\", \"median\", \"max\"]\n",
    "    xcol = dfsummary.shape[1]\n",
    "    colarr = [dfsummary.columns[xcol-1]]\n",
    "    for item in dfsummary.columns[0:(xcol-1)]:\n",
    "        colarr.append(item)\n",
    "    dfto = dfsummary[colarr]\n",
    "    return dfto\n",
    "\n",
    "## activation functions and their derivatives\n",
    "def logistic(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    t = np.exp(-x)\n",
    "    if mode == \"n\":\n",
    "        fx = 1/(1+t)\n",
    "    if mode == \"d\":\n",
    "        fx = t/(1+t)**2\n",
    "    return fx\n",
    "\n",
    "## define new functions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756a2c4-d0aa-406c-83e5-7cb7612118f8",
   "metadata": {},
   "source": [
    "#### Hyperparameters: number of layers, size of each layer, and activation function per layer\n",
    "<strong>Indication:</strong> The hyperparameters ```NL```, ```NpL``` and ```Nfx``` are very important throughout the code. They define the number of layers, the number of neurons per each layer, and the number of input features. Always make sure that the size of ```NpL``` and ```ActivFun``` is equal to ```NL```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad18c0-1f28-4b53-a7ed-d7afc91d3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NL is the number of layers including the hidden layers AND the output layer\n",
    "NL = 2\n",
    "## NpL defines the number of neurones per each layer. The length of NpL should be exactly equal to NL\n",
    "NpL = [3, 1]\n",
    "## ActivFun defines the activation functions per each layer\n",
    "ActivFun = ['logistic', 'logistic']\n",
    "\n",
    "## Add the number of features x\n",
    "Nfx = 2\n",
    "## List of size of layers -1: important for automatic definition of parameters\n",
    "NpLm1 = [Nfx]\n",
    "for iL in np.arange(len(NpL)-1):\n",
    "    NpLm1.append(NpL[iL])\n",
    "print(\"Number of neurons per layer: \", NpL)\n",
    "print(\"Number of neurons from the previous layer: \", NpLm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534c71d-16a5-4983-91ca-a17eaf93d704",
   "metadata": {},
   "source": [
    "#### Initialization of parameters\n",
    "<strong>Task 2:</strong> Initialize the parameters (weights and biases) using the function ```np.random.rand``` (https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html). Convenient initial parameter values should be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b7284-6901-4ebb-ad03-7a4f084b38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of weights and biases\n",
    "Wts = []\n",
    "bias = []\n",
    "for iL in np.arange(len(NpL)):\n",
    "    ## random initialization\n",
    "    ## the initial parameters should be between -1 and 1\n",
    "    ## use the function np.random.rand to make random initializations (but these will be between 0 and 1)\n",
    "    WL = \n",
    "    bL = \n",
    "    ## appending\n",
    "    Wts.append(WL)\n",
    "    bias.append(bL)\n",
    "print(Wts[0].shape)\n",
    "print(bias[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1489f56-3100-4d8d-b896-ec61e8fefd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Construct the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8ca49-503a-42b0-aff5-8ac97870d7f3",
   "metadata": {},
   "source": [
    "<strong>Task 3:</strong> Complete this function with the equations for each layer to compute the vectors $z_{L}$ and their activation $y_{L} = \\sigma(z_{L})$. The function should return a list ```z``` and a list ```y``` that have ```NpL``` arrays each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fa3c3-b925-4139-94bf-d8926b19a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(x, NpL, Nfx, Wts, bias, ActivFun):\n",
    "    '''\n",
    "    This function computes the output of a neural network containing NL layers\n",
    "    where NL is the length of NpL. \n",
    "    o NpL contains the number of neurons per each hidden layer + the output layer. \n",
    "    o Nfx is the number of input features.\n",
    "    o Wts is a list that contains the 2D arrays of weights for each layer.\n",
    "    Each 2D array of weights has dimensions nL x nL-1, nL being the number of neurons\n",
    "    of current layer L, and nL-1 the number of neurons (or features) of layer L-1.\n",
    "    o bias is a list of 1D arrays of weights for each layer.\n",
    "    Each 1D array has dimensions nL x 1. When there are many data points (or members), say\n",
    "    n data points, the bias should be repeated using the function np.tile.\n",
    "    o ActivFun contains the name of the activation function for each layer.\n",
    "    \n",
    "    '''\n",
    "    n = x.shape[1]\n",
    "    yLm1 = x\n",
    "    ## z is a list that saves the zL arrays for each hidden and output layer\n",
    "    z = []\n",
    "    ## similarly, y is a list that saves the yL arrays. Specifically, yL[NL-1] contains the output.\n",
    "    y = []\n",
    "    for iL in np.arange(len(NpL)):\n",
    "        ## parameters\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "        ## multiplication\n",
    "        ## make sure that the operation is correct for n individual points.\n",
    "        ## the dimension of zL should be nL x n. Since bL is nL x 1, use np.tile to overcome this issue.\n",
    "        zL =  \n",
    "        z.append(\n",
    "        ## activation\n",
    "        ## to call a function given its name, use the function fx = globals()[\"fun_name\"]\n",
    "        sigma = globals()[ActivFun[iL]]\n",
    "        yL = \n",
    "        y.append(\n",
    "        ## move to next layer\n",
    "        yLm1 = \n",
    "    return y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5a04e-934c-4581-959a-80a248f88955",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Verify that the ANN works for a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c4cfb-ec16-4965-bb56-4c58f0ed574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input: 4 data points x 2 features (Nfx = 2)\n",
    "input_features = np.array([[0,0], \n",
    "                           [0,1], \n",
    "                           [1,0], \n",
    "                           [1,1]])\n",
    "print(input_features.shape)\n",
    "print(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e5650-c1ab-45b5-964c-ae5fa86d1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: 4 data points x 1 feature\n",
    "target_output = np.array([[0], [1], [1], [1]])\n",
    "print(target_output.shape)\n",
    "print(target_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3007438-db04-41c0-b8ff-f8979d623d60",
   "metadata": {},
   "source": [
    "#### Generate outputs for the 4 individuals\n",
    "Run this cell to test your neural network. It should deliver an output ysim of shape (n,1), where n is the number of data points (NOT features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5b6d4-42df-4369-b1cb-7c810591382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This where you can test your neural network\n",
    "x_in = input_features.T\n",
    "y,z = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "ysim = y[NL-1].T\n",
    "print(ysim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e5665-4ef6-4d0f-ac01-b433335d741b",
   "metadata": {},
   "source": [
    "#### Goodness of fit: MSE and RMSE\n",
    "<strong>Task 4:</strong> Complete the equation of MSE and RMSE to estimate the errors of the simulation output of the neural network. Use the matrix product to compute the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566beb62-ba89-4d96-9296-0cdb112551d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = target_output.shape[0]\n",
    "MSE = \n",
    "print(MSE)\n",
    "RMSE = \n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342e97d-0aa0-425d-a088-5d56c0e91205",
   "metadata": {},
   "source": [
    "## <strong>Part 2: Training a neural network</strong>\n",
    " In this part, a backpropagation algorithm is implemented to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60de4bf-e0f8-43e4-b46e-c19178980f89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Read the data, define the training and the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81668774-7c27-4b55-bb2d-56c847cb8dec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading the dataset\n",
    "Any machine learning task involves preparing/preprocessing the data to make it ready for digestion by the machine learning algorithm.\n",
    "\n",
    "At this stage, the data has a 2D shape (lines = individuals or data points, columns = features). The dimensions of the shape of the data help you define the structure of your neural network.\n",
    "\n",
    "The \"abalone_data.xlsx\" dataset that are used for this exercise can be downloaded from https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/. They contain measurements of the following properties of 4177 members of marine snails (<em>haliotis</em>):\n",
    "<ol>\n",
    "  <li><em>Type</em>: male (1), immature (0), female (-1); </li>\n",
    "  <li><em>LongestShell_mm</em>: longest shell measurement in mm; </li>\n",
    "  <li><em>Diameter_mm</em>: length perpendicular to LongestShell in mm;</li>\n",
    "  <li><em>Height_mm</em>: height of the member with meat in shell in mm;</li>\n",
    "  <li><em>WholeWeight_g</em>: mass of the whole abalone in g;</li>\n",
    "  <li><em>ShuckedWeight_g</em>: weight of meat in the abalone in g;</li>  \n",
    "  <li><em>VisceraWeight_g</em>: gut weight of the abalone (after bleeding) in g;</li>\n",
    "  <li><em>ShellWeight_g</em>: weight of the abalone after being dried in g;</li>\n",
    "  <li><em>Age_yr</em>: age of the abalone in years.</li>\n",
    "</ol> \n",
    " \n",
    "The objective of the exercise is to succeed at predicting the age of the abalone knowing its gender, form, and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042e30d-1c19-4ec6-9a06-cf99eb3998b8",
   "metadata": {},
   "source": [
    "<strong>Task 5:</strong> Complete the first instruction ```pd.read_excel``` to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2512e-0bad-4597-99f8-f38ec666b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(, sheet_name= )\n",
    "display(df.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "dfsum = datasum(df)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "display(dfsum.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa151f77-adf9-44d7-abcf-dd0138ac06d9",
   "metadata": {},
   "source": [
    "#### Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b7b25-91fa-42b1-8199-668c2197ca6f",
   "metadata": {},
   "source": [
    "<strong>Task 6:</strong> After reading the dataset, complete the instructions using ```df.sample``` and ```df.drop``` to split the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a38bc-be46-4b48-8b6d-99e0793f23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## percentage of data to be used for training\n",
    "percTrain = 0.7\n",
    "\n",
    "## index of training and test\n",
    "#trainindex = np.random.rand(len(df)) < percTrain\n",
    "dftrain = \n",
    "dftest =  \n",
    "print(len(dftrain)/len(df))\n",
    "print(len(dftest)/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79be5dc-1211-4f43-a0d5-4c003f50cf03",
   "metadata": {},
   "source": [
    "#### Defining features and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a21213-5d5a-48eb-9ccf-0c0b4702365f",
   "metadata": {},
   "source": [
    "<strong>Task 7:</strong> Complete the instructions to define the target/output/dependent variable (age of the abalone) and the attributes/input/independent variables (remaining variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2686eaf-5431-45fe-9c35-a4423890b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = \n",
    "xtrain = \n",
    "#print(xtrain.head())\n",
    "#print(ytrain.shape)\n",
    "ytest = \n",
    "xtest = \n",
    " \n",
    "## printing some information\n",
    "print('Shape of original data : ', df.shape)\n",
    "print('xtrain : ',xtrain.shape, 'ytrain : ',ytrain.shape)\n",
    "print('xtest  : ',xtest.shape,  'ytest  : ',ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe48c69-9b0f-425f-9bde-f197b6985645",
   "metadata": {},
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11566c-b3bd-402f-a72e-9edac0bcb137",
   "metadata": {},
   "source": [
    "<strong>Task 8:</strong> Estimate the mean and standard deviation for each column/feature from the <ins>train</ins> dataset and use them to scale both the train and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da817fc5-9145-4ce1-8e01-e681db1ee91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimate the mean and the standard deviation from the train dataset\n",
    "xmean = \n",
    "xstd =\n",
    "## scaling\n",
    "xtrain_scl =\n",
    "xtest_scl ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de3846-0fc6-4bed-bc0b-e3f24db3398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_summary = datasum(xtrain)\n",
    "x_test_summary = datasum(xtest)\n",
    "x_train_scl_summary = datasum(xtrain_scl)\n",
    "x_test_scl_summary = datasum(xtest_scl)\n",
    "## convert to arrays\n",
    "xtrain_scl, ytrain = np.array(xtrain_scl), np.array(ytrain)\n",
    "xtest_scl, ytest = np.array(xtest_scl), np.array(ytest)\n",
    "## print the dataset before and after scaling\n",
    "display(x_train_summary.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset - before scaling\"))\n",
    "display(x_train_scl_summary.style.hide(axis = \"index\").set_caption(\"Statistics of the dataset - after scaling\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa1da9-2f7c-42d9-9545-a82495751e05",
   "metadata": {},
   "source": [
    "### Step 2: Build the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a2149-4fb3-4ddb-adb7-2078324fecbe",
   "metadata": {},
   "source": [
    "<strong>Task 9:</strong> Complete the following function to train the neural network using the gradient descent method based on backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91295f-036a-4c83-9075-2498dfaa53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_backpro(x, ytrue, NpL, Nfx, Wts, bias, ActivFun, lr):\n",
    "    '''\n",
    "    o Shape of x: Nfx * n, where n is the number of data points, and Nfx the number of features\n",
    "    o Shape of ytrue: n * 1, where n is the number of data points.\n",
    "    \n",
    "    '''\n",
    "    ## step 1: feed forward\n",
    "    n = x.shape[1]\n",
    "    yLm1 = x\n",
    "    z = []\n",
    "    y = []\n",
    "    for iL in np.arange(len(NpL)):\n",
    "        ## get the parameters for the current layer\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "        ## estimate zL from yLm1\n",
    "        zL =  \n",
    "        z.append(\n",
    "        ## activation: estimate yL from zL\n",
    "        sigma = \n",
    "        yL = \n",
    "        y.append(\n",
    "        ## move to next layer\n",
    "        yLm1 = \n",
    "\n",
    "    ## step 2: backpropagation\n",
    "    ytrue = ytrue.T\n",
    "    dJ_dy = \n",
    "    for iL in reversed(np.arange(len(NpL))):\n",
    "        ## getting zL of the current layer\n",
    "        zL = \n",
    "        ## estimating dJ_dz from dJ_dy\n",
    "        sigma = \n",
    "        dJ_dz = \n",
    "        ## getting the parameters of current layer\n",
    "        WL = \n",
    "        bL = \n",
    "       \n",
    "        ## estimating dJ_dW from dJ_dz\n",
    "        ## dJ_dz : (nL x n)\n",
    "        ## yLm1 : (nL-1 x n)\n",
    "        ## getting yL-1\n",
    "        if(iL == 0):\n",
    "            yLm1 =\n",
    "        else:\n",
    "            yLm1 = \n",
    "        dJ_dW = \n",
    "        \n",
    "        ## estimating dJ_db from dJ_dz\n",
    "        ## dJ_db : nL x 1\n",
    "        ## dJ_dz : nL x n\n",
    "        dJ_db =\n",
    "        \n",
    "        ## backpropagating the gradient from layer L to layer L-1\n",
    "        ## WL : nL x nL-1\n",
    "        ## dJ_dz : nL x n\n",
    "        ## dJ_dy (L-1) : nL-1 x n\n",
    "        dJ_dy =\n",
    "        \n",
    "        ## Updating the parameters\n",
    "        WL = \n",
    "        bL = \n",
    "        Wts[iL] = \n",
    "        bias[iL] = \n",
    "    \n",
    "    return Wts, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3927e-e855-4dda-9272-c4453f22c6a9",
   "metadata": {},
   "source": [
    "<strong>Task 10:</strong> Complete the initialization of parameters (exactly the same as in part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082f3a-ec71-4502-95af-6e52d8c4412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NL is the number of layers including the hidden layers AND the output layer\n",
    "NL = 2\n",
    "## NpL defines the number of neurones per each layer. The length of NpL should be exactly equal to NL\n",
    "NpL = [6,1]\n",
    "## Add the number of features x\n",
    "Nfx = 8\n",
    "## ActivFun defines the activation functions per each layer\n",
    "ActivFun = ['logistic', 'relu']\n",
    "\n",
    "## List of size of layers -1: important for automatic definition of parameters\n",
    "NpLm1 = [Nfx]\n",
    "for iL in np.arange(len(NpL)-1):\n",
    "    NpLm1.append(NpL[iL])\n",
    "print(\"Number of neurons per layer: \", NpL)\n",
    "print(\"Number of neurons from the previous layer: \", NpLm1)\n",
    "\n",
    "## Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "## Number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "## List of weights and biases\n",
    "Wts = []\n",
    "bias = []\n",
    "for iL in np.arange(len(NpL)):\n",
    "    ## random initialization\n",
    "    WL =\n",
    "    bL = \n",
    "    ## appending\n",
    "    Wts.append(\n",
    "    bias.append("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75e783-8bbc-4fdd-85ca-45edd9a9e8e0",
   "metadata": {},
   "source": [
    "<strong>Task 11:</strong> Complete the following lines in order to keep track of (1) epochs, (2) training error (MSE), and (3) test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a36fc6-aeec-4a24-8318-48283781d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate: \", lr, \"  Number of epochs: \", epochs)\n",
    "MSEtrain = np.array([])\n",
    "MSEtest = np.array([])\n",
    "epoch = np.array([])\n",
    "sys.stdout.write('\\r')\n",
    "for iepoch in np.arange(epochs):\n",
    "    epoch = np.append(epoch, iepoch)\n",
    "    ## train the neural network\n",
    "    x_in = \n",
    "    Wts, bias = ANN_backpro(x = x_in, ytrue = ytrain, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun, lr = lr)\n",
    "        \n",
    "    ## estimate the MSE for the train dataset\n",
    "    x_in = \n",
    "    yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "    ytrainsim = \n",
    "    ntrain = ytrain.shape[0]\n",
    "    Error_train = \n",
    "    \n",
    "    ## estimate the MSE for the test dataset\n",
    "    x_in = \n",
    "    yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "    ytestsim = \n",
    "    ntest = ytest.shape[0]\n",
    "    Error_test = \n",
    "   \n",
    "    ## keeping track of the errors\n",
    "    MSEtrain = np.append(MSEtrain, Error_train[0,0])\n",
    "    MSEtest = np.append(MSEtest, Error_test[0,0]) \n",
    "    \n",
    "    ## print the evolution\n",
    "    sys.stdout.write('\\r' \"Epoch: \" + str(int(iepoch + 1)).rjust(5,'0') + \"/\"\n",
    "                    + str(int(epochs)).rjust(5,'0') + \" \" +\n",
    "                    \"Training error: \" + str(round(Error_train[0,0],2)) + \n",
    "                    \"  Test error: \" + str(round(Error_test[0,0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab8eb3-e3ca-4573-9041-347e5ab30ce1",
   "metadata": {},
   "source": [
    "Now, we can see the evolution of the training and test errors across the epochs. Note that only the RMSE (in years) is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917284a-2611-425a-93f6-44e6a4609b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialization of the plot\n",
    "plt.grid(color='black', axis='y', linestyle='-', linewidth=0.5)    \n",
    "plt.grid(color='black', axis='x', linestyle='-', linewidth=0.5)   \n",
    "plt.grid(which='minor',color='grey', axis='x', linestyle=':', linewidth=0.5)     \n",
    "plt.grid(which='minor',color='grey', axis='y', linestyle=':', linewidth=0.5)    \n",
    "plt.xticks(fontsize=16); plt.yticks(fontsize=16)   \n",
    "plt.xlabel('epoch',fontsize=16 )\n",
    "plt.ylabel(r'$RMSE_{train}$ (yr), $RMSE_{test}$ (yr)', size = 16)\n",
    "## plotting the data\n",
    "plt.plot(epoch, MSEtrain**0.5, color = \"blue\", linewidth = 2., label = \"Training error\")\n",
    "plt.plot(epoch, MSEtest**0.5, color = \"orange\", linewidth = 2., label = \"Test error\")\n",
    "plt.title(\"Prediction error\", fontsize = 16)\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.legend(loc=\"upper right\", prop={'size': 15})\n",
    "plt.savefig(\"fig01.png\", dpi = 300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbd929-abc0-45f4-bc9b-a37a9959e4c5",
   "metadata": {},
   "source": [
    "<strong>Task 12:</strong> Complete the code to compute the outputs on the test dataset using the optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddcb1a-64d8-4921-aadd-92a9de3edf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing the results for the test dataset\n",
    "x_in = \n",
    "yout, zout = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "ytestsim = \n",
    "Error_test = \n",
    "## Making a scatter plot\n",
    "## initialization of the plot\n",
    "plt.grid(color='black', axis='y', linestyle='-', linewidth=0.5)    \n",
    "plt.grid(color='black', axis='x', linestyle='-', linewidth=0.5)   \n",
    "plt.grid(which='minor',color='grey', axis='x', linestyle=':', linewidth=0.5)     \n",
    "plt.grid(which='minor',color='grey', axis='y', linestyle=':', linewidth=0.5)    \n",
    "plt.xticks(fontsize=16); plt.yticks(fontsize=16)   \n",
    "plt.xlabel(r'$Age_{obs}$ (yr)',fontsize=16 )\n",
    "plt.ylabel(r'$Age_{sim}$ (yr)',fontsize=16 )\n",
    "## plotting the data\n",
    "plt.scatter(ytest, ytestsim, color = \"red\", marker = \"o\")\n",
    "plt.plot([0., 30.], [0., 30.], color='k', linestyle='-', linewidth=2)\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.savefig(\"fig02.png\", dpi = 300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf23d2a-355f-4269-9007-e059cfe4a7d1",
   "metadata": {},
   "source": [
    "## <strong>Part 3: Make your life easier with Keras</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fac845-9b3c-46a7-b3d1-cebf25f94e0a",
   "metadata": {},
   "source": [
    "In this part, an implementation of a DNN using functions from Keras is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff88e48-d89c-410b-b456-50399d21c3b5",
   "metadata": {},
   "source": [
    "### Step 1: Build the ANN architecture using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759ba1c-824b-4b79-9246-39f3cf54ec70",
   "metadata": {},
   "source": [
    "<strong>Task 13:</strong> Modify the following function to implement the same ANN as the one implemented in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365ca70-94e2-4806-8636-ae1aba182817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_keras(shape):\n",
    "  \n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "  model.add(keras.layers.Dense(16, activation='relu', name='HiddenLayer01'))\n",
    "  model.add(keras.layers.Dense(1, name='Output'))\n",
    "  \n",
    "  model.compile(optimizer = 'adam',\n",
    "                loss      = 'mse',\n",
    "                metrics   = ['mae', 'mse'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13627a-f857-49fd-bd4e-2e658d475628",
   "metadata": {},
   "source": [
    "The following lines help create and instantiate an ANN using the function ```ANN_keras```.\n",
    "\n",
    "<strong>Task 14:</strong> Use ```Nfx``` to create a \"copy\" of the ANN defined using ```ANN_keras```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b572375-3211-406c-830e-66b6ed49c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ANN_keras( (,) )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279dd64-51f1-443e-be11-744617509b70",
   "metadata": {},
   "source": [
    "### Step 2: Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df30be7-f04e-45de-8ce4-80298793b713",
   "metadata": {},
   "source": [
    "Now, the model is ready for training. The following function launches the training of the ANN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d22521-ceb5-4f33-94f9-21abf60d973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x train,\n",
    "                    y train,\n",
    "                    epochs          = 50,\n",
    "                    batch_size      = length of training dataset,\n",
    "                    validation_data = (x test, y test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f2dab-fec3-4497-94a2-a1460c8f02dc",
   "metadata": {},
   "source": [
    "The test scores are computed using the optimized parameter set on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd647516-9782-44a6-831a-f3d1d220d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(xtest_scl, ytest, verbose=0)\n",
    "\n",
    "print('test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe857-d3fd-48da-a9dc-a75e686f8b68",
   "metadata": {},
   "source": [
    "We can make a prediction with the Keras-built ANN and compare it with that from your own ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b3e22-cd95-4261-bd58-acef44622bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [ 1, 0.3, 0.1, 0.2, 0.2, 0.5, 0.7, -1. ]\n",
    "mydata = np.array(mydata).reshape(1,8)\n",
    "print(mydata.shape)\n",
    "predictions = model.predict(mydata)\n",
    "print(\"predicted age using Keras-built network: \", round(predictions[0,0],2), \"yr\")\n",
    "yout, zout = ANN(x = mydata.T, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "print(\"predicted age using your neural network: \", round(yout[NL-1][0,0], 2), \"yr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e237b49-85bd-475d-b536-3cfc4f1f67b0",
   "metadata": {},
   "source": [
    "Finally, we can use parameters estimated by Keras and your neural network to test whether you get the same estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813e857-e6b8-4da4-92df-d8f406849736",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keras = model.get_weights()\n",
    "len_par = len(param_keras)\n",
    "wts_keras = [param_keras[i].T for i in np.arange(0, len_par, 2)]\n",
    "bias_keras = [param_keras[i].reshape(param_keras[i].shape[0], 1) for i in np.arange(1, len_par, 2)]\n",
    "ActivFun_Keras = ['relu', 'relu', 'relu', 'linear']\n",
    "NpL_Keras = [16,32,16,1]\n",
    "Nfx = 8\n",
    "yout_k, zout_k = ANN(x = mydata.T, NpL = NpL_Keras, Nfx = Nfx, Wts = wts_keras, bias = bias_keras, ActivFun = ActivFun_Keras)\n",
    "print(\"predicted age using Keras-built network: \", round(predictions[0,0],2), \"yr\")\n",
    "print(\"predicted age using your neural network and Keras-estimated parameters: \", round(yout_k[len(NpL_Keras)-1][0,0], 2), \"yr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
