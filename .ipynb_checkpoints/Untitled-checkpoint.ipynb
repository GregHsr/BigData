{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688eff86-d550-419d-927e-24508ed292b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "## a function that takes a dataframe and returns some basic stastics of its columns\n",
    "def datasum(df):\n",
    "    dfmin = df.apply(np.min, 0)\n",
    "    dfmedian = df.apply(np.median, 0)\n",
    "    dfmean = df.apply(np.mean,0)\n",
    "    dfmax = df.apply(np.max, 0)\n",
    "    dfstd = df.apply(np.std, 0)\n",
    "    dfsummary = pd.DataFrame(np.array([dfmean, dfstd, dfmin, dfmedian, dfmax]), columns=list(df.columns[:]))\n",
    "    dfsummary[\"Statistic\"] = [\"mean\", \"std\", \"min\", \"median\", \"max\"]\n",
    "    xcol = dfsummary.shape[1]\n",
    "    colarr = [dfsummary.columns[xcol-1]]\n",
    "    for item in dfsummary.columns[0:(xcol-1)]:\n",
    "        colarr.append(item)\n",
    "    dfto = dfsummary[colarr]\n",
    "    return dfto\n",
    "\n",
    "## activation functions and their derivatives\n",
    "def logistic(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    t = np.exp(-x)\n",
    "    if mode == \"n\":\n",
    "        fx = 1/(1+t)\n",
    "    if mode == \"d\":\n",
    "        fx = t/(1+t)**2\n",
    "    return fx\n",
    "\n",
    "## define new functions here\n",
    "def tanh(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    p = np.exp(x)\n",
    "    n = np.exp(-x)\n",
    "    if mode == \"n\":\n",
    "        fx = p-n/(p+n)\n",
    "    if mode == \"d\":\n",
    "        fx = 1- (p-n/(p+n))**2\n",
    "    return fx\n",
    "\n",
    "def ReLU(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    if mode == \"n\":\n",
    "        fx = max(0,x)\n",
    "    if mode == \"d\":\n",
    "        if x < 0:\n",
    "            fx = 0\n",
    "        else:\n",
    "            fx = 1\n",
    "    return fx\n",
    "\n",
    "def Linear(x, mode = \"n\"):\n",
    "    '''\n",
    "    mode = \"n\" : returns f(x)\n",
    "    mode = \"d\": returns f'(x)\n",
    "    '''\n",
    "    if mode == \"n\":\n",
    "        fx = x\n",
    "    if mode == \"d\":\n",
    "        fx = 1\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a277839f-9640-4bec-9c60-35c8ff1c9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons per layer:  [3, 1]\n",
      "Number of neurons from the previous layer:  [2, 3]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "\n",
    "## NL is the number of layers including the hidden layers AND the output layer\n",
    "NL = 2\n",
    "\n",
    "## NpL defines the number of neurones per each layer. The length of NpL should be exactly equal to NL\n",
    "NpL = [3, 1]\n",
    "\n",
    "if len(NpL) != NL:\n",
    "    print(\"Warning: length error (Npl)\")\n",
    "    \n",
    "## ActivFun defines the activation functions per each layer\n",
    "ActivFun = ['logistic', 'logistic']\n",
    "\n",
    "if len(ActivFun) != NL:\n",
    "    print(\"Warning: length error (ActivFun)\")\n",
    "\n",
    "## Add the number of features x\n",
    "Nfx = 2\n",
    "\n",
    "## List of size of layers -1: important for automatic definition of parameters\n",
    "NpLm1 = [Nfx]\n",
    "for iL in np.arange(len(NpL)-1):\n",
    "    NpLm1.append(NpL[iL])\n",
    "\n",
    "print(\"Number of neurons per layer: \", NpL)\n",
    "print(\"Number of neurons from the previous layer: \", NpLm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1228cd85-4a94-4857-b5df-89d7d59d3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wts vaut: [array([0.73385614]), array([-0.14919551])]\n",
      "(1,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "\n",
    "## List of weights and biases\n",
    "Wts = []\n",
    "bias = []\n",
    "\n",
    "for iL in np.arange(len(NpL)):\n",
    "    ## random initialization\n",
    "    ## the initial parameters should be between -1 and 1\n",
    "    ## use the function np.random.rand to make random initializations (but these will be between 0 and 1)\n",
    "    sign = np.random.rand(1)\n",
    "    if sign < 0.5 : \n",
    "        WL = np.random.rand(1)\n",
    "    else : \n",
    "        WL = -np.random.rand(1)\n",
    "    sign = np.random.rand(1)\n",
    "    if sign < 0.5 : \n",
    "        bL = np.random.rand(1)\n",
    "    else : \n",
    "        bL = -np.random.rand(1)\n",
    "\n",
    "    ## appending\n",
    "    Wts.append(WL)\n",
    "    bias.append(bL)\n",
    "\n",
    "print('Wts vaut:', Wts)\n",
    "print(Wts[0].shape)\n",
    "print(bias[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3992db02-3ee2-4181-b8d9-a2aae3bdfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(x, NpL, Nfx, Wts, bias, ActivFun):\n",
    "    '''\n",
    "    This function computes the output of a neural network containing NL layers\n",
    "    where NL is the length of NpL. \n",
    "    o NpL contains the number of neurons per each hidden layer + the output layer. \n",
    "    o Nfx is the number of input features.\n",
    "    o Wts is a list that contains the 2D arrays of weights for each layer.\n",
    "    Each 2D array of weights has dimensions nL x nL-1, nL being the number of neurons\n",
    "    of current layer L, and nL-1 the number of neurons (or features) of layer L-1.\n",
    "    o bias is a list of 1D arrays of weights for each layer.\n",
    "    Each 1D array has dimensions nL x 1. When there are many data points (or members), say\n",
    "    n data points, the bias should be repeated using the function np.tile.\n",
    "    o ActivFun contains the name of the activation function for each layer.\n",
    "    '''\n",
    "    \n",
    "    n = x.shape[1]\n",
    "    print('n vaut:', n)\n",
    "    print('NL vaut:', len(NpL))\n",
    "    yLm1 = x\n",
    "    ## z is a list that saves the zL arrays for each hidden and output layer\n",
    "    z = []\n",
    "    ## similarly, y is a list that saves the yL arrays. Specifically, yL[NL-1] contains the output.\n",
    "    y = []\n",
    "    \n",
    "    for iL in np.arange(len(NpL)):\n",
    "        ## parameters\n",
    "        WL = Wts[iL]\n",
    "        bL = bias[iL]\n",
    "        \n",
    "        ## multiplication\n",
    "        ## make sure that the operation is correct for n individual points.\n",
    "        ## the dimension of zL should be nL x n. Since bL is nL x 1, use np.tile to overcome this issue.\n",
    "        print('valeurs',WL,yLm1)\n",
    "        #zL = np.dot(WL,yLm1) #+ np.tile(bL,len(NpL))\n",
    "        zL=z\n",
    "        z.append(zL)\n",
    "        \n",
    "        ## activation\n",
    "        ## to call a function given its name, use the function fx = globals()[\"fun_name\"]\n",
    "        sigma = globals()[ActivFun[iL]]\n",
    "        #yL = [sigma(k) for k in z]\n",
    "        yL=y\n",
    "        y.append(yL)\n",
    "        \n",
    "        ## move to next layer\n",
    "        yLm1 = yL\n",
    "\n",
    "    return y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b326c9f4-ea2a-4957-83f4-2c12686485d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "(4, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "n vaut: 4\n",
      "NL vaut: 2\n",
      "valeurs [0.31703606] [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "valeurs [0.25108537] [[...]]\n"
     ]
    }
   ],
   "source": [
    "## Input: 4 data points x 2 features (Nfx = 2)\n",
    "input_features = np.array([[0,0], \n",
    "                           [0,1], \n",
    "                           [1,0], \n",
    "                           [1,1]])\n",
    "print(input_features.shape)\n",
    "print(input_features)\n",
    "\n",
    "# Output: 4 data points x 1 feature\n",
    "target_output = np.array([[0], [1], [1], [1]])\n",
    "print(target_output.shape)\n",
    "print(target_output)\n",
    "\n",
    "## This where you can test your neural network\n",
    "x_in = input_features.T\n",
    "\n",
    "#print(x_in)\n",
    "#print(NpL)\n",
    "#print(Nfx)\n",
    "#print(bias[0])\n",
    "#print(ActivFun)\n",
    "#print(Wts[0])\n",
    "\n",
    "y,z = ANN(x = x_in, NpL = NpL, Nfx = Nfx, Wts = Wts, bias = bias, ActivFun = ActivFun)\n",
    "#ysim = y[NL-1].T\n",
    "#print(ysim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61803a6f-d0aa-4997-9527-e84306f1aac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
